---
title: "[validmind](/reference/validmind.html).FeatureImportance"
sidebar: validmind-reference
toc-depth: 4
toc-expand: 4
# module.qmd.jinja2
---

<!-- function.qmd.jinja2 -->

## FeatureImportance[()]{.muted}

<!-- signatures.jinja2 -->

::: {.signature}

<div class="decorator">@<!-- types.jinja2 - format_type --><!-- DEBUG: ExprCall --><span class="n">{'arguments': ["'model_explainability'", "'sklearn'"], 'cls': 'ExprCall', 'function': {'cls': 'ExprName', 'name': 'tags'}}</span><!-- DEBUG is_mapping: True -->
<!-- DEBUG type_class: dict -->
<!-- DEBUG cls_value: ExprCall -->
<!-- DEBUG cls_type: str -->
</div><div class="decorator">@<!-- types.jinja2 - format_type --><!-- DEBUG: ExprCall --><span class="n">{'arguments': ["'regression'", "'time_series_forecasting'"], 'cls': 'ExprCall', 'function': {'cls': 'ExprName', 'name': 'tasks'}}</span><!-- DEBUG is_mapping: True -->
<!-- DEBUG type_class: dict -->
<!-- DEBUG cls_value: ExprCall -->
<!-- DEBUG cls_type: str -->
</div>
<span class="kw">def</span><span class="name">FeatureImportance</span>(<span class="params"><span class="n">dataset</span><span class="p">:</span><!-- types.jinja2 - format_type --><!-- DEBUG: ExprName --><span class="n">VMDataset</span><!-- DEBUG is_mapping: True -->
<!-- DEBUG type_class: dict -->
<!-- DEBUG cls_value: ExprName -->
<!-- DEBUG cls_type: str -->
<span class="muted">,</span></span><span class="params"><span class="n">model</span><span class="p">:</span><!-- types.jinja2 - format_type --><!-- DEBUG: ExprName --><span class="n">VMModel</span><!-- DEBUG is_mapping: True -->
<!-- DEBUG type_class: dict -->
<!-- DEBUG cls_value: ExprName -->
<!-- DEBUG cls_type: str -->
<span class="muted">,</span></span><span class="params"><span class="n">num_features</span><span class="p">:</span><!-- types.jinja2 - format_type --><!-- DEBUG: ExprName --><span class="nb">int</span><!-- DEBUG is_mapping: True -->
<!-- DEBUG type_class: dict -->
<!-- DEBUG cls_value: ExprName -->
<!-- DEBUG cls_type: str -->
<span class="o">=</span><span class="kc">3</span></span>):

:::

<!-- docstring.jinja2 -->

Compute feature importance scores for a given model and generate a summary table

with the top important features.

### Purpose

The Feature Importance Comparison test is designed to compare the feature importance scores for different models when applied to various datasets. By doing so, it aims to identify the most impactful features and assess the consistency of feature importance across models.

### Test Mechanism

This test works by iterating through each dataset-model pair and calculating permutation feature importance (PFI) scores. It then generates a summary table containing the top `num_features` important features for each model. The process involves:

- Extracting features and target data from each dataset.
- Computing PFI scores using `sklearn.inspection.permutation_importance`.
- Sorting and selecting the top features based on their importance scores.
- Compiling these features into a summary table for comparison.

### Signs of High Risk

- Key features expected to be important are ranked low, indicating potential issues with model training or data quality.
- High variance in feature importance scores across different models, suggesting instability in feature selection.

### Strengths

- Provides a clear comparison of the most important features for each model.
- Uses permutation importance, which is a model-agnostic method and can be applied to any estimator.

### Limitations

- Assumes that the dataset is provided as a DataFrameDataset object with `x_df` and `y_df` methods to access feature and target data.
- Requires that `model.model` is compatible with `sklearn.inspection.permutation_importance`.
- The function's output is dependent on the number of features specified by `num_features`, which defaults to 3 but can be adjusted.

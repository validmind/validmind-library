{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Processing Functions in ValidMind\n",
    "\n",
    "Welcome! This notebook demonstrates how to use post-processing functions with ValidMind tests to customize test outputs. You'll learn various ways to modify test results including updating tables, adding/removing tables, creating figures from tables, and vice versa.\n",
    "\n",
    "## Contents\n",
    "- [About Post-Processing Functions](#about-post-processing-functions)\n",
    "- [Key Concepts](#key-concepts)\n",
    "- [Setup and Prerequisites](#setup-and-prerequisites)\n",
    "- [Simple Tabular Updates](#simple-tabular-updates)\n",
    "- [Adding Tables](#adding-tables) \n",
    "- [Removing Tables](#removing-tables)\n",
    "- [Creating Figures from Tables](#creating-figures-from-tables)\n",
    "- [Creating Tables from Figures](#creating-tables-from-figures)\n",
    "- [Re-Drawing Confusion Matrix](#re-drawing-confusion-matrix)\n",
    "- [Re-Drawing ROC Curve](#re-drawing-roc-curve)\n",
    "- [Custom Test Example](#custom-test-example)\n",
    "\n",
    "## About Post-Processing Functions\n",
    "\n",
    "Post-processing functions allow you to customize the output of ValidMind tests before they are saved to the platform. These functions take a TestResult object as input and return a modified TestResult object.\n",
    "\n",
    "Common use cases include:\n",
    "- Reformatting table data\n",
    "- Adding or removing tables/figures\n",
    "- Creating new visualizations from test data\n",
    "- Customizing test pass/fail criteria\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**TestResult object**: The main object that post-processing functions work with, containing:\n",
    "- tables: List of ResultTable objects\n",
    "- figures: List of Figure objects \n",
    "- passed: Boolean indicating test pass/fail status\n",
    "- raw_data: Additional data from test execution\n",
    "\n",
    "**ResultTable**: Object representing tabular data with:\n",
    "- title: Table title\n",
    "- data: Pandas DataFrame or list of dictionaries\n",
    "\n",
    "**Figure**: Object representing plots/visualizations with:\n",
    "- figure: matplotlib or plotly figure object\n",
    "- key: Unique identifier\n",
    "- ref_id: Reference ID linking to test\n",
    "\n",
    "## Setup and Prerequisites\n",
    "\n",
    "First, we'll set up our environment and load sample data using the customer churn dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import validmind as vm\n",
    "from validmind.datasets.classification import customer_churn\n",
    "\n",
    "vm.init()\n",
    "\n",
    "raw_df = customer_churn.load_data()\n",
    "\n",
    "train_df, validation_df, test_df = customer_churn.preprocess(raw_df)\n",
    "\n",
    "x_train = train_df.drop(customer_churn.target_column, axis=1)\n",
    "y_train = train_df[customer_churn.target_column]\n",
    "x_val = validation_df.drop(customer_churn.target_column, axis=1)\n",
    "y_val = validation_df[customer_churn.target_column]\n",
    "\n",
    "model = xgb.XGBClassifier(early_stopping_rounds=10)\n",
    "model.set_params(\n",
    "    eval_metric=[\"error\", \"logloss\", \"auc\"],\n",
    ")\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    eval_set=[(x_val, y_val)],\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "vm_raw_dataset = vm.init_dataset(\n",
    "    dataset=raw_df,\n",
    "    input_id=\"raw_dataset\",\n",
    "    target_column=customer_churn.target_column,\n",
    "    class_labels=customer_churn.class_labels,\n",
    ")\n",
    "\n",
    "vm_train_ds = vm.init_dataset(\n",
    "    dataset=train_df,\n",
    "    input_id=\"train_dataset\",\n",
    "    target_column=customer_churn.target_column,\n",
    ")\n",
    "\n",
    "vm_test_ds = vm.init_dataset(\n",
    "    dataset=test_df, input_id=\"test_dataset\", target_column=customer_churn.target_column\n",
    ")\n",
    "\n",
    "vm_model = vm.init_model(\n",
    "    model,\n",
    "    input_id=\"model\",\n",
    ")\n",
    "\n",
    "vm_train_ds.assign_predictions(\n",
    "    model=vm_model,\n",
    ")\n",
    "\n",
    "vm_test_ds.assign_predictions(\n",
    "    model=vm_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a refresher, here is how we run a test normally, without any post-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests import run_test\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.model_validation.sklearn.ClassifierPerformance\",\n",
    "    inputs={\"dataset\": vm_test_ds, \"model\": vm_model},\n",
    "    generate_description=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Tabular Updates\n",
    "\n",
    "The simplest form of post-processing is modifying existing table data. Here we demonstrate updating class labels in a classification performance table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.vm_models.result import TestResult\n",
    "\n",
    "\n",
    "def add_class_labels(result: TestResult):\n",
    "    result.tables[0].data[\"Class\"] = (\n",
    "        result.tables[0]\n",
    "        .data[\"Class\"]\n",
    "        .map(lambda x: \"Churn\" if x == \"1\" else \"No Churn\" if x == \"0\" else x)\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.model_validation.sklearn.ClassifierPerformance\",\n",
    "    inputs={\"dataset\": vm_test_ds, \"model\": vm_model},\n",
    "    generate_description=False,\n",
    "    post_process_fn=add_class_labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Tables\n",
    "\n",
    "Sometimes you may want to add supplementary tables to provide additional context or information. This example shows how to add a legend table mapping class values to labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.vm_models.result import ResultTable\n",
    "\n",
    "def add_table(result: TestResult):\n",
    "    # add legend table to show map of class value to class label\n",
    "    result.add_table(\n",
    "        ResultTable(\n",
    "            title=\"Class Legend\",\n",
    "            data=[\n",
    "                {\"Class Value\": \"0\", \"Class Label\": \"No Churn\"},\n",
    "                {\"Class Value\": \"1\", \"Class Label\": \"Churn\"},\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.model_validation.sklearn.ClassifierPerformance\",\n",
    "    inputs={\"dataset\": vm_test_ds, \"model\": vm_model},\n",
    "    generate_description=False,\n",
    "    post_process_fn=add_table,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Tables \n",
    "\n",
    "You can also remove tables that may not be relevant for your use case. Here we demonstrate removing a specific table from the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_table(result: TestResult):\n",
    "    result.tables.pop(1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.model_validation.sklearn.ClassifierPerformance\",\n",
    "    inputs={\"dataset\": vm_test_ds, \"model\": vm_model},\n",
    "    generate_description=False,\n",
    "    post_process_fn=remove_table,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Figures from Tables\n",
    "\n",
    "A powerful use of post-processing is creating visualizations from tabular data. This example shows creating a bar plot from an outliers table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly_express import bar\n",
    "from validmind.vm_models.figure import Figure\n",
    "\n",
    "\n",
    "def create_figure(result: TestResult):\n",
    "    fig = bar(result.tables[0].data, x=\"Variable\", y=\"Total Count of Outliers\")\n",
    "\n",
    "    result.add_figure(\n",
    "        Figure(\n",
    "            figure=fig,\n",
    "            key=\"outlier_count_by_variable\",\n",
    "            ref_id=result.ref_id,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.data_validation.IQROutliersTable\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    generate_description=False,\n",
    "    post_process_fn=create_figure,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Tables from Figures\n",
    "\n",
    "The reverse operation - extracting tabular data from figures - is also possible. Here we demonstrate creating a table from figure data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(result: TestResult):\n",
    "    for fig in result.figures:\n",
    "        data = fig.figure.data[0]\n",
    "\n",
    "        table_data = [\n",
    "            {\"Percentile\": x, \"Outlier Count\": y}\n",
    "            for x, y in zip(data.x, data.y)\n",
    "        ]\n",
    "\n",
    "        result.add_table(\n",
    "            ResultTable(\n",
    "                title=fig.figure.layout.title.text,\n",
    "                data=table_data,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.data_validation.IQROutliersBarPlot\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    generate_description=False,\n",
    "    # post_process_fn=create_table,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Drawing Confusion Matrix\n",
    "\n",
    "Sometimes you may want to completely replace the default visualizations. This example shows how to redraw a confusion matrix using matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def re_draw_class_imbalance(result: TestResult):\n",
    "    data = result.tables[0].data\n",
    "    # Exited Percentage of Rows (%) Pass/Fail\n",
    "    # 0       0                 80.25%      Pass\n",
    "    # 1       1                 19.75%      Pass\n",
    "\n",
    "    result.figures = []\n",
    "\n",
    "    # use matplotlib to plot the confusion matrix\n",
    "    fig = plt.figure()\n",
    "\n",
    "    # show a bar plot of the class imbalance with matplotlib\n",
    "    plt.bar(data[\"Exited\"], data[\"Percentage of Rows (%)\"])\n",
    "    plt.xlabel(\"Exited\")\n",
    "    plt.ylabel(\"Percentage of Rows (%)\")\n",
    "    plt.title(\"Class Imbalance\")\n",
    "\n",
    "    result.add_figure(\n",
    "        Figure(\n",
    "            figure=fig,\n",
    "            key=\"confusion_matrix\",\n",
    "            ref_id=result.ref_id,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.data_validation.ClassImbalance\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    generate_description=False,\n",
    "    post_process_fn=re_draw_class_imbalance,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Drawing ROC Curve\n",
    "\n",
    "Here is another example of re-drawing a figure. This time we are re-drawing the ROC curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_roc_curve(result: TestResult):\n",
    "    result.raw_data.fpr\n",
    "\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.model_validation.sklearn.ROCCurve\",\n",
    "    inputs={\"dataset\": vm_test_ds, \"model\": vm_model},\n",
    "    generate_description=False,\n",
    "    post_process_fn=post_process_roc_curve,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Test Example\n",
    "\n",
    "While we envision that post-processing functions are most useful for modifying built-in (ValidMind  Library) tests, there are cases where you may want to use them for your own custom tests. Let's see an example of a situation where this is the case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotly_express import bar\n",
    "from validmind.vm_models.figure import Figure\n",
    "from validmind.vm_models.result import TestResult\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "@vm.test(\"my_custom_tests.Sensitivity\")\n",
    "def sensitivity_test(strike=None):\n",
    "    \"\"\"This is sensitivity test\"\"\"\n",
    "    price = strike * random.random()\n",
    "\n",
    "    return pd.DataFrame({\"Option price\": [price]})\n",
    "\n",
    "\n",
    "def process_results(result: TestResult):\n",
    "\n",
    "    df = pd.DataFrame(result.tables[0].data)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df[\"strike\"].values, y=df[\"Option price\"].values, mode=\"lines\")\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        # title=params[\"title\"],\n",
    "        # xaxis_title=params[\"xlabel\"],\n",
    "        # yaxis_title=params[\"ylabel\"],\n",
    "        showlegend=True,\n",
    "        template=\"plotly_white\",  # Adds a grid by default\n",
    "    )\n",
    "\n",
    "    result.add_figure(\n",
    "        Figure(\n",
    "            figure=fig,\n",
    "            key=\"sensitivity_to_strike\",\n",
    "            ref_id=result.ref_id,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = run_test(\n",
    "    \"my_custom_tests.Sensitivity:ToStrike\",\n",
    "    param_grid={\n",
    "        \"strike\": list(np.linspace(0, 100, 20)),\n",
    "    },\n",
    "    post_process_fn=process_results,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-BbKYUwN1-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

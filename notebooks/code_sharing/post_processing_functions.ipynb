{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import validmind as vm\n",
    "from validmind.datasets.classification import customer_churn\n",
    "\n",
    "vm.init()\n",
    "\n",
    "raw_df = customer_churn.load_data()\n",
    "\n",
    "train_df, validation_df, test_df = customer_churn.preprocess(raw_df)\n",
    "\n",
    "x_train = train_df.drop(customer_churn.target_column, axis=1)\n",
    "y_train = train_df[customer_churn.target_column]\n",
    "x_val = validation_df.drop(customer_churn.target_column, axis=1)\n",
    "y_val = validation_df[customer_churn.target_column]\n",
    "\n",
    "model = xgb.XGBClassifier(early_stopping_rounds=10)\n",
    "model.set_params(\n",
    "    eval_metric=[\"error\", \"logloss\", \"auc\"],\n",
    ")\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    eval_set=[(x_val, y_val)],\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "vm_raw_dataset = vm.init_dataset(\n",
    "    dataset=raw_df,\n",
    "    input_id=\"raw_dataset\",\n",
    "    target_column=customer_churn.target_column,\n",
    "    class_labels=customer_churn.class_labels,\n",
    ")\n",
    "\n",
    "vm_train_ds = vm.init_dataset(\n",
    "    dataset=train_df,\n",
    "    input_id=\"train_dataset\",\n",
    "    target_column=customer_churn.target_column,\n",
    ")\n",
    "\n",
    "vm_test_ds = vm.init_dataset(\n",
    "    dataset=test_df, input_id=\"test_dataset\", target_column=customer_churn.target_column\n",
    ")\n",
    "\n",
    "vm_model = vm.init_model(\n",
    "    model,\n",
    "    input_id=\"model\",\n",
    ")\n",
    "\n",
    "vm_train_ds.assign_predictions(\n",
    "    model=vm_model,\n",
    ")\n",
    "\n",
    "vm_test_ds.assign_predictions(\n",
    "    model=vm_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests import run_test\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.model_validation.sklearn.ClassifierPerformance\",\n",
    "    inputs={\"dataset\": vm_test_ds, \"model\": vm_model},\n",
    "    generate_description=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Tabular Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.vm_models.result import TestResult\n",
    "\n",
    "\n",
    "def add_class_labels(result: TestResult):\n",
    "    result.tables[0].data[\"Class\"] = (\n",
    "        result.tables[0]\n",
    "        .data[\"Class\"]\n",
    "        .map(lambda x: \"Churn\" if x == \"1\" else \"No Churn\" if x == \"0\" else x)\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.model_validation.sklearn.ClassifierPerformance\",\n",
    "    inputs={\"dataset\": vm_test_ds, \"model\": vm_model},\n",
    "    generate_description=False,\n",
    "    post_process_fn=add_class_labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.vm_models.result import ResultTable\n",
    "\n",
    "def add_table(result: TestResult):\n",
    "    # add legend table to show map of class value to class label\n",
    "    result.add_table(\n",
    "        ResultTable(\n",
    "            title=\"Class Legend\",\n",
    "            data=[\n",
    "                {\"Class Value\": \"0\", \"Class Label\": \"No Churn\"},\n",
    "                {\"Class Value\": \"1\", \"Class Label\": \"Churn\"},\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.model_validation.sklearn.ClassifierPerformance\",\n",
    "    inputs={\"dataset\": vm_test_ds, \"model\": vm_model},\n",
    "    generate_description=False,\n",
    "    post_process_fn=add_table,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_table(result: TestResult):\n",
    "    result.tables.pop(1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.model_validation.sklearn.ClassifierPerformance\",\n",
    "    inputs={\"dataset\": vm_test_ds, \"model\": vm_model},\n",
    "    generate_description=False,\n",
    "    post_process_fn=remove_table,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Figure from Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly_express import bar\n",
    "from validmind.vm_models.figure import Figure\n",
    "\n",
    "\n",
    "def create_figure(result: TestResult):\n",
    "    fig = bar(result.tables[0].data, x=\"Variable\", y=\"Total Count of Outliers\")\n",
    "\n",
    "    result.add_figure(\n",
    "        Figure(\n",
    "            figure=fig,\n",
    "            key=\"outlier_count_by_variable\",\n",
    "            ref_id=result.ref_id,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.data_validation.IQROutliersTable\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    generate_description=False,\n",
    "    post_process_fn=create_figure,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tables from Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(result: TestResult):\n",
    "    for fig in result.figures:\n",
    "        data = fig.figure.data[0]\n",
    "\n",
    "        table_data = [\n",
    "            {\"Percentile\": x, \"Outlier Count\": y}\n",
    "            for x, y in zip(data.x, data.y)\n",
    "        ]\n",
    "\n",
    "        result.add_table(\n",
    "            ResultTable(\n",
    "                title=fig.figure.layout.title.text,\n",
    "                data=table_data,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.data_validation.IQROutliersBarPlot\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    generate_description=False,\n",
    "    # post_process_fn=create_table,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"stop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Draw Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def re_draw_class_imbalance(result: TestResult):\n",
    "    data = result.tables[0].data\n",
    "    # Exited Percentage of Rows (%) Pass/Fail\n",
    "    # 0       0                 80.25%      Pass\n",
    "    # 1       1                 19.75%      Pass\n",
    "\n",
    "    result.figures = []\n",
    "\n",
    "    # use matplotlib to plot the confusion matrix\n",
    "    fig = plt.figure()\n",
    "\n",
    "    # show a bar plot of the class imbalance with matplotlib\n",
    "    plt.bar(data[\"Exited\"], data[\"Percentage of Rows (%)\"])\n",
    "    plt.xlabel(\"Exited\")\n",
    "    plt.ylabel(\"Percentage of Rows (%)\")\n",
    "    plt.title(\"Class Imbalance\")\n",
    "\n",
    "    result.add_figure(\n",
    "        Figure(\n",
    "            figure=fig,\n",
    "            key=\"confusion_matrix\",\n",
    "            ref_id=result.ref_id,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.data_validation.ClassImbalance\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    generate_description=False,\n",
    "    post_process_fn=re_draw_class_imbalance,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.data_validation.ClassImbalance\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    generate_description=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_class_imbalance(result: TestResult):\n",
    "    result.passed = None\n",
    "    result.figures = []\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.data_validation.ClassImbalance\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    generate_description=False,\n",
    "    post_process_fn=post_process_class_imbalance,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_test(\n",
    "    \"validmind.model_validation.sklearn.ROCCurve\",\n",
    "    inputs={\"dataset\": vm_test_ds, \"model\": vm_model},\n",
    "    generate_description=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_roc_curve(result: TestResult):\n",
    "    result.raw_data.fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotly_express import bar\n",
    "from validmind.vm_models.figure import Figure\n",
    "from validmind.vm_models.result import TestResult\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "@vm.test(\"my_custom_tests.Sensitivity\")\n",
    "def sensitivity_test(strike=None):\n",
    "    \"\"\"This is sensitivity test\"\"\"\n",
    "    price = strike * random.random()\n",
    "\n",
    "    return pd.DataFrame({\"Option price\": [price]})\n",
    "\n",
    "\n",
    "def process_results(result: TestResult):\n",
    "\n",
    "    df = pd.DataFrame(result.tables[0].data)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=df[\"strike\"].values, y=df[\"Option price\"].values, mode=\"lines\")\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        # title=params[\"title\"],\n",
    "        # xaxis_title=params[\"xlabel\"],\n",
    "        # yaxis_title=params[\"ylabel\"],\n",
    "        showlegend=True,\n",
    "        template=\"plotly_white\",  # Adds a grid by default\n",
    "    )\n",
    "\n",
    "    result.add_figure(\n",
    "        Figure(\n",
    "            figure=fig,\n",
    "            key=\"sensitivity_to_strike\",\n",
    "            ref_id=result.ref_id,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = run_test(\n",
    "    \"my_custom_tests.Sensitivity:ToStrike\",\n",
    "    param_grid={\n",
    "        \"strike\": list(np.linspace(0, 100, 20)),\n",
    "    },\n",
    "    post_process_fn=process_results,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests import list_tests\n",
    "\n",
    "list_tests()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-BbKYUwN1-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

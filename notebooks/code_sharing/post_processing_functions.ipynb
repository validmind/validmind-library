{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Processing Functions in ValidMind\n",
    "\n",
    "Welcome! This notebook demonstrates how to use post-processing functions with ValidMind tests to customize test outputs. You'll learn various ways to modify test results including updating tables, adding/removing tables, creating figures from tables, and vice versa.\n",
    "\n",
    "## Contents\n",
    "- [About Post-Processing Functions](#about-post-processing-functions)\n",
    "- [Key Concepts](#key-concepts)\n",
    "- [Setup and Prerequisites](#setup-and-prerequisites)\n",
    "- [Simple Tabular Updates](#simple-tabular-updates)\n",
    "- [Adding Tables](#adding-tables) \n",
    "- [Removing Tables](#removing-tables)\n",
    "- [Creating Figures from Tables](#creating-figures-from-tables)\n",
    "- [Creating Tables from Figures](#creating-tables-from-figures)\n",
    "- [Re-Drawing Confusion Matrix](#re-drawing-confusion-matrix)\n",
    "- [Re-Drawing ROC Curve](#re-drawing-roc-curve)\n",
    "- [Custom Test Example](#custom-test-example)\n",
    "\n",
    "## About Post-Processing Functions\n",
    "\n",
    "Post-processing functions allow you to customize the output of ValidMind tests before they are saved to the platform. These functions take a TestResult object as input and return a modified TestResult object.\n",
    "\n",
    "Common use cases include:\n",
    "- Reformatting table data\n",
    "- Adding or removing tables/figures\n",
    "- Creating new visualizations from test data\n",
    "- Customizing test pass/fail criteria\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**`validmind.vm_models.result.TestResult`**: Whenever a test is run with the `run_test` function in ValidMind, the items returned/produced by the test are bundled into a single `TestResult` object. There are several attributes on this object that are useful to know about:\n",
    "- `tables`: List of `validmind.vm_models.result.ResultTable` objects (see below)\n",
    "- `figures`: List of `validmind.vm_models.figure.Figure` objects (see below)\n",
    "- `passed`: Optional boolean indicating test pass/fail status. `None` indicates that the test is not a pass/fail test (previously known as a threshold test).\n",
    "- `raw_data`: Optional `validmind.vm_models.result.RawData` object containing additional data from test execution. Some ValidMind tests will produce this raw data to be used in post-processing functions. This data is not displayed in the test result or sent to the ValidMind platform (currently). To view the available raw data, you can run `result.raw_data.inspect()` which will return a dictionary where the keys are the raw data attributes available and the values are string representations of the data.\n",
    "\n",
    "**`validmind.vm_models.result.ResultTable`**: ValidMind object representing tables displayed in the test result and sent to the ValidMind platform:\n",
    "- `title`: Optional table title\n",
    "- `data`: Pandas dataframe\n",
    "\n",
    "**`validmind.vm_models.figure.Figure`**: ValidMind object representing plots/visualizations displayed in the test result and sent to the ValidMind platform:\n",
    "- `figure`: matplotlib or plotly figure object\n",
    "- `key`: Unique identifier\n",
    "- `ref_id`: Reference ID linking to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Prerequisites\n",
    "\n",
    "First, we'll set up our environment and load sample data using the customer churn dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import validmind as vm\n",
    "from validmind.datasets.classification import customer_churn\n",
    "\n",
    "raw_df = customer_churn.load_data()\n",
    "\n",
    "train_df, validation_df, test_df = customer_churn.preprocess(raw_df)\n",
    "\n",
    "x_train = train_df.drop(customer_churn.target_column, axis=1)\n",
    "y_train = train_df[customer_churn.target_column]\n",
    "x_val = validation_df.drop(customer_churn.target_column, axis=1)\n",
    "y_val = validation_df[customer_churn.target_column]\n",
    "\n",
    "model = xgb.XGBClassifier(early_stopping_rounds=10)\n",
    "model.set_params(\n",
    "    eval_metric=[\"error\", \"logloss\", \"auc\"],\n",
    ")\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    eval_set=[(x_val, y_val)],\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "vm_raw_dataset = vm.init_dataset(\n",
    "    dataset=raw_df,\n",
    "    input_id=\"raw_dataset\",\n",
    "    target_column=customer_churn.target_column,\n",
    "    class_labels=customer_churn.class_labels,\n",
    "    __log=False,\n",
    ")\n",
    "\n",
    "vm_train_ds = vm.init_dataset(\n",
    "    dataset=train_df,\n",
    "    input_id=\"train_dataset\",\n",
    "    target_column=customer_churn.target_column,\n",
    "    __log=False,\n",
    ")\n",
    "\n",
    "vm_test_ds = vm.init_dataset(\n",
    "    dataset=test_df,\n",
    "    input_id=\"test_dataset\",\n",
    "    target_column=customer_churn.target_column,\n",
    "    __log=False,\n",
    ")\n",
    "\n",
    "vm_model = vm.init_model(\n",
    "    model,\n",
    "    input_id=\"model\",\n",
    "    __log=False,\n",
    ")\n",
    "\n",
    "vm_train_ds.assign_predictions(\n",
    "    model=vm_model,\n",
    ")\n",
    "\n",
    "vm_test_ds.assign_predictions(\n",
    "    model=vm_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a refresher, here is how we run a test normally, without any post-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.tests import run_test\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.model_validation.sklearn.ClassifierPerformance\",\n",
    "    inputs={\"dataset\": vm_test_ds, \"model\": vm_model},\n",
    "    generate_description=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Tabular Updates\n",
    "\n",
    "The simplest form of post-processing is modifying existing table data. Here we demonstrate updating class labels in a classification performance table.\n",
    "\n",
    "Some key concepts to keep in mind:\n",
    "- Tables produced by a test are accessible via the `result.tables` attribute\n",
    "    - The `result.tables` attribute is a list of `ResultTable` objects which are simple data structures that contain a `data` attribute and an optional `title` attribute\n",
    "    - The `data` attribute is guaranteed to be a `pd.DataFrame` whether the test code itself returns a `pd.DataFrame` or a list of dictionaries\n",
    "    - The `title` attribute is optional and can be set by tests that return a dictionary where the keys are the table titles and the values are the table data (e.g. `{\"Classifier Performance\": performance_df, \"Class Legend\": [{\"Class Value\": \"0\", \"Class Label\": \"No Churn\"}, {\"Class Value\": \"1\", \"Class Label\": \"Churn\"}]}}`)\n",
    "- Post-processing functions can directly modify any of the tables in the `result.tables` list and return the modified `TestResult` object... This can be useful for renaming columns, adding/removing rows, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.vm_models.result import TestResult\n",
    "\n",
    "\n",
    "def add_class_labels(result: TestResult):\n",
    "    result.tables[0].data[\"Class\"] = (\n",
    "        result.tables[0]\n",
    "        .data[\"Class\"]\n",
    "        .map(lambda x: \"Churn\" if x == \"1\" else \"No Churn\" if x == \"0\" else x)\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.model_validation.sklearn.ClassifierPerformance\",\n",
    "    inputs={\"dataset\": vm_test_ds, \"model\": vm_model},\n",
    "    generate_description=False,\n",
    "    post_process_fn=add_class_labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Tables\n",
    "\n",
    "This example shows how to add a legend table mapping class values to labels using the `TestResult.add_table()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_table(result: TestResult):\n",
    "    # add legend table to show map of class value to class label\n",
    "    result.add_table(\n",
    "        title=\"Class Legend\",\n",
    "        table=[\n",
    "            {\"Class Value\": \"0\", \"Class Label\": \"No Churn\"},\n",
    "            {\"Class Value\": \"1\", \"Class Label\": \"Churn\"},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.model_validation.sklearn.ClassifierPerformance\",\n",
    "    inputs={\"dataset\": vm_test_ds, \"model\": vm_model},\n",
    "    generate_description=False,\n",
    "    post_process_fn=add_table,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Tables \n",
    "\n",
    "If there are tables in the test result that you don't want to display or log to the ValidMind platform, you can remove them using the `TestResult.remove_table()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_table(result: TestResult):\n",
    "    result.remove_table(1)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.model_validation.sklearn.ClassifierPerformance\",\n",
    "    inputs={\"dataset\": vm_test_ds, \"model\": vm_model},\n",
    "    generate_description=False,\n",
    "    post_process_fn=remove_table,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Figures from Tables\n",
    "\n",
    "A powerful use of post-processing is creating visualizations from tabular data. This example shows creating a bar plot from an outliers table using the `TestResult.add_figure()` method. This method can take a `matplotlib`, `plotly`, raw PNG `bytes`, or a `validmind.vm_models.figure.Figure` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.express import bar\n",
    "\n",
    "\n",
    "def create_figure(result: TestResult):\n",
    "    result.add_figure(\n",
    "        bar(result.tables[0].data, x=\"Variable\", y=\"Total Count of Outliers\")\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.data_validation.IQROutliersTable\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    generate_description=False,\n",
    "    post_process_fn=create_figure,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Tables from Figures\n",
    "\n",
    "The reverse operation - extracting tabular data from figures - is also possible. However, its recommended instead to use the raw data produced by the test (assuming it is available) as the approach below requires deeper knowledge of the underlying figure (e.g. `matplotlib` or `plotly`) and may not be as robust/maintainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(result: TestResult):\n",
    "    for fig in result.figures:\n",
    "        data = fig.figure.data[0]\n",
    "\n",
    "        result.add_table(\n",
    "            title=fig.figure.layout.title.text,\n",
    "            table=[\n",
    "                {\"Percentile\": x, \"Outlier Count\": y}\n",
    "                for x, y in zip(data.x, data.y)\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.data_validation.IQROutliersBarPlot\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    generate_description=False,\n",
    "    post_process_fn=create_table,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Drawing Confusion Matrix\n",
    "\n",
    "A less common example is re-drawing a figure. This example uses the table produced by the test to create a matplotlib confusion matrix figure and removes the existing plotly figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def re_draw_class_imbalance(result: TestResult):\n",
    "    data = result.tables[0].data\n",
    "\n",
    "    # remove the existing figure\n",
    "    result.remove_figure(0)\n",
    "\n",
    "    # use matplotlib to plot the confusion matrix\n",
    "    fig = plt.figure()\n",
    "\n",
    "    plt.bar(data[\"Exited\"], data[\"Percentage of Rows (%)\"])\n",
    "    plt.xlabel(\"Exited\")\n",
    "    plt.ylabel(\"Percentage of Rows (%)\")\n",
    "    plt.title(\"Class Imbalance\")\n",
    "\n",
    "    # add the figure to the result\n",
    "    result.add_figure(fig)\n",
    "\n",
    "    # close the figure to avoid showing it in the test result\n",
    "    plt.close()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.data_validation.ClassImbalance\",\n",
    "    inputs={\"dataset\": vm_test_ds},\n",
    "    generate_description=False,\n",
    "    post_process_fn=re_draw_class_imbalance,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Drawing ROC Curve\n",
    "\n",
    "This example shows re-drawing the ROC curve using the raw data produced by the test. This is the recommended approach to reproducing figures or tables from test results as it allows you to get intermediate and other raw data that was originally used by the test to produce the figures or tables we want to reproduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's run the test without post-processing to see the original result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the test without post-processing\n",
    "result = run_test(\n",
    "    \"validmind.model_validation.sklearn.ROCCurve\",\n",
    "    inputs={\"dataset\": vm_test_ds, \"model\": vm_model},\n",
    "    generate_description=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a `TestResult` object, we can inspect the raw data to see what is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.raw_data.inspect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what is available in the raw data, we can build a post-processing function that uses this raw data to reproduce the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_roc_curve(result: TestResult):\n",
    "    fpr = result.raw_data.fpr\n",
    "    tpr = result.raw_data.tpr\n",
    "    auc = result.raw_data.auc\n",
    "\n",
    "    # remove the existing figure\n",
    "    result.remove_figure(0)\n",
    "\n",
    "    # use matplotlib to plot the ROC curve\n",
    "    fig = plt.figure()\n",
    "\n",
    "    plt.plot(fpr, tpr, label=f\"ROC Curve (AUC = {auc:.2f})\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "    result.add_figure(fig)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = run_test(\n",
    "    \"validmind.model_validation.sklearn.ROCCurve\",\n",
    "    inputs={\"dataset\": vm_test_ds, \"model\": vm_model},\n",
    "    generate_description=False,\n",
    "    post_process_fn=post_process_roc_curve,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Test Example\n",
    "\n",
    "While we envision that post-processing functions are most useful for modifying built-in (ValidMind  Library) tests, there are also cases where you may want to use them for your own custom tests. Let's see an example of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from validmind import test\n",
    "from validmind.tests import run_test\n",
    "\n",
    "\n",
    "@test(\"custom.CorrelationBetweenVariables\")\n",
    "def CorrelationBetweenVariables(var1: str, var2: str):\n",
    "    \"\"\"This fake test shows the relationship between two variables\"\"\"\n",
    "    data = pd.DataFrame(\n",
    "        {\n",
    "            \"Variable 1\": np.random.rand(20),\n",
    "            \"Variable 2\": np.random.rand(20),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return [{\"Correlation between var1 and var2\": data.corr().iloc[0, 1]}]\n",
    "\n",
    "\n",
    "variables = [\"Age\", \"Balance\", \"CreditScore\", \"EstimatedSalary\"]\n",
    "\n",
    "result = run_test(\n",
    "    \"custom.CorrelationBetweenVariables\",\n",
    "    param_grid={\n",
    "        \"var1\": variables,\n",
    "        \"var2\": variables,\n",
    "    }, # this will automatically generate all combinations of variables for var1 and var2\n",
    "    generate_description=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the test result now contains a table with the correlation between each pair of variables like this:\n",
    "\n",
    "| var1 | var2 | Correlation between var1 and var2 |\n",
    "|------|------|-----------------------------------|\n",
    "| Age | Age | 0.3001 |\n",
    "| Age | Balance | -0.4185 |\n",
    "| Age | CreditScore | 0.2952 |\n",
    "| Age | EstimatedSalary | -0.2855 |\n",
    "| Balance | Age | 0.0141 |\n",
    "| Balance | Balance | -0.1513 |\n",
    "| Balance | CreditScore | 0.2401 |\n",
    "| Balance | EstimatedSalary | 0.1198 |\n",
    "| CreditScore | Age | -0.2320 |\n",
    "| CreditScore | Balance | 0.4125 |\n",
    "| CreditScore | CreditScore | 0.1726 |\n",
    "| CreditScore | EstimatedSalary | 0.3187 |\n",
    "| EstimatedSalary | Age | -0.1774 |\n",
    "| EstimatedSalary | Balance | -0.1202 |\n",
    "| EstimatedSalary | CreditScore | 0.1488 |\n",
    "| EstimatedSalary | EstimatedSalary | 0.0524 |\n",
    "\n",
    "Now let's say we don't really want to see the big table of correlations. Instead, we want to see a heatmap of the correlations. We can use a post-processing function to create a heatmap from the table and add it to the test result while removing the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def create_heatmap(result: TestResult):\n",
    "    # get the data from the existing table\n",
    "    data = result.tables[0].data\n",
    "\n",
    "    # remove the existing table\n",
    "    result.remove_table(0)\n",
    "    \n",
    "    # Create a pivot table from the data to get it in matrix form\n",
    "    matrix = pd.pivot_table(\n",
    "        data,\n",
    "        values='Correlation between var1 and var2',\n",
    "        index='var1',\n",
    "        columns='var2'\n",
    "    )\n",
    "\n",
    "    # remove the existing figure \n",
    "    result.remove_figure(0)\n",
    "\n",
    "    # use plotly to create a heatmap\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=matrix.values,\n",
    "        x=matrix.columns,\n",
    "        y=matrix.index,\n",
    "        colorscale='RdBu',\n",
    "        zmid=0,  # Center the color scale at 0\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=\"Correlation Heatmap\",\n",
    "        xaxis_title=\"Variable\",\n",
    "        yaxis_title=\"Variable\",\n",
    "    )\n",
    "\n",
    "    # add the figure to the result\n",
    "    result.add_figure(fig)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "result = run_test(\n",
    "    \"custom.CorrelationBetweenVariables\",\n",
    "    param_grid={\n",
    "        \"var1\": variables,\n",
    "        \"var2\": variables,\n",
    "    },\n",
    "    generate_description=False,\n",
    "    post_process_fn=create_heatmap,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-BbKYUwN1-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

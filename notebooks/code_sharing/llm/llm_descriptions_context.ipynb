{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Context to LLM-based Descriptions\n",
    "\n",
    "\n",
    "When running ValidMind tests, the LLM-based descriptions are generated using the test results, the test name, and the static test description provided in the test's docstring. While this metadata offers a valuable high-level overview of the test, the insights produced by the LLM-based descriptions may not always align with specific use cases or incorporate bank policy requirements. \n",
    "\n",
    "In this notebook, we will show how to add context to the LLM-based descriptions to provide additional information about the test or the use case. Providing use-case context is useful when you want to provide information about the intended use and technique of the model or the insitution policies and standards specific to a use case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the ValidMind Library\n",
    "\n",
    "To install the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q validmind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the ValidMind Library\n",
    "\n",
    "ValidMind generates a unique _code snippet_ for each registered model to connect with your developer environment. You initialize the ValidMind Library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\n",
    "\n",
    "<a id='toc3_1_'></a>\n",
    "\n",
    "### Get your code snippet\n",
    "\n",
    "1. In a browser, [log in to ValidMind](https://docs.validmind.ai/guide/configuration/log-in-to-validmind.html).\n",
    "\n",
    "2. In the left sidebar, navigate to **Model Inventory** and click **+ Register Model**.\n",
    "\n",
    "3. Enter the model details and click **Continue**. ([Need more help?](https://docs.validmind.ai/guide/model-inventory/register-models-in-inventory.html))\n",
    "\n",
    "   For example, to register a model for use with this notebook, select:\n",
    "\n",
    "   - Documentation template: `Binary classification`\n",
    "   - Use case: `Marketing/Sales - Attrition/Churn Management`\n",
    "\n",
    "   You can fill in other options according to your preference.\n",
    "\n",
    "4. Go to **Getting Started** and click **Copy snippet to clipboard**.\n",
    "\n",
    "Next, [load your model identifier credentials from an `.env` file](https://docs.validmind.ai/developer/model-documentation/store-credentials-in-env-file.html) or replace the placeholder with your own code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your model identifier credentials from an `.env` file\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv .env\n",
    "\n",
    "# Or replace with your code snippet\n",
    "\n",
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "  api_host = \"https://api.prod.validmind.ai/api/v1/tracking\",\n",
    "  api_key = \"...\",\n",
    "  api_secret = \"...\",\n",
    "  model = \"...\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Python environment\n",
    "\n",
    "Next, let's import the necessary libraries and set up your Python environment for data analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the sample dataset\n",
    "\n",
    "The sample dataset used here is provided by the ValidMind library. To be able to use it, you need to import the dataset and load it into a pandas [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), a two-dimensional tabular data structure that makes use of rows and columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sample dataset from the library\n",
    "\n",
    "from validmind.datasets.classification import customer_churn\n",
    "\n",
    "print(\n",
    "    f\"Loaded demo dataset with: \\n\\n\\t• Target column: '{customer_churn.target_column}' \\n\\t• Class labels: {customer_churn.class_labels}\"\n",
    ")\n",
    "\n",
    "raw_df = customer_churn.load_data()\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepocess the raw dataset\n",
    "\n",
    "Preprocessing performs a number of operations to get ready for the subsequent steps:\n",
    "\n",
    "- Preprocess the data: Splits the DataFrame (`df`) into multiple datasets (`train_df`, `validation_df`, and `test_df`) using `demo_dataset.preprocess` to simplify preprocessing.\n",
    "- Separate features and targets: Drops the target column to create feature sets (`x_train`, `x_val`) and target sets (`y_train`, `y_val`).\n",
    "- Initialize XGBoost classifier: Creates an `XGBClassifier` object with early stopping rounds set to 10.\n",
    "- Set evaluation metrics: Specifies metrics for model evaluation as \"error,\" \"logloss,\" and \"auc.\"\n",
    "- Fit the model: Trains the model on `x_train` and `y_train` using the validation set `(x_val, y_val)`. Verbose output is disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, validation_df, test_df = customer_churn.preprocess(raw_df)\n",
    "\n",
    "x_train = train_df.drop(customer_churn.target_column, axis=1)\n",
    "y_train = train_df[customer_churn.target_column]\n",
    "x_val = validation_df.drop(customer_churn.target_column, axis=1)\n",
    "y_val = validation_df[customer_churn.target_column]\n",
    "\n",
    "model = xgb.XGBClassifier(early_stopping_rounds=10)\n",
    "model.set_params(\n",
    "    eval_metric=[\"error\", \"logloss\", \"auc\"],\n",
    ")\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    eval_set=[(x_val, y_val)],\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the ValidMind objects\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the datasets\n",
    "\n",
    "Before you can run tests, you must first initialize a ValidMind dataset object using the [`init_dataset`](https://docs.validmind.ai/validmind/validmind.html#init_dataset) function from the ValidMind (`vm`) module.\n",
    "\n",
    "This function takes a number of arguments:\n",
    "\n",
    "- `dataset` — the raw dataset that you want to provide as input to tests\n",
    "- `input_id` - a unique identifier that allows tracking what inputs are used when running each individual test\n",
    "- `target_column` — a required argument if tests require access to true values. This is the name of the target column in the dataset\n",
    "- `class_labels` — an optional value to map predicted classes to class labels\n",
    "\n",
    "With all datasets ready, you can now initialize the raw, training and test datasets (`raw_df`, `train_df` and `test_df`) created earlier into their own dataset objects using [`vm.init_dataset()`](https://docs.validmind.ai/validmind/validmind.html#init_dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_raw_dataset = vm.init_dataset(\n",
    "    dataset=raw_df,\n",
    "    input_id=\"raw_dataset\",\n",
    "    target_column=customer_churn.target_column,\n",
    "    class_labels=customer_churn.class_labels,\n",
    ")\n",
    "\n",
    "vm_train_ds = vm.init_dataset(\n",
    "    dataset=train_df,\n",
    "    input_id=\"train_dataset\",\n",
    "    target_column=customer_churn.target_column,\n",
    ")\n",
    "\n",
    "vm_test_ds = vm.init_dataset(\n",
    "    dataset=test_df, input_id=\"test_dataset\", target_column=customer_churn.target_column\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a model object\n",
    "\n",
    "Additionally, you need to initialize a ValidMind model object (`vm_model`) that can be passed to other functions for analysis and tests on the data. You simply intialize this model object with [`vm.init_model()`](https://docs.validmind.ai/validmind/validmind.html#init_model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_model = vm.init_model(\n",
    "    model,\n",
    "    input_id=\"model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign predictions to the datasets\n",
    "\n",
    "We can now use the assign_predictions() method from the Dataset object to link existing predictions to any model. If no prediction values are passed, the method will compute predictions automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_train_ds.assign_predictions(\n",
    "    model=vm_model,\n",
    ")\n",
    "\n",
    "vm_test_ds.assign_predictions(\n",
    "    model=vm_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the LLM descriptions context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding no context to the LLM descriptions\n",
    "\n",
    "By default, the LLM descriptions context is disabled. This means that the LLM descriptions will not include any additional context. As a result, the LLM descriptions will be generated in isolation, without any additional context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.data_validation.DatasetDescription\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_raw_dataset,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding use case context to the LLM descriptions\n",
    "\n",
    "To enable the LLM descriptions context, you need to set the `VALIDMIND_LLM_DESCRIPTIONS_CONTEXT_ENABLED` environment variable to `1`. This will enable the LLM descriptions context, which will be used to provide additional context to the LLM descriptions. This is a global setting that will affect all tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"VALIDMIND_LLM_DESCRIPTIONS_CONTEXT_ENABLED\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_case_context = \"\"\"\n",
    "\n",
    "This is a customer churn prediction model for a banking loan application system using XGBoost classifier. \n",
    "\n",
    "Key Model Information:\n",
    "- Use Case: Predict customer churn risk during loan application process\n",
    "- Model Type: Binary classification using XGBoost\n",
    "- Critical Decision Point: Used in loan approval workflow\n",
    "\n",
    "Regulatory Requirements:\n",
    "- Subject to model risk management review and validation\n",
    "- Results require validation review for regulatory compliance\n",
    "- Model decisions directly impact loan approval process\n",
    "- Does this result raise any regulatory concerns?\n",
    "\n",
    "Validation Focus:\n",
    "- Explain strengths and weaknesses of the test and the context of whether the result is acceptable.\n",
    "- What does the result indicate about model reliability?\n",
    "- Is the result within acceptable thresholds for loan decisioning?\n",
    "- What are the implications for customer impact?\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "os.environ[\"VALIDMIND_LLM_DESCRIPTIONS_CONTEXT\"] = use_case_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.data_validation.DatasetDescription\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_raw_dataset,\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disabling the LLM descriptions context\n",
    "\n",
    "To disable the LLM descriptions context, you need to set the `VALIDMIND_LLM_DESCRIPTIONS_CONTEXT_ENABLED` environment variable to `0`. This will disable the LLM descriptions context, which will be used to provide additional context to the LLM descriptions. This is a global setting that will affect all tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"VALIDMIND_LLM_DESCRIPTIONS_CONTEXT_ENABLED\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.data_validation.DatasetDescription\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_raw_dataset,\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding test-specific context to the LLM descriptions\n",
    "\n",
    "We can also add test-specific context to the LLM descriptions. This is useful when you want to provide test-specific validation criteria about the test that is being run. All we need to do in this case is to set the `VALIDMIND_LLM_DESCRIPTIONS_CONTEXT_ENABLED` environment variable to `1` and join the test-specific context to the use-case context using the `VALIDMIND_LLM_DESCRIPTIONS_CONTEXT` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"VALIDMIND_LLM_DESCRIPTIONS_CONTEXT_ENABLED\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Description\n",
    "\n",
    "Rather than relying on generic dataset result descriptions in isolation, we use the context to specify precise thresholds for missing values, appropriate data types for banking variables (like `CreditScore` and `Balance`), and valid value ranges based on particular business rules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_context = \"\"\"\n",
    "\n",
    "Acceptance Criteria:\n",
    "- Missing Values: All critical features must have less than 5% missing values (including CreditScore, Balance, Age)\n",
    "- Data Types: All columns must have appropriate data types (numeric for CreditScore/Balance/Age, categorical for Geography/Gender)\n",
    "- Cardinality: Categorical variables must have fewer than 50 unique values, while continuous variables should show appropriate distinct value counts (e.g., high for EstimatedSalary, exactly 2 for Boolean fields)\n",
    "- Value Ranges: Numeric fields must fall within business-valid ranges (CreditScore: 300-850, Age: ≥18, Balance: ≥0)\n",
    "\"\"\".strip()\n",
    "\n",
    "context = f\"\"\"\n",
    "{use_case_context}\n",
    "\n",
    "{test_context}\n",
    "\"\"\".strip()\n",
    "\n",
    "os.environ[\"VALIDMIND_LLM_DESCRIPTIONS_CONTEXT\"] = context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.data_validation.DatasetDescription\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_raw_dataset,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Imbalance\n",
    "\n",
    "This context adds value to the LLM description by providing defined risk levels to assess class representation. By categorizing classes into Low, Medium, and High Risk, the LLM can generate more nuanced and actionable insights, ensuring that the analysis aligns with business requirements for balanced datasets. This approach not only highlights potential issues but also guides necessary documentation and mitigation strategies for high-risk classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_context = \"\"\"\n",
    "\n",
    "Acceptance Criteria:\n",
    "\n",
    "• Risk Levels for Class Representation:\n",
    "  - Low Risk: Each class represents 20% or more of the total dataset\n",
    "  - Medium Risk: Each class represents between 10% and 19.9% of the total dataset\n",
    "  - High Risk: Any class represents less than 10% of the total dataset\n",
    "\n",
    "• Overall Requirement:\n",
    "  - All classes must achieve at least Medium Risk status to pass\n",
    "\"\"\".strip()\n",
    "\n",
    "context = f\"\"\"\n",
    "{use_case_context}\n",
    "\n",
    "{test_context}\n",
    "\"\"\".strip()\n",
    "\n",
    "os.environ[\"VALIDMIND_LLM_DESCRIPTIONS_CONTEXT\"] = context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.data_validation.ClassImbalance\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_raw_dataset,\n",
    "    },\n",
    "    params={\n",
    "        \"min_percent_threshold\": 10,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High Cardinality\n",
    "\n",
    "In this case, the context specifies a risk-based criteria for the number of distinct values in categorical features. This helps the LLM to generate more nuanced and actionable insights, ensuring the descriptions are more relevant to the bank's policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_context = \"\"\"\n",
    "\n",
    "Acceptance Criteria:\n",
    "\n",
    "• Risk Levels for Distinct Values in Categorical Features:\n",
    "  - Low Risk: Each categorical column has fewer than 50 distinct values or less than 5% unique values relative to the total dataset size\n",
    "  - Medium Risk: Each categorical column has between 50 and 100 distinct values or between 5% and 10% unique values\n",
    "  - High Risk: Any categorical column has more than 100 distinct values or more than 10% unique values\n",
    "\n",
    "• Overall Requirement:\n",
    "  - All categorical columns must achieve at least Medium Risk status to pass\n",
    "\"\"\".strip()\n",
    "\n",
    "context = f\"\"\"\n",
    "{use_case_context}\n",
    "\n",
    "{test_context}\n",
    "\"\"\".strip()\n",
    "\n",
    "os.environ[\"VALIDMIND_LLM_DESCRIPTIONS_CONTEXT\"] = context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.data_validation.HighCardinality\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_raw_dataset,\n",
    "    },\n",
    "    params= {\n",
    "        \"num_threshold\": 100,\n",
    "        \"percent_threshold\": 0.1,\n",
    "        \"threshold_type\": \"percent\"\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values\n",
    "\n",
    "We use the test-specific context to establish differentiated risk thresholds across features. Rather than applying uniform criteria, the context allows for specific requirements for critical financial features (`CreditScore`, `Balance`, `Age`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_context = \"\"\"\n",
    "Test-Specific Context for Missing Values Analysis:\n",
    "\n",
    "Acceptance Criteria:\n",
    "\n",
    "• Risk Levels for Missing Values:\n",
    "  - Low Risk: Less than 1% missing values in any column\n",
    "  - Medium Risk: Between 1% and 5% missing values\n",
    "  - High Risk: More than 5% missing values\n",
    "\n",
    "• Feature-Specific Requirements:\n",
    "  - Critical Features (CreditScore, Balance, Age):\n",
    "    * Must maintain Low Risk status\n",
    "    * No missing values allowed\n",
    "  \n",
    "  - Secondary Features (Tenure, NumOfProducts, EstimatedSalary):\n",
    "    * Must achieve at least Medium Risk status\n",
    "    * Up to 3% missing values acceptable\n",
    "\n",
    "  - Categorical Features (Geography, Gender):\n",
    "    * Must achieve at least Medium Risk status\n",
    "    * Up to 5% missing values acceptable\n",
    "\"\"\".strip()\n",
    "\n",
    "context = f\"\"\"\n",
    "{use_case_context}\n",
    "\n",
    "{test_context}\n",
    "\"\"\".strip()\n",
    "\n",
    "os.environ[\"VALIDMIND_LLM_DESCRIPTIONS_CONTEXT\"] = context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.data_validation.MissingValues\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_raw_dataset,\n",
    "    },\n",
    "    params= {\n",
    "        \"min_threshold\": 1\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique Rows\n",
    "\n",
    "The test-specific context establishes variable-specific thresholds based on business expectations. Rather than applying uniform criteria, it recognizes that high variability is expected in features like `EstimatedSalary` (>90%) and `Balance` (>50%), while enforcing strict limits on categorical features like `Geography` (<5 values), ensuring meaningful validation aligned with banking data characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_context = \"\"\"\n",
    "\n",
    "Acceptance Criteria:\n",
    "\n",
    "• High-Variability Expected Features:\n",
    "  - EstimatedSalary: Must have >90% unique values\n",
    "  - Balance: Must have >50% unique values\n",
    "  - CreditScore: Must have between 5-10% unique values\n",
    "\n",
    "• Medium-Variability Features:\n",
    "  - Age: Should have between 0.5-2% unique values\n",
    "  - Tenure: Should have between 0.1-0.5% unique values\n",
    "\n",
    "• Low-Variability Features:\n",
    "  - Binary Features (HasCrCard, IsActiveMember, Gender, Exited): Must have exactly 2 unique values\n",
    "  - Geography: Must have fewer than 5 unique values\n",
    "  - NumOfProducts: Must have fewer than 10 unique values\n",
    "\n",
    "• Overall Requirements:\n",
    "  - Features must fall within their specified ranges to pass\n",
    "\"\"\".strip()\n",
    "\n",
    "context = f\"\"\"\n",
    "{use_case_context}\n",
    "\n",
    "{test_context}\n",
    "\"\"\".strip()\n",
    "\n",
    "os.environ[\"VALIDMIND_LLM_DESCRIPTIONS_CONTEXT\"] = context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.data_validation.UniqueRows\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_raw_dataset,\n",
    "    },\n",
    "    params= {\n",
    "        \"min_percent_threshold\": 1\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Too Many Zero Values\n",
    "\n",
    "The context in this case is used to provide meaning and expectations for different variables. For instance, zero values in `Balance` and `Tenure` indicate risk, whereas zeros in binary variables like `HasCrCard` or `IsActiveMember` are expected. This tailored context ensures that the analysis accurately reflects the business significance of zero values across different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_context = \"\"\"\n",
    "\n",
    "Acceptance Criteria:\n",
    "- Numerical Features Only: Test evaluates only continuous numeric columns (Balance, Tenure), \n",
    "  excluding binary columns (HasCrCard, IsActiveMember)\n",
    "\n",
    "- Risk Level Thresholds for Balance and Tenure:\n",
    "  - High Risk: More than 5% zero values\n",
    "  - Medium Risk: Between 3% and 5% zero values\n",
    "  - Low Risk: Less than 3% zero values\n",
    "\n",
    "- Individual Column Requirements:\n",
    "  - Balance: Must be Low Risk (banking context requires accurate balance tracking)\n",
    "  - Tenure: Must be Low or Medium Risk (some zero values acceptable for new customers)\n",
    "\n",
    "• Overall Test Result: Test must achieve \"Pass\" status (Low Risk) for Balance, and at least Medium Risk for Tenure\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "context = f\"\"\"\n",
    "{use_case_context}\n",
    "\n",
    "{test_context}\n",
    "\"\"\".strip()\n",
    "\n",
    "os.environ[\"VALIDMIND_LLM_DESCRIPTIONS_CONTEXT\"] = context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.data_validation.TooManyZeroValues\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_raw_dataset,\n",
    "    },\n",
    "    params= {\n",
    "        \"max_percent_threshold\": 0.03\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IQR Outliers Table\n",
    "\n",
    "In this case we use test-specific context to incorporate risk levels tailored to key variables like `CreditScore`, `Age`, and `NumOfProducts` that otherwise would not be considered for outlier analysis if we ran the test without context where all variables would be evaluated without any business criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_context = \"\"\"\n",
    "\n",
    "Acceptance Criteria:\n",
    "- Risk Levels for Outliers:\n",
    "    - Low Risk: 0-50 outliers\n",
    "    - Medium Risk: 51-300 outliers\n",
    "    - High Risk: More than 300 outliers\n",
    "- Feature-Specific Requirements:\n",
    "    - CreditScore, Age, NumOfProducts: Must maintain Low Risk status to ensure data quality and model reliability\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "context = f\"\"\"\n",
    "{use_case_context}\n",
    "\n",
    "{test_context}\n",
    "\"\"\".strip()\n",
    "\n",
    "os.environ[\"VALIDMIND_LLM_DESCRIPTIONS_CONTEXT\"] = context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.data_validation.IQROutliersTable\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_raw_dataset,\n",
    "    },\n",
    "    params= {\n",
    "        \"threshold\": 1.5\n",
    "        }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive Statistics\n",
    "\n",
    "The test-specific context is used in this case to provide risk-based thresholds aligned with the bank's policy. For instance, `CreditScore` ranges of 550-850 are considered low risk based on standard credit assessment practices, while `Balance` thresholds reflect typical retail banking ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_context = \"\"\"\n",
    "\n",
    "Acceptance Criteria:\n",
    "\n",
    "• CreditScore:\n",
    "  - Low Risk: 550-850\n",
    "  - Medium Risk: 450-549\n",
    "  - High Risk: <450 or missing\n",
    "  - Justification: Banking standards require reliable credit assessment\n",
    "\n",
    "• Age:\n",
    "  - Low Risk: 18-75\n",
    "  - Medium Risk: 76-85\n",
    "  - High Risk: >85 or <18\n",
    "  - Justification: Core banking demographic with age-appropriate products\n",
    "\n",
    "• Balance:\n",
    "  - Low Risk: 0-200,000\n",
    "  - Medium Risk: 200,001-250,000\n",
    "  - High Risk: >250,000\n",
    "  - Justification: Typical retail banking balance ranges\n",
    "\n",
    "• Tenure:\n",
    "  - Low Risk: 1-10 years\n",
    "  - Medium Risk: <1 year\n",
    "  - High Risk: 0 or >10 years\n",
    "  - Justification: Expected customer relationship duration\n",
    "\n",
    "• EstimatedSalary:\n",
    "  - Low Risk: 25,000-150,000\n",
    "  - Medium Risk: 150,001-200,000\n",
    "  - High Risk: <25,000 or >200,000\n",
    "  - Justification: Typical income ranges for retail banking customers\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "context = f\"\"\"\n",
    "{use_case_context}\n",
    "\n",
    "{test_context}\n",
    "\"\"\".strip()\n",
    "\n",
    "os.environ[\"VALIDMIND_LLM_DESCRIPTIONS_CONTEXT\"] = context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.data_validation.DescriptiveStatistics\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_raw_dataset,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson Correlation Matrix\n",
    "\n",
    "For this test, the context provides meaningful correlation ranges between specific variable pairs based on business criteria. For example, while a general correlation analysis might flag any correlation above 0.7 as concerning, the test-specific context specifies that `Balance` and `NumOfProducts` should maintain a negative correlation between -0.4 and 0, reflecting expected banking relationships. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_context = \"\"\"\n",
    "\n",
    "Acceptance Criteria:\n",
    "\n",
    "• Target Variable Correlations (Exited):\n",
    "  - Must show correlation coefficients between ±0.1 and ±0.3 with Age, CreditScore, and Balance\n",
    "  - Should not exceed ±0.2 correlation with other features\n",
    "  - Justification: Ensures predictive power while avoiding target leakage\n",
    "\n",
    "• Feature Correlations:\n",
    "  - Balance & NumOfProducts: Must maintain correlation between -0.4 and 0\n",
    "  - Age & Tenure: Should show positive correlation between 0.1 and 0.3\n",
    "  - CreditScore & Balance: Should maintain correlation between 0.1 and 0.3\n",
    "\n",
    "• Binary Feature Correlations:\n",
    "  - HasCreditCard & IsActiveMember: Must not exceed ±0.15 correlation\n",
    "  - Binary features should not show strong correlations (>±0.2) with continuous features\n",
    "\n",
    "• Overall Requirement:\n",
    "  - No feature pair should exceed ±0.7 correlation to avoid multicollinearity\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "context = f\"\"\"\n",
    "{use_case_context}\n",
    "\n",
    "{test_context}\n",
    "\"\"\".strip()\n",
    "\n",
    "os.environ[\"VALIDMIND_LLM_DESCRIPTIONS_CONTEXT\"] = context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.data_validation.PearsonCorrelationMatrix\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_raw_dataset,\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-py3.10",
   "language": "python",
   "name": "validmind-py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

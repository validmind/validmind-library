{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# G-Eval Integration for DeepEval within ValidMind\n",
        "\n",
        "Let's learn how to integrate [DeepEval](https://github.com/confident-ai/deepeval) with the ValidMind Library to evaluate Large Language Models (LLMs) and AI agents. \n",
        "Large Language Model (LLM) evaluation requires robust metrics to assess model outputs. G-Eval, a key feature of DeepEval, uses LLMs themselves to evaluate model responses across dimensions like factual accuracy, coherence, and relevance, etc. This notebook demonstrates how to leverage G-Eval metrics within ValidMind's testing infrastructure to create comprehensive, automated evaluations of LLM outputs.\n",
        "\n",
        "To integrate DeepEval with ValidMind, we'll:\n",
        " 1. Set up both frameworks and install required dependencies\n",
        " 2. Create a dataset with source texts and generated summaries\n",
        " 3. Analyze the evaluation results using G-eval custom metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Contents    \n",
        "- [Introduction](#toc1_)    \n",
        "  - [Before you begin](#toc2_1_)    \n",
        "  - [Key concepts](#toc2_2_)    \n",
        "- [Setting up](#toc3_)    \n",
        "  - [Install required packages](#toc3_1_)    \n",
        "  - [Initialize ValidMind](#toc3_2_)    \n",
        "- [Custom Metrics with G-Eval](#toc4_)    \n",
        "  - [Technical accuracy](#toc4_1_)    \n",
        "  - [Clarity and Comprehensiveness](#toc4_2_)    \n",
        "  - [Business Context Appropriateness](#toc4_3_)    \n",
        "  - [Tool Usage Appropriateness](#toc4_4_)    \n",
        "  - [Coherence Evaluation](#toc4_5_)    \n",
        "- [In summary](#toc5_)    \n",
        "- [Next steps](#toc6_)    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc1_\"></a>\n",
        "\n",
        "## Introduction\n",
        "**G-Eval** is a framework that uses large language models (LLMs) as evaluators—essentially treating an LLM as a “judge” to assess the quality of other LLM outputs. Instead of relying on traditional metrics like BLEU or ROUGE, G-Eval enables natural-language evaluation criteria (e.g., “rate how factual this summary is”). The framework guides the judge model through structured reasoning steps, producing more consistent, transparent, and interpretable scoring results. It is particularly effective for subjective or open-ended tasks such as summarization, dialogue generation, and content evaluation.\n",
        "\n",
        "Key advantages of G-Eval include:\n",
        "\n",
        "* **Structured reasoning:** Uses a step-by-step approach to improve reliability and reduce bias.\n",
        "* **Custom evaluation criteria:** Supports diverse factors like accuracy, tone, safety, or style.\n",
        "* **Enhanced consistency:** Provides more repeatable judgments than earlier LLM-as-a-judge methods.\n",
        "* **Production scalability:** Integrates easily with CI/CD pipelines via tools like *DeepEval*.\n",
        "* **Broader applicability:** Works across multiple domains and task types, from creative writing to factual QA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc2_1_\"></a>\n",
        "\n",
        "### Before you begin\n",
        "\n",
        "This notebook assumes you have basic familiarity with Python and Large Language Models. You'll need:\n",
        "\n",
        "- Python 3.8 or higher\n",
        "- Access to OpenAI API (for DeepEval metrics evaluation)\n",
        "- ValidMind account and model registration\n",
        "\n",
        "If you encounter errors due to missing modules, install them with `pip install` and re-run the notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc2_2_\"></a>\n",
        "\n",
        "### Key concepts\n",
        "\n",
        "**LLMTestCase**: A DeepEval object that represents a single test case with input, expected output, actual output, and optional context.\n",
        "\n",
        "**G-Eval**: Generative evaluation using LLMs to assess response quality based on custom criteria.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc3_\"></a>\n",
        "\n",
        "## Setting up\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc3_1_\"></a>\n",
        "\n",
        "### Install required packages\n",
        "\n",
        "First, let's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q validmind"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc3_2_\"></a>\n",
        "\n",
        "### Initialize ValidMind\n",
        "\n",
        "ValidMind generates a unique _code snippet_ for each registered model to connect with your developer environment. You initialize the ValidMind Library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\n",
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"background-color: #B5B5B510; color: black; border: 1px solid #083E44; border-left-width: 5px; box-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);border-radius: 5px;\"><span style=\"color: #083E44;\"><b>For access to all features available in this notebook, you'll need access to a ValidMind account.</b></span>\n",
        "<br></br>\n",
        "<a href=\"https://docs.validmind.ai/guide/configuration/register-with-validmind.html\" style=\"color: #DE257E;\"><b>Register with ValidMind</b></a></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load your model identifier credentials from an `.env` file\n",
        "%load_ext dotenv\n",
        "%dotenv .env\n",
        "\n",
        "# # Or replace with your code snippet\n",
        "import validmind as vm\n",
        "\n",
        "vm.init(\n",
        "    api_host=\"...\",\n",
        "    api_key=\"...\",\n",
        "    api_secret=\"...\",\n",
        "    model=\"...\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import warnings\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from deepeval.metrics.g_eval.utils import Rubric\n",
        "from deepeval.test_case import LLMTestCaseParams\n",
        "from validmind.datasets.llm import LLMAgentDataset\n",
        "import pandas as pd\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_rows', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Create test cases\n",
        "\n",
        "Let's create test cases to demonstrate the G-Eval custom metrics functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a test dataset for evaluating the custom metrics\n",
        "test_cases = [\n",
        "    LLMTestCase(\n",
        "        input=\"What is machine learning?\",\n",
        "        actual_output=\"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It uses statistical techniques to allow computers to find patterns in data.\",\n",
        "        context=[\"Machine learning is a branch of AI that focuses on building applications that learn from data and improve their accuracy over time without being programmed to do so.\"],\n",
        "        expected_output=\"Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.\"\n",
        "    ),  \n",
        "    LLMTestCase(\n",
        "        input=\"How do I implement a neural network?\",\n",
        "        actual_output=\"To implement a neural network, you need to: 1) Define the network architecture (layers, neurons), 2) Initialize weights and biases, 3) Implement forward propagation, 4) Calculate loss, 5) Perform backpropagation, and 6) Update weights using gradient descent.\",\n",
        "        context=[\"Neural networks are computing systems inspired by biological neural networks. They consist of layers of interconnected nodes that process and transmit signals.\"],\n",
        "        expected_output=\"Neural network implementation involves defining network architecture, initializing parameters, implementing forward and backward propagation, and using optimization algorithms for training.\"\n",
        "    )\n",
        "]\n",
        "\n",
        "# Create Agent dataset\n",
        "geval_dataset = LLMAgentDataset.from_test_cases(\n",
        "    test_cases=test_cases,\n",
        "    input_id=\"geval_dataset\"\n",
        ")\n",
        "geval_dataset._df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scorers in ValidMind\n",
        "\n",
        "Scorers are evaluation metrics that analyze model outputs and store their results in the dataset. When using `assign_scores()`:\n",
        "\n",
        "- For Geval scorer adds new columns (score, reason and criteria) to the dataset with format: `GEval_{metric_name}_score`, `GEval_{metric_name}_reason` and `GEval_{metric_name}_criteria`\n",
        "- The column contains the numeric score (typically 0-1) for each example\n",
        "- Multiple scorers can be run on the same dataset, each adding their own columns\n",
        "- Scores are persisted in the dataset for later analysis and visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"toc4_\"></a>\n",
        "\n",
        "## Custom Metrics with G-Eval\n",
        "One of DeepEval's most powerful features is the ability to create custom evaluation metrics using G-Eval (Generative Evaluation). This enables domain-specific evaluation criteria tailored to your use case.\n",
        "\n",
        "\n",
        "<a id=\"toc4_1_\"></a>\n",
        "\n",
        "### Technical accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name=\"Technical Accuracy\"\n",
        "criteria=\"\"\"Evaluate whether the response is technically accurate and uses appropriate \n",
        "terminology for the domain. Consider if the explanations are scientifically sound \n",
        "and if technical concepts are explained correctly.\"\"\"\n",
        "threshold=0.8\n",
        "geval_dataset.assign_scores(\n",
        "    metrics = \"validmind.scorer.llm.deepeval.GEval\",\n",
        "    metric_name=name, \n",
        "    criteria = criteria,\n",
        "    threshold=threshold,\n",
        "    evaluation_params={\n",
        "        LLMTestCaseParams.INPUT: \"input\",\n",
        "        LLMTestCaseParams.ACTUAL_OUTPUT: \"actual_output\",\n",
        "    }\n",
        ")\n",
        "geval_dataset._df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"toc4_2_\"></a>\n",
        "\n",
        "### Clarity and Comprehensiveness\n",
        "This evaluation assesses the clarity and comprehensiveness of responses, focusing on how well-structured and understandable they are. The criteria examines whether responses are logically organized, address all aspects of questions thoroughly, and maintain an appropriate level of detail without being overly verbose.\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name=\"Clarity and Comprehensiveness\"\n",
        "criteria=\"\"\"Evaluate the clarity, structure, and comprehensiveness of the actual output \n",
        "in relation to the expected output. The response should be clear, well-organized, and \n",
        "comparable in coverage to the expected output, addressing all relevant aspects without \n",
        "being overly verbose. Deduct points if important points or details present in the expected \n",
        "output are missing or inaccurately conveyed in the actual output.\"\"\"\n",
        "threshold=0.75\n",
        "\n",
        "geval_dataset.assign_scores(\n",
        "    metrics = \"validmind.scorer.llm.deepeval.GEval\",\n",
        "    metric_name=name, \n",
        "    criteria = criteria,\n",
        "    threshold=threshold,\n",
        "    evaluation_params={\n",
        "        LLMTestCaseParams.INPUT: \"input\",\n",
        "        LLMTestCaseParams.ACTUAL_OUTPUT: \"actual_output\",\n",
        "        LLMTestCaseParams.EXPECTED_OUTPUT: \"expected_output\",\n",
        "    }\n",
        ")\n",
        "geval_dataset._df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"toc4_3_\"></a>\n",
        "\n",
        "### Business Context Appropriateness\n",
        "\n",
        "This evaluation assesses whether responses are appropriate for a business context, considering factors like professional tone, business relevance, and actionable insights. The criteria focuses on ensuring content would be valuable and applicable for business users.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name=\"Business Context Appropriateness\"\n",
        "criteria=\"\"\"Evaluate whether the response is appropriate for a business context. \n",
        "Consider if the tone is professional, if the content is relevant to business needs, \n",
        "and if it provides actionable information that would be valuable to a business user.\"\"\"\n",
        "threshold=0.7\n",
        "geval_dataset.assign_scores(\n",
        "    metrics = \"validmind.scorer.llm.deepeval.GEval\",\n",
        "    metric_name=name, \n",
        "    criteria = criteria,\n",
        "    threshold=threshold,\n",
        "    evaluation_params={\n",
        "        LLMTestCaseParams.INPUT: \"input\",\n",
        "        LLMTestCaseParams.ACTUAL_OUTPUT: \"actual_output\",\n",
        "        LLMTestCaseParams.EXPECTED_OUTPUT: \"expected_output\",\n",
        "    }\n",
        ")\n",
        "geval_dataset._df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"toc4_5_\"></a>\n",
        "\n",
        "### Conciseness Evaluation\n",
        "This evaluation assesses how well the responses flow and connect logically. It examines whether the content builds naturally from sentence to sentence to form a coherent narrative, rather than just being a collection of related but disconnected information. The evaluation considers factors like fluency, logical progression, and overall readability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "criteria = \"\"\"\n",
        "    Evaluate the conciseness of the generation on a continuous scale from 0 to 1.\n",
        "    A generation can be considered concise (Score: 1) if it directly and succinctly\n",
        "    answers the question posed, focusing specifically on the information requested\n",
        "    without including unnecessary, irrelevant, or excessive details.\"\"\"\n",
        "\n",
        "evaluation_steps=[\n",
        "        \"Read the input and identify which pieces of information need to be conveyed.\"\n",
        "        \"Read the actual_output and check if it includes all the required information.\",\n",
        "        \"Check if the actual_output excludes irrelevant details or redundancies.\",\n",
        "        \"Check if the wording is as brief as possible while still being clear and complete.\",\n",
        "        \"Assign a score (e.g., 0-10) based on how well the actual_output meets the above.\"\n",
        "    ]\n",
        "\n",
        "rubric=[\n",
        "        Rubric(score_range=(0, 1), expected_outcome=\"Very poor Conciseness\"),\n",
        "        Rubric(score_range=(2, 3), expected_outcome=\"Poor Conciseness\"),\n",
        "        Rubric(score_range=(4, 5), expected_outcome=\"Fair Conciseness\"),\n",
        "        Rubric(score_range=(6, 7), expected_outcome=\"Good Conciseness\"),\n",
        "        Rubric(score_range=(8, 10), expected_outcome=\"Excellent Conciseness\"),\n",
        "    ]\n",
        "\n",
        "geval_dataset.assign_scores(\n",
        "    metrics = \"validmind.scorer.llm.deepeval.GEval\",\n",
        "    metric_name=\"Conciseness\", \n",
        "    criteria = criteria,\n",
        "    rubric=rubric,\n",
        "    evaluation_steps=evaluation_steps,\n",
        "    evaluation_params={\n",
        "        LLMTestCaseParams.INPUT: \"input\",\n",
        "        LLMTestCaseParams.ACTUAL_OUTPUT: \"actual_output\",\n",
        "    }\n",
        ")\n",
        "geval_dataset._df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's plot all of these metrics together in a Boxplot Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vm.tests.run_test(\n",
        "    \"validmind.plots.BoxPlot\",\n",
        "    inputs={\"dataset\": geval_dataset},\n",
        "    params={\n",
        "        \"columns\": [\n",
        "            \"GEval_Technical_Accuracy_score\",\n",
        "            \"GEval_Clarity_and_Comprehensiveness_score\",\n",
        "            \"GEval_Business_Context_Appropriateness_score\",\n",
        "            \"GEval_Conciseness_score\"\n",
        "        ],\n",
        "        \"title\": \"Distribution of G-Eval Scores\",\n",
        "        \"ylabel\": \"Score\",\n",
        "    }\n",
        ").log()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc6_\"></a>\n",
        "\n",
        "## Next steps\n",
        "\n",
        "**Explore Advanced Features:**\n",
        "- **Continuous Evaluation**: Set up automated LLM evaluation pipelines\n",
        "- **Metrics Customization**: Create domain-specific evaluation criteria\n",
        "- **Integration Patterns**: Embed evaluation into your LLM development workflow\n",
        "\n",
        "**Additional Resources:**\n",
        "- [ValidMind Library Documentation](https://docs.validmind.ai/developer/validmind-library.html) - Complete API reference and tutorials\n",
        "\n",
        "**Try These Examples:**\n",
        "- Implement custom business-specific evaluation metrics\n",
        "- Create automated evaluation pipelines for model deployment\n",
        "- Integrate with your existing ML infrastructure and workflows\n",
        "- Explore multi-modal evaluation scenarios (text, code, images)\n",
        "\n",
        "Start building comprehensive LLM evaluation workflows that combine the power of DeepEval's specialized metrics with ValidMind's structured testing and documentation framework.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<small>\n",
        "\n",
        "***\n",
        "\n",
        "Copyright © 2023-2026 ValidMind Inc. All rights reserved.<br>\n",
        "Refer to [LICENSE in the root of the GitHub `validmind-library` repository](https://github.com/validmind/validmind-library/blob/main/LICENSE) for details.<br>\n",
        "SPDX-License-Identifier: AGPL-3.0 AND ValidMind Commercial</small>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ValidMind Library",
      "language": "python",
      "name": "validmind"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

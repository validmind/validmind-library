{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# G-Eval Integration for DeepEval within ValidMind\n",
        "\n",
        "Let's learn how to integrate [DeepEval](https://github.com/confident-ai/deepeval) with the ValidMind Library to evaluate Large Language Models (LLMs) and AI agents. This notebook demonstrates how to use DeepEval's G-eval custom evaluation metrics within ValidMind's testing infrastructure.\n",
        "\n",
        "To integrate DeepEval with ValidMind, we'll:\n",
        " 1. Set up both frameworks and install required dependencies\n",
        " 2. Create a dataset with source texts and generated summaries\n",
        " 3. Analyze the evaluation results using G-eval custom metrics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Contents    \n",
        "- [Introduction](#toc1_)    \n",
        "- [About DeepEval Integration](#toc2_)    \n",
        "  - [Before you begin](#toc2_1_)    \n",
        "  - [Key concepts](#toc2_2_)    \n",
        "- [Setting up](#toc3_)    \n",
        "  - [Install required packages](#toc3_1_)    \n",
        "  - [Initialize ValidMind](#toc3_2_)    \n",
        "- [Custom Metrics with G-Eval](#toc4_)    \n",
        "  - [Technical accuracy](#toc4_1_)    \n",
        "  - [Clarity and Comprehensiveness](#toc4_2_)    \n",
        "  - [Business Context Appropriateness](#toc4_3_)    \n",
        "  - [Tool Usage Appropriateness](#toc4_4_)    \n",
        "  - [Coherence Evaluation](#toc4_5_)    \n",
        "- [In summary](#toc5_)    \n",
        "- [Next steps](#toc6_)    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc1_\"></a>\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Large Language Model (LLM) evaluation is critical for understanding model performance across different tasks and scenarios. This notebook demonstrates how to integrate DeepEval's comprehensive evaluation framework with ValidMind's testing infrastructure to create a robust LLM evaluation pipeline.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc2_\"></a>\n",
        "\n",
        "## About DeepEval Integration\n",
        "\n",
        "DeepEval is a comprehensive evaluation framework for LLMs that provides metrics for various scenarios including hallucination detection, answer relevancy, faithfulness, and custom evaluation criteria. ValidMind is a platform for managing model risk and documentation through automated testing.\n",
        "\n",
        "Together, these tools enable comprehensive LLM evaluation within a structured, compliant framework.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc2_1_\"></a>\n",
        "\n",
        "### Before you begin\n",
        "\n",
        "This notebook assumes you have basic familiarity with Python and Large Language Models. You'll need:\n",
        "\n",
        "- Python 3.8 or higher\n",
        "- Access to OpenAI API (for DeepEval metrics evaluation)\n",
        "- ValidMind account and model registration\n",
        "\n",
        "If you encounter errors due to missing modules, install them with `pip install` and re-run the notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc2_2_\"></a>\n",
        "\n",
        "### Key concepts\n",
        "\n",
        "**LLMTestCase**: A DeepEval object that represents a single test case with input, expected output, actual output, and optional context.\n",
        "\n",
        "**G-Eval**: Generative evaluation using LLMs to assess response quality based on custom criteria.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc3_\"></a>\n",
        "\n",
        "## Setting up\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc3_1_\"></a>\n",
        "\n",
        "### Install required packages\n",
        "\n",
        "First, let's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q validmind"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc3_2_\"></a>\n",
        "\n",
        "### Initialize ValidMind\n",
        "\n",
        "ValidMind generates a unique _code snippet_ for each registered model to connect with your developer environment. You initialize the ValidMind Library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\n",
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"background-color: #B5B5B510; color: black; border: 1px solid #083E44; border-left-width: 5px; box-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);border-radius: 5px;\"><span style=\"color: #083E44;\"><b>For access to all features available in this notebook, you'll need access to a ValidMind account.</b></span>\n",
        "<br></br>\n",
        "<a href=\"https://docs.validmind.ai/guide/configuration/register-with-validmind.html\" style=\"color: #DE257E;\"><b>Register with ValidMind</b></a></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load your model identifier credentials from an `.env` file\n",
        "%load_ext dotenv\n",
        "%dotenv .env\n",
        "\n",
        "# Or replace with your code snippet\n",
        "import validmind as vm\n",
        "\n",
        "vm.init(\n",
        "    api_host=\"...\",\n",
        "    api_key=\"...\",\n",
        "    api_secret=\"...\",\n",
        "    model=\"...\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from deepeval.test_case import LLMTestCase, ToolCall\n",
        "from deepeval.dataset import Golden\n",
        "from validmind.datasets.llm import LLMAgentDataset\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Create test cases\n",
        "\n",
        "Let's create test cases to demonstrate the G-Eval custom metrics functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a test dataset for evaluating the custom metrics\n",
        "test_cases = [\n",
        "    LLMTestCase(\n",
        "        input=\"What is machine learning?\",\n",
        "        actual_output=\"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It uses statistical techniques to allow computers to find patterns in data.\",\n",
        "        context=[\"Machine learning is a branch of AI that focuses on building applications that learn from data and improve their accuracy over time without being programmed to do so.\"],\n",
        "        expected_output=\"Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.\"\n",
        "    ),  \n",
        "    LLMTestCase(\n",
        "        input=\"How do I implement a neural network?\",\n",
        "        actual_output=\"To implement a neural network, you need to: 1) Define the network architecture (layers, neurons), 2) Initialize weights and biases, 3) Implement forward propagation, 4) Calculate loss, 5) Perform backpropagation, and 6) Update weights using gradient descent.\",\n",
        "        context=[\"Neural networks are computing systems inspired by biological neural networks. They consist of layers of interconnected nodes that process and transmit signals.\"],\n",
        "        expected_output=\"Neural network implementation involves defining network architecture, initializing parameters, implementing forward and backward propagation, and using optimization algorithms for training.\"\n",
        "    )\n",
        "]\n",
        "\n",
        "# Create Agent dataset\n",
        "geval_dataset = LLMAgentDataset.from_test_cases(\n",
        "    test_cases=test_cases,\n",
        "    input_id=\"geval_dataset\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"toc4_\"></a>\n",
        "\n",
        "## Custom Metrics with G-Eval\n",
        "\n",
        "One of DeepEval's most powerful features is the ability to create custom evaluation metrics using G-Eval (Generative Evaluation). This enables domain-specific evaluation criteria tailored to your use case.\n",
        "\n",
        "<a id=\"toc4_1_\"></a>\n",
        "\n",
        "### Technical accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name=\"Technical Accuracy\"\n",
        "criteria=\"\"\"Evaluate whether the response is technically accurate and uses appropriate \n",
        "terminology for the domain. Consider if the explanations are scientifically sound \n",
        "and if technical concepts are explained correctly.\"\"\"\n",
        "threshold=0.8\n",
        "geval_dataset.assign_scores(\n",
        "    metrics = \"validmind.scorer.llm.deepeval.GEval\",\n",
        "    metric_name=name, \n",
        "    criteria = criteria,\n",
        "    threshold=threshold\n",
        ")\n",
        "geval_dataset._df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"toc4_2_\"></a>\n",
        "\n",
        "### Clarity and Comprehensiveness\n",
        "This evaluation assesses the clarity and comprehensiveness of responses, focusing on how well-structured and understandable they are. The criteria examines whether responses are logically organized, address all aspects of questions thoroughly, and maintain an appropriate level of detail without being overly verbose.\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name=\"Clarity and Comprehensiveness\"\n",
        "criteria=\"\"\"Assess whether the response is clear, well-structured, and comprehensive. \n",
        "The response should be easy to understand, logically organized, and address all \n",
        "aspects of the user's question without being overly verbose.\"\"\"\n",
        "threshold=0.75\n",
        "\n",
        "geval_dataset.assign_scores(\n",
        "    metrics = \"validmind.scorer.llm.deepeval.GEval\",\n",
        "    metric_name=name, \n",
        "    criteria = criteria,\n",
        "    threshold=threshold\n",
        ")\n",
        "geval_dataset._df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"toc4_3_\"></a>\n",
        "\n",
        "### Business Context Appropriateness\n",
        "\n",
        "This evaluation assesses whether responses are appropriate for a business context, considering factors like professional tone, business relevance, and actionable insights. The criteria focuses on ensuring content would be valuable and applicable for business users.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name=\"Business Context Appropriateness\"\n",
        "criteria=\"\"\"Evaluate whether the response is appropriate for a business context. \n",
        "Consider if the tone is professional, if the content is relevant to business needs, \n",
        "and if it provides actionable information that would be valuable to a business user.\"\"\"\n",
        "threshold=0.7\n",
        "geval_dataset.assign_scores(\n",
        "    metrics = \"validmind.scorer.llm.deepeval.GEval\",\n",
        "    metric_name=name, \n",
        "    criteria = criteria,\n",
        "    threshold=threshold\n",
        ")\n",
        "geval_dataset._df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name=\"Tool Usage Appropriateness\"\n",
        "criteria=\"\"\"Evaluate whether the agent used appropriate tools for the given task. \n",
        "Consider if the tools were necessary, if they were used correctly, and if the \n",
        "agent's reasoning for tool selection was sound.\"\"\"\n",
        "threshold=0.8\n",
        "geval_dataset.assign_scores(\n",
        "    metrics = \"validmind.scorer.llm.deepeval.GEval\",\n",
        "    metric_name=name, \n",
        "    criteria = criteria,\n",
        "    threshold=threshold\n",
        ")\n",
        "geval_dataset._df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"toc4_5_\"></a>\n",
        "\n",
        "### Coherence Evaluation\n",
        "This evaluation assesses how well the responses flow and connect logically. It examines whether the content builds naturally from sentence to sentence to form a coherent narrative, rather than just being a collection of related but disconnected information. The evaluation considers factors like fluency, logical progression, and overall readability.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "criteria = \"\"\"Coherence (1-5) - the collective quality of all sentences. We align this dimension with\n",
        "the DUC quality question of structure and coherence whereby the summary should be\n",
        "well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to sentence to a coherent body of information about a topic.\"\"\"\n",
        "\n",
        "evaluation_steps=[\n",
        "        \"Read the news article carefully and identify the main topic and key points.\",\n",
        "        \"Read the summary and compare it to the news article. Check if the summary covers the main topic and key points of the news article, and if it presents them in a clear and logical order.\",\n",
        "        \"Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.\"\n",
        "    ]\n",
        "\n",
        "rubrics = [\n",
        "      {\n",
        "          \"score\":0, \n",
        "          \"criteria\":\"Measure the fluency of the actual output.\",\n",
        "          \"expected_outcome\": \"The output should be fluent and natural sounding\"\n",
        "      },\n",
        "      {\n",
        "          \"score\":2, \n",
        "          \"criteria\":\"Measure the logical flow of the actual output.\",\n",
        "          \"expected_outcome\": \"The output should flow logically from one point to the next\"\n",
        "      },\n",
        "      {\n",
        "          \"score\":3, \n",
        "          \"criteria\":\"Measure the linguistic flow of the actual output.\",\n",
        "          \"expected_outcome\": \"The output should have good linguistic structure and readability\"\n",
        "      }\n",
        "]\n",
        "\n",
        "geval_dataset.assign_scores(\n",
        "    metrics = \"validmind.scorer.llm.deepeval.GEval\",\n",
        "    metric_name=\"Coherence\", \n",
        "    criteria = criteria,\n",
        "    input_column=\"context\",\n",
        ")\n",
        "geval_dataset._df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc6_\"></a>\n",
        "\n",
        "## Next steps\n",
        "\n",
        "**Explore Advanced Features:**\n",
        "- **Continuous Evaluation**: Set up automated LLM evaluation pipelines\n",
        "- **Metrics Customization**: Create domain-specific evaluation criteria\n",
        "- **Integration Patterns**: Embed evaluation into your LLM development workflow\n",
        "\n",
        "**Additional Resources:**\n",
        "- [ValidMind Library Documentation](https://docs.validmind.ai/developer/validmind-library.html) - Complete API reference and tutorials\n",
        "\n",
        "**Try These Examples:**\n",
        "- Implement custom business-specific evaluation metrics\n",
        "- Create automated evaluation pipelines for model deployment\n",
        "- Integrate with your existing ML infrastructure and workflows\n",
        "- Explore multi-modal evaluation scenarios (text, code, images)\n",
        "\n",
        "Start building comprehensive LLM evaluation workflows that combine the power of DeepEval's specialized metrics with ValidMind's structured testing and documentation framework.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ValidMind Library",
      "language": "python",
      "name": "validmind"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

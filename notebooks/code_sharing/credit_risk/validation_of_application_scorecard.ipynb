{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of an application scorecard model \n",
    "\n",
    "As a model validator, your task is to independently assess the application scorecard model developed using the ValidMind Library, which is based on Kaggle's [Lending Club](https://www.kaggle.com/datasets/devanshi23/loan-data-2007-2014/data. Your role focuses on evaluating the model developer's work by conducting thorough testing and validation, potentially including the use of challenger models to benchmark performance.\n",
    "\n",
    "An application scorecard model is a type of statistical model used in credit scoring to evaluate the creditworthiness of potential borrowers by generating a score based on various characteristics of an applicant such as credit history, income, employment status, and other relevant financial data.\n",
    " - This score assists lenders in making informed decisions about whether to approve or reject loan applications, as well as in determining the terms of the loan, including interest rates and credit limits.\n",
    " - Effective validation of application scorecard models ensures that lenders can manage risk efficiently while maintaining a fast and transparent loan application process for applicants.\n",
    "\n",
    "This interactive notebook provides a step-by-step guide for:\n",
    "\n",
    " - Loading the developer model and verifying the data quality steps performed by the model developer.\n",
    " - Independently replicating the model's results and conducting additional tests to assess performance, stability, and robustness.\n",
    " - Setting up test inputs and challenger models for comparative analysis.\n",
    " - Running validation tests, analyzing results, and logging findings to ValidMind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc1_'></a>\n",
    "\n",
    "## About ValidMind\n",
    "ValidMind is a suite of tools for managing model risk, including risk associated with AI and statistical models.\n",
    "\n",
    "You use the ValidMind Library to automate documentation and validation tests, and then use the ValidMind Platform to collaborate on model documentation. Together, these products simplify model risk management, facilitate compliance with regulations and institutional standards, and enhance collaboration between yourself and model validators.\n",
    "\n",
    "<a id='toc1_1_'></a>\n",
    "\n",
    "### Before you begin\n",
    "This notebook assumes you have basic familiarity with Python, including an understanding of how functions work. If you are new to Python, you can still run the notebook but we recommend further familiarizing yourself with the language.\n",
    "\n",
    "If you encounter errors due to missing modules in your Python environment, install the modules with `pip install`, and then re-run the notebook. For more help, refer to [Installing Python Modules](https://docs.python.org/3/installing/index.html).\n",
    "\n",
    "<a id='toc1_2_'></a>\n",
    "\n",
    "### New to ValidMind?\n",
    "If you haven't already seen our [Get started with the ValidMind Library](https://docs.validmind.ai/developer/get-started-validmind-library.html), we recommend you begin by exploring the available resources in this section. There, you can learn more about documenting models, find code samples, or read our developer reference.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: #B5B5B510; color: black; border: 1px solid #083E44; border-left-width: 5px; box-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);border-radius: 5px;\"><span style=\"color: #083E44;\"><b>For access to all features available in this notebook, create a free ValidMind account.</b></span>\n",
    "<br></br>\n",
    "Signing up is FREE — <a href=\"https://docs.validmind.ai/guide/configuration/register-with-validmind.html\" style=\"color: #DE257E;\"><b>Register with ValidMind</b></a></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc1_3_'></a>\n",
    "\n",
    "### Key concepts\n",
    "\n",
    "**Model documentation**: A structured and detailed record pertaining to a model, encompassing key components such as its underlying assumptions, methodologies, data sources, inputs, performance metrics, evaluations, limitations, and intended uses. It serves to ensure transparency, adherence to regulatory requirements, and a clear understanding of potential risks associated with the model’s application.\n",
    "\n",
    "**Documentation template**: Functions as a test suite and lays out the structure of model documentation, segmented into various sections and sub-sections. Documentation templates define the structure of your model documentation, specifying the tests that should be run, and how the results should be displayed.\n",
    "\n",
    "**Tests**: A function contained in the ValidMind Library, designed to run a specific quantitative test on the dataset or model. Tests are the building blocks of ValidMind, used to evaluate and document models and datasets, and can be run individually or as part of a suite defined by your model documentation template.\n",
    "\n",
    "**Custom tests**: Custom tests are functions that you define to evaluate your model or dataset. These functions can be registered via the ValidMind Library to be used with the ValidMind Platform.\n",
    "\n",
    "**Inputs**: Objects to be evaluated and documented in the ValidMind Library. They can be any of the following:\n",
    "\n",
    "- **model**: A single model that has been initialized in ValidMind with [`vm.init_model()`](https://docs.validmind.ai/validmind/validmind.html#init_model).\n",
    "- **dataset**: Single dataset that has been initialized in ValidMind with [`vm.init_dataset()`](https://docs.validmind.ai/validmind/validmind.html#init_dataset).\n",
    "- **models**: A list of ValidMind models - usually this is used when you want to compare multiple models in your custom test.\n",
    "- **datasets**: A list of ValidMind datasets - usually this is used when you want to compare multiple datasets in your custom test. See this [example](https://docs.validmind.ai/notebooks/how_to/run_tests_that_require_multiple_datasets.html) for more information.\n",
    "\n",
    "**Parameters**: Additional arguments that can be passed when running a ValidMind test, used to pass additional information to a test, customize its behavior, or provide additional context.\n",
    "\n",
    "**Outputs**: Custom tests can return elements like tables or plots. Tables may be a list of dictionaries (each representing a row) or a pandas DataFrame. Plots may be matplotlib or plotly figures.\n",
    "\n",
    "**Test suites**: Collections of tests designed to run together to automate and generate model documentation end-to-end for specific use-cases.\n",
    "\n",
    "Example: The [`classifier_full_suite`](https://docs.validmind.ai/validmind/validmind/test_suites/classifier.html#ClassifierFullSuite) test suite runs tests from the [`tabular_dataset`](https://docs.validmind.ai/validmind/validmind/test_suites/tabular_datasets.html) and [`classifier`](https://docs.validmind.ai/validmind/validmind/test_suites/classifier.html) test suites to fully document the data and model sections for binary classification model use-cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc2_'></a>\n",
    "\n",
    "## Install the ValidMind Library\n",
    "\n",
    "To install the library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q validmind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"VALIDMIND_LLM_DESCRIPTIONS_CONTEXT_ENABLED\"] = \"1\"\n",
    "\n",
    "context = \"\"\"\n",
    "FORMAT FOR THE LLM DESCRIPTIONS: \n",
    "    **<Test Name>** is designed to <begin with a concise overview of what the test does and its primary purpose, \n",
    "    extracted from the test description>.\n",
    "\n",
    "    The test operates by <write a paragraph about the test mechanism, explaining how it works and what it measures. \n",
    "    Include any relevant formulas or methodologies mentioned in the test description.>\n",
    "\n",
    "    The primary advantages of this test include <write a paragraph about the test's strengths and capabilities, \n",
    "    highlighting what makes it particularly useful for specific scenarios.>\n",
    "\n",
    "    Users should be aware that <write a paragraph about the test's limitations and potential risks. \n",
    "    Include both technical limitations and interpretation challenges. \n",
    "    If the test description includes specific signs of high risk, incorporate these here.>\n",
    "\n",
    "    **Key Insights:**\n",
    "\n",
    "    The test results reveal:\n",
    "\n",
    "    - **<insight title>**: <comprehensive description of one aspect of the results>\n",
    "    - **<insight title>**: <comprehensive description of another aspect>\n",
    "    ...\n",
    "\n",
    "    Based on these results, <conclude with a brief paragraph that ties together the test results with the test's \n",
    "    purpose and provides any final recommendations or considerations.>\n",
    "\n",
    "ADDITIONAL INSTRUCTIONS:\n",
    "    Present insights in order from general to specific, with each insight as a single bullet point with bold title.\n",
    "    You are a model validator and the goal is to identify risk and/or suggest room for improvements or recommendations on what Model Developer should do in order to improve outcomes and reduce risk\n",
    "\n",
    "    For each metric in the test results, include in the test overview:\n",
    "    - The metric's purpose and what it measures\n",
    "    - Its mathematical formula\n",
    "    - The range of possible values\n",
    "    - What constitutes good/bad performance\n",
    "    - How to interpret different values\n",
    "\n",
    "    Each insight should progressively cover:\n",
    "    1. Overall scope and distribution\n",
    "    2. Complete breakdown of all elements with specific values\n",
    "    3. Natural groupings and patterns\n",
    "    4. Comparative analysis between datasets/categories\n",
    "    5. Stability and variations\n",
    "    6. Notable relationships or dependencies\n",
    "\n",
    "    Remember:\n",
    "    - Keep all insights at the same level (no sub-bullets or nested structures)\n",
    "    - Make each insight complete and self-contained\n",
    "    - Include specific numerical values and ranges\n",
    "    - Cover all elements in the results comprehensively\n",
    "    - Maintain clear, concise language\n",
    "    - Use only \"- **Title**: Description\" format for insights\n",
    "    - Progress naturally from general to specific observations\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "os.environ[\"VALIDMIND_LLM_DESCRIPTIONS_CONTEXT\"] = context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc3_'></a>\n",
    "\n",
    "## Initialize the ValidMind Library\n",
    "\n",
    "ValidMind generates a unique _code snippet_ for each registered model to connect with your developer environment. You initialize the ValidMind Library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\n",
    "\n",
    "<a id='toc3_1_'></a>\n",
    "\n",
    "### Get your code snippet\n",
    "\n",
    "1. In a browser, [log in to ValidMind](https://docs.validmind.ai/guide/configuration/log-in-to-validmind.html).\n",
    "\n",
    "2. In the left sidebar, navigate to **Model Inventory** and click **+ Register Model**.\n",
    "\n",
    "3. Enter the model details and click **Continue**. ([Need more help?](https://docs.validmind.ai/guide/model-inventory/register-models-in-inventory.html))\n",
    "\n",
    "   For example, to register a model for use with this notebook, select:\n",
    "\n",
    "   - Documentation template: `Credit Risk Scorecard`\n",
    "   - Use case: `Credit Risk - CECL`\n",
    "\n",
    "   You can fill in other options according to your preference.\n",
    "\n",
    "4. Go to **Getting Started** and click **Copy snippet to clipboard**.\n",
    "\n",
    "Next, [load your model identifier credentials from an `.env` file](https://docs.validmind.ai/developer/model-documentation/store-credentials-in-env-file.html) or replace the placeholder with your own code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your model identifier credentials from an `.env` file\n",
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    ",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc4_'></a>\n",
    "\n",
    "## Initialize the Python environment and import Model Developer Model\n",
    "\n",
    "Next, let's import the Model Developer model, used as the model developer as the champion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "#Load the saved model\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.load_model(\"xgb_model_champion.pkl\")\n",
    "xgb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure that we have to appropriate order in feature names from Champion model and dataset\n",
    "cols_when_model_builds = xgb_model.get_booster().feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc4_1_'></a>\n",
    "\n",
    "### Preview the Validation Report Template\n",
    "\n",
    "A template predefines sections for your model documentation and provides a general outline to follow, making the documentation process much easier.\n",
    "\n",
    "You'll upload documentation and test results into this template later on. For now, take a look at the structure that the template provides with the `vm.preview_template()` function from the ValidMind library and note the empty sections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.preview_template()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc5_'></a>\n",
    "\n",
    "## Load the sample dataset that Model Developer provided to the Validation Team, that was used to develop, train and test the model.\n",
    "\n",
    "The sample dataset used here is provided by the ValidMind library. To be able to use it, you'll need to import the dataset and load it into a pandas [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), a two-dimensional tabular data structure that makes use of rows and columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.datasets.credit_risk import lending_club\n",
    "\n",
    "df = lending_club.load_data(source=\"offline\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc5_1_'></a>\n",
    "\n",
    "### Obtain the Prepocessed dataset from Model Developer for data quality testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_df = lending_club.preprocess(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc5_2_'></a>\n",
    "\n",
    "### Obtain the final Feature engineered dataset that Model Developer uses to train and test the model\n",
    "\n",
    "In the feature engineering phase, we apply specific transformations to optimize the dataset for predictive modeling in our application scorecard. \n",
    "\n",
    "Using the `ending_club.feature_engineering()` function, the Model Developer conducted the following operations:\n",
    "- **WoE encoding**: Converts both numerical and categorical features into Weight of Evidence (WoE) values. WoE is a statistical measure used in scorecard modeling that quantifies the relationship between a predictor variable and the binary target variable. It calculates the ratio of the distribution of good outcomes to the distribution of bad outcomes for each category or bin of a feature. This transformation helps to ensure that the features are predictive and consistent in their contribution to the model.\n",
    "- **Integration of WoE bins**: Ensures that the WoE transformed values are integrated throughout the dataset, replacing the original feature values while excluding the target variable from this transformation. This transformation is used to maintain a consistent scale and impact of each variable within the model, which helps make the predictions more stable and accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_df = lending_club.feature_engineering(preprocess_df)\n",
    "fe_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc6_'></a>\n",
    "\n",
    "## As a Model Validator we want to split the featured engineered dataset into train and test for Validation testing purposes. In addition, as a Validator we also want to include challenger/benchmark models.\n",
    "\n",
    "In this section, we will to a train and split randomly as the Validator want's to independently challenge the developer \n",
    "- We begin by dividing our data, which is based on Weight of Evidence (WoE) features, into training and testing sets (`train_df`, `test_df`). \n",
    "- With `lending_club.split`, we employ a simple random split, randomly allocating data points to each set to ensure a mix of examples in both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "train_df, test_df = lending_club.split(fe_df, test_size=0.2)\n",
    "\n",
    "x_train = train_df.drop(lending_club.target_column, axis=1)\n",
    "y_train = train_df[lending_club.target_column]\n",
    "\n",
    "x_test = test_df.drop(lending_club.target_column, axis=1)\n",
    "y_test = test_df[lending_club.target_column]\n",
    "\n",
    "#now let's apply the order of features from the champion model construction\n",
    "x_train = x_train[cols_when_model_builds]\n",
    "x_test = x_test[cols_when_model_builds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_use = ['annual_inc_woe',\n",
    " 'verification_status_woe',\n",
    " 'emp_length_woe',\n",
    " 'installment_woe',\n",
    " 'term_woe',\n",
    " 'home_ownership_woe',\n",
    " 'purpose_woe',\n",
    " 'open_acc_woe',\n",
    " 'total_acc_woe',\n",
    " 'int_rate_woe',\n",
    " 'sub_grade_woe',\n",
    " 'grade_woe','loan_status']\n",
    "\n",
    "\n",
    "train_df = train_df[cols_use]\n",
    "test_df = test_df[cols_use]\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As a Model Validator I also want to investigate potential challenger models - Let's train two challenger models as basis for the testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Random Forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=50, \n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "rf_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second Challenger Model a Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Logistic Regression grid params\n",
    "log_reg_params = {\n",
    "    \"penalty\": [\"l1\", \"l2\"],\n",
    "    \"C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    \"solver\": [\"liblinear\"],\n",
    "}\n",
    "\n",
    "# Grid search for Logistic Regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\n",
    "grid_log_reg.fit(x_train, y_train)\n",
    "\n",
    "# Logistic Regression best estimator\n",
    "log_reg = grid_log_reg.best_estimator_\n",
    "log_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc6_1_'></a>\n",
    "\n",
    "### Compute probabilities as this is the raw probabilitistc output from the models of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xgb_prob = xgb_model.predict_proba(x_train)[:, 1]\n",
    "test_xgb_prob = xgb_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "train_rf_prob = rf_model.predict_proba(x_train)[:, 1]\n",
    "test_rf_prob = rf_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "train_log_prob = log_reg.predict_proba(x_train)[:, 1]\n",
    "test_log_prob = log_reg.predict_proba(x_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc6_2_'></a>\n",
    "\n",
    "### Compute binary predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_off_threshold = 0.3 \n",
    "\n",
    "train_xgb_binary_predictions = (train_xgb_prob > cut_off_threshold).astype(int)\n",
    "test_xgb_binary_predictions = (test_xgb_prob > cut_off_threshold).astype(int)\n",
    "\n",
    "train_rf_binary_predictions = (train_rf_prob > cut_off_threshold).astype(int)\n",
    "test_rf_binary_predictions = (test_rf_prob > cut_off_threshold).astype(int)\n",
    "\n",
    "train_log_binary_predictions = (train_log_prob > cut_off_threshold).astype(int)\n",
    "test_log_binary_predictions = (test_log_prob > cut_off_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc7_'></a>\n",
    "\n",
    "## Document the model\n",
    "\n",
    "To document the model with the ValidMind Library, you'll need to:\n",
    "1. Preprocess the raw dataset\n",
    "2. Initialize some training and test datasets\n",
    "3. Initialize a model object you can use for testing\n",
    "4. Run the full suite of tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc7_1_'></a>\n",
    "\n",
    "### Initialize the ValidMind datasets\n",
    "\n",
    "Before you can run tests, you must first initialize a ValidMind dataset object using the [`init_dataset`](https://docs.validmind.ai/validmind/validmind.html#init_dataset) function from the ValidMind (`vm`) module.\n",
    "\n",
    "This function takes a number of arguments:\n",
    "\n",
    "- `dataset`: The dataset that you want to provide as input to tests.\n",
    "- `input_id`: A unique identifier that allows tracking what inputs are used when running each individual test.\n",
    "- `target_column`: A required argument if tests require access to true values. This is the name of the target column in the dataset.\n",
    "\n",
    "With all datasets ready, you can now initialize the raw, processed, training and test datasets (`raw_df`, `preprocessed_df`, `fe_df`,  `train_df` and `test_df`) created earlier into their own dataset objects using [`vm.init_dataset()`](https://docs.validmind.ai/validmind/validmind.html#init_dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_raw_dataset = vm.init_dataset(\n",
    "    dataset=df,\n",
    "    input_id=\"raw_dataset\",\n",
    "    target_column=lending_club.target_column,\n",
    ")\n",
    "\n",
    "vm_preprocess_dataset = vm.init_dataset(\n",
    "    dataset=preprocess_df,\n",
    "    input_id=\"preprocess_dataset\",\n",
    "    target_column=lending_club.target_column,\n",
    ")\n",
    "\n",
    "vm_fe_dataset = vm.init_dataset(\n",
    "    dataset=fe_df,\n",
    "    input_id=\"fe_dataset\",\n",
    "    target_column=lending_club.target_column,\n",
    ")\n",
    "\n",
    "vm_train_ds = vm.init_dataset(\n",
    "    dataset=train_df,\n",
    "    input_id=\"train_dataset\",\n",
    "    target_column=lending_club.target_column,\n",
    ")\n",
    "\n",
    "vm_test_ds = vm.init_dataset(\n",
    "    dataset=test_df,\n",
    "    input_id=\"test_dataset\",\n",
    "    target_column=lending_club.target_column,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc7_2_'></a>\n",
    "\n",
    "### Initialize a model object\n",
    "\n",
    "You will also need to initialize a ValidMind model object (`vm_model`) that can be passed to other functions for analysis and tests on the data. You simply intialize this model object with [`vm.init_model()`](https://docs.validmind.ai/validmind/validmind.html#init_model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_xgb_model = vm.init_model(\n",
    "    xgb_model,\n",
    "    input_id=\"xgb_model_developer_champion\",\n",
    ")\n",
    "\n",
    "vm_rf_model = vm.init_model(\n",
    "    rf_model,\n",
    "    input_id=\"rf_model\",\n",
    ")\n",
    "\n",
    "vm_log_model = vm.init_model(\n",
    "    log_reg,\n",
    "    input_id=\"log_model\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='toc7_3_'></a>\n",
    "\n",
    "### Assign prediction values and probabilities to the datasets\n",
    "\n",
    "With our model now trained, we'll move on to assigning both the predictive probabilities coming directly from the model's predictions, and the binary prediction after applying the cutoff threshold described in the previous steps. \n",
    "- These tasks are achieved through the use of the `assign_predictions()` method associated with the VM `dataset` object.\n",
    "- This method links the model's class prediction values and probabilities to our VM train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Model\n",
    "vm_train_ds.assign_predictions(\n",
    "    model=vm_xgb_model,\n",
    "    prediction_values=train_xgb_binary_predictions,\n",
    "    prediction_probabilities=train_xgb_prob,\n",
    ")\n",
    "\n",
    "vm_test_ds.assign_predictions(\n",
    "    model=vm_xgb_model,\n",
    "    prediction_values=test_xgb_binary_predictions,\n",
    "    prediction_probabilities=test_xgb_prob,\n",
    ")\n",
    "\n",
    "# Random Forest Model\n",
    "vm_train_ds.assign_predictions(\n",
    "    model=vm_rf_model,\n",
    "    prediction_values=train_rf_binary_predictions,\n",
    "    prediction_probabilities=train_rf_prob,\n",
    ")\n",
    "\n",
    "vm_test_ds.assign_predictions(\n",
    "    model=vm_rf_model,\n",
    "    prediction_values=test_rf_binary_predictions,\n",
    "    prediction_probabilities=test_rf_prob,\n",
    ")\n",
    "\n",
    "\n",
    "# Logistic Regression \n",
    "vm_train_ds.assign_predictions(\n",
    "    model=vm_log_model,\n",
    "    prediction_values=train_log_binary_predictions,\n",
    "    prediction_probabilities=train_log_prob,\n",
    ")\n",
    "\n",
    "vm_test_ds.assign_predictions(\n",
    "    model=vm_log_model,\n",
    "    prediction_values=test_log_binary_predictions,\n",
    "    prediction_probabilities=test_log_prob,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute credit risk scores\n",
    "\n",
    "In this phase, we translate model predictions into actionable scores using probability estimates generated by our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xgb_scores = lending_club.compute_scores(train_xgb_prob)\n",
    "test_xgb_scores = lending_club.compute_scores(test_xgb_prob)\n",
    "train_rf_scores = lending_club.compute_scores(train_rf_prob)\n",
    "test_rf_scores = lending_club.compute_scores(test_rf_prob)\n",
    "train_log_scores = lending_club.compute_scores(train_log_prob)\n",
    "test_log_scores = lending_club.compute_scores(test_log_prob)\n",
    "\n",
    "# Assign scores to the datasets\n",
    "vm_train_ds.add_extra_column(\"xgb_scores\", train_xgb_scores)\n",
    "vm_test_ds.add_extra_column(\"xgb_scores\", test_xgb_scores)\n",
    "vm_train_ds.add_extra_column(\"rf_scores\", train_rf_scores)\n",
    "vm_test_ds.add_extra_column(\"rf_scores\", test_rf_scores)\n",
    "vm_train_ds.add_extra_column(\"log_scores\", train_log_scores)\n",
    "vm_test_ds.add_extra_column(\"log_scores\", test_log_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation Testing\n",
    "\n",
    "In the section below you (Model Validator) will select a series of tests from ValidMind in order to Independently challenge the Model Developer evidence and Assessment. In addition to this the Model Validator will also configure custom tets (tests not available out of the box). The focus will be on the following testing:\n",
    "\n",
    "- Ensuring Data used for training and testing the model is of appropriate data quality\n",
    "- Ensuring that Raw Data has been pre-processed appropriately and the resulting feature engineered dataset reflects this\n",
    "- Comprehensive testing around Model Performance of both the Developer Champion Model and challenger models developed by Validator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##first we start with Data Quality Testing, and we are going to leverage ValidMind tests for this purpose\n",
    "#Explore tests\n",
    "from validmind.tests import (\n",
    "    describe_test,\n",
    "    list_tests,\n",
    "    list_tasks,\n",
    "    list_tags,\n",
    "    list_tasks_and_tags,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's find the data quality tests relevant for a classification use-case\n",
    "list_tasks_and_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list the tests that we want to run as a validator for data_quality\n",
    "list_tests(\n",
    "    tags=[\"data_quality\"], task=\"classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dq = list_tests(tags=[\"data_quality\"], task=\"classification\",pretty=False)\n",
    "dq              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's run the list of tests, first focusing on dataquality of the datasets used for training, and then we will do a comparison test from raw to final train datasets, i.e. has the developer addressed potential concerns\n",
    "for test in dq:\n",
    "    vm.tests.run_test(\n",
    "        test,\n",
    "        inputs={\n",
    "            \"dataset\": vm_preprocess_dataset\n",
    "        },\n",
    "    ).log() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do a comparison of the raw dataset and the pre-processed dataset, i.e. has the Developer adjusted the processed the dataset from raw to pre-processed appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######next let's do a comparison test between the pre-processed data and raw datasets\n",
    "for test in dq:\n",
    "    vm.tests.run_test(\n",
    "        test,\n",
    "        input_grid={\n",
    "            \"dataset\": [vm_raw_dataset,vm_preprocess_dataset]\n",
    "        },\n",
    "    ).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's do some independent testing with regards to performance of the champion model (xgboost). \n",
    "#First we want to test independently on the champion model and then we will move forward to add challenger models that we have trained and defined before\n",
    "list_tests(tags=[\"model_performance\"], task=\"classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpt = ['validmind.model_validation.sklearn.ClassifierPerformance:xgboost_champion','validmind.model_validation.sklearn.ConfusionMatrix:xgboost_champion',\n",
    "       'validmind.model_validation.sklearn.MinimumAccuracy:xgboost_champion', 'validmind.model_validation.sklearn.MinimumF1Score:xgboost_champion','validmind.model_validation.sklearn.ROCCurve:xgboost_champion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in mpt:\n",
    "    vm.tests.run_test(\n",
    "        test,\n",
    "        inputs={\n",
    "            \"dataset\": vm_test_ds, \"model\" : vm_xgb_model, \n",
    "        },\n",
    "    ).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#excellent-  we have now conducted similar tests as developer in order to verify the results. Now let's provide some challenge by introducting challenger models\n",
    "\n",
    "#first let's provide some identifiers to the tests since they are now challenger model tests\n",
    "mpt_chall = ['validmind.model_validation.sklearn.ClassifierPerformance:xgboost_champion_vs_challengers','validmind.model_validation.sklearn.ConfusionMatrix:xgboost_champion_vs_challengers',\n",
    "       'validmind.model_validation.sklearn.MinimumAccuracy:xgboost_champion_vs_challengers', 'validmind.model_validation.sklearn.MinimumF1Score:xgboost_champion_vs_challengers','validmind.model_validation.sklearn.ROCCurve:xgboost_champion_vs_challengers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"VALIDMIND_LLM_DESCRIPTIONS_CONTEXT_ENABLED\"] = \"1\"\n",
    "\n",
    "context = \"\"\"\n",
    "FORMAT FOR THE LLM DESCRIPTIONS: \n",
    "    **<Test Name>** is designed to <begin with a concise overview of what the test does and its primary purpose, \n",
    "    extracted from the test description>.\n",
    "\n",
    "    The test operates by <write a paragraph about the test mechanism, explaining how it works and what it measures. \n",
    "    Include any relevant formulas or methodologies mentioned in the test description.>\n",
    "\n",
    "    The primary advantages of this test include <write a paragraph about the test's strengths and capabilities, \n",
    "    highlighting what makes it particularly useful for specific scenarios.>\n",
    "\n",
    "    Users should be aware that <write a paragraph about the test's limitations and potential risks. \n",
    "    Include both technical limitations and interpretation challenges. \n",
    "    If the test description includes specific signs of high risk, incorporate these here.>\n",
    "\n",
    "    **Key Insights:**\n",
    "\n",
    "    The test results reveal:\n",
    "\n",
    "    - **<insight title>**: <comprehensive description of one aspect of the results>\n",
    "    - **<insight title>**: <comprehensive description of another aspect>\n",
    "    ...\n",
    "\n",
    "    Based on these results, <conclude with a brief paragraph that ties together the test results with the test's \n",
    "    purpose and provides any final recommendations or considerations.>\n",
    "\n",
    "ADDITIONAL INSTRUCTIONS:\n",
    "\n",
    "    The champion model as the basis for comparison is called \"xgb_model_developer_champion\" and emphasis should be on the following:\n",
    "    - The metrics for the champion model compared agains the challenger models\n",
    "    - Which model potentially outperforms the champion model based on the metrics, this should be highlighted and emphasized\n",
    "\n",
    "\n",
    "    For each metric in the test results, include in the test overview:\n",
    "    - The metric's purpose and what it measures\n",
    "    - Its mathematical formula\n",
    "    - The range of possible values\n",
    "    - What constitutes good/bad performance\n",
    "    - How to interpret different values\n",
    "\n",
    "    Each insight should progressively cover:\n",
    "    1. Overall scope and distribution\n",
    "    2. Complete breakdown of all elements with specific values\n",
    "    3. Natural groupings and patterns\n",
    "    4. Comparative analysis between datasets/categories\n",
    "    5. Stability and variations\n",
    "    6. Notable relationships or dependencies\n",
    "\n",
    "    Remember:\n",
    "    - Champion model (xgb_model_developer_champion) is the selection and challenger models are used to challenge the selection\n",
    "    - Keep all insights at the same level (no sub-bullets or nested structures)\n",
    "    - Make each insight complete and self-contained\n",
    "    - Include specific numerical values and ranges\n",
    "    - Cover all elements in the results comprehensively\n",
    "    - Maintain clear, concise language\n",
    "    - Use only \"- **Title**: Description\" format for insights\n",
    "    - Progress naturally from general to specific observations\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "os.environ[\"VALIDMIND_LLM_DESCRIPTIONS_CONTEXT\"] = context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's do same performance tests as above, but let's challenge the actual model itself and add two additional benchmark models\n",
    "for test in mpt_chall:\n",
    "    vm.tests.run_test(\n",
    "        test,\n",
    "        input_grid={\n",
    "            \"dataset\": [vm_test_ds], \"model\" : [vm_xgb_model,vm_log_model,vm_rf_model], \n",
    "        },\n",
    "    ).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now let's dig a little bit deeper into one of the tests that allows the Validator to custoimze parameters and thresholds for performance standards - and let's diregard RF model as we have\n",
    "### learned that the RF model is not a viable candidate based on the performance metrics.\n",
    "result = vm.tests.run_test(\n",
    "    'validmind.model_validation.sklearn.MinimumF1Score:AdjThreshold',\n",
    "    input_grid={\n",
    "        \"dataset\": [vm_test_ds], \"model\" : [vm_xgb_model,vm_log_model], 'params':{'min_threshold': 0.35},\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Robustness and Stability Testing Comparison Between the Two Models\n",
    "list_tests(tags=[\"model_diagnosis\"], task=\"classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see if models suffer from any overfit potentials and also where there are potential sub-segments of issues\n",
    "overfit_testing = ['validmind.model_validation.sklearn.TrainingTestDegradation:Champion_vs_LogRegression','validmind.model_validation.sklearn.OverfitDiagnosis:Champion_vs_LogRegression'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in overfit_testing:\n",
    "    vm.tests.run_test(\n",
    "        test,\n",
    "        input_grid={\n",
    "            \"datasets\": [[vm_train_ds,vm_test_ds]], \"model\" : [vm_xgb_model,vm_log_model], \n",
    "        },\n",
    "    ).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now finally let's conduct robustness and stability testing of the two models:\n",
    "stab_robust = ['validmind.model_validation.sklearn.RobustnessDiagnosis:Champion_vs_LogRegression'] # 'validmind.model_validation.sklearn.WeakspotsDiagnosis:Champion_vs_LogRegression'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in stab_robust:\n",
    "    vm.tests.run_test(\n",
    "        test,\n",
    "        input_grid={\n",
    "            \"datasets\": [[vm_train_ds,vm_test_ds]], \"model\" : [vm_xgb_model,vm_log_model], \n",
    "        },\n",
    "    ).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's verify the feature importance and inspect differences - different models might have more intuitive feature impacts that might lead to decisions in selection of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FI = list_tests(tags=[\"feature_importance\"], task=\"classification\",pretty=False)\n",
    "FI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in FI:\n",
    "    vm.tests.run_test(\n",
    "        \"\".join((test,':Champion_vs_LogisticRegression')),\n",
    "        input_grid={\n",
    "            \"dataset\": [vm_test_ds], \"model\" : [vm_xgb_model,vm_log_model], \n",
    "        },\n",
    "    ).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Let's finish off with a custom test example - scoring (customization of output to a FICO score type)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "@vm.test(\"my_custom_tests.ScoreToOdds\")\n",
    "def score_to_odds_analysis(dataset, score_column='score', score_bands=[410, 440, 470]):\n",
    "    \"\"\"\n",
    "    Analyzes the relationship between score bands and odds (good:bad ratio).\n",
    "    Good odds = (1 - default_rate) / default_rate\n",
    "    \n",
    "    Higher scores should correspond to higher odds of being good.\n",
    "\n",
    "    If there are multiple scores provided through score_column, this means that there are two different models and the scores reflect each model\n",
    "\n",
    "    If there are more scores provided in the score_column then focus the assessment on the differences between the two scores and indicate through evidence which one is preferred.\n",
    "    \"\"\"\n",
    "    df = dataset.df\n",
    "    \n",
    "    # Create score bands\n",
    "    df['score_band'] = pd.cut(\n",
    "        df[score_column],\n",
    "        bins=[-np.inf] + score_bands + [np.inf],\n",
    "        labels=[f'<{score_bands[0]}'] + \n",
    "               [f'{score_bands[i]}-{score_bands[i+1]}' for i in range(len(score_bands)-1)] +\n",
    "               [f'>{score_bands[-1]}']\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics per band\n",
    "    results = df.groupby('score_band').agg({\n",
    "        dataset.target_column: ['mean', 'count']\n",
    "    })\n",
    "    \n",
    "    results.columns = ['Default Rate', 'Total']\n",
    "    results['Good Count'] = results['Total'] - (results['Default Rate'] * results['Total'])\n",
    "    results['Bad Count'] = results['Default Rate'] * results['Total']\n",
    "    results['Odds'] = results['Good Count'] / results['Bad Count']\n",
    "    \n",
    "    # Create visualization\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add odds bars\n",
    "    fig.add_trace(go.Bar(\n",
    "        name='Odds (Good:Bad)',\n",
    "        x=results.index,\n",
    "        y=results['Odds'],\n",
    "        marker_color='blue'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Score-to-Odds Analysis',\n",
    "        yaxis=dict(title='Odds Ratio (Good:Bad)'),\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = vm.tests.run_test(\n",
    "    \"my_custom_tests.ScoreToOdds:Champion_vs_Challenger\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_test_ds,\n",
    "    },\n",
    "    param_grid={\n",
    "        \"score_column\": [\"xgb_scores\",\"log_scores\"],\n",
    "        \"score_bands\": [[500, 540, 570]],\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally we got all of the tests from the Developer that was provided as evidence, now as a final task we will verify testing being appropriately recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.utils import preview_test_config\n",
    "\n",
    "test_config = {'validmind.data_validation.DatasetDescription:raw_data': {'inputs': {'dataset': 'raw_dataset'}},\n",
    " 'validmind.data_validation.DescriptiveStatistics:raw_data': {'inputs': {'dataset': 'raw_dataset'}},\n",
    " 'validmind.data_validation.MissingValues:raw_data': {'inputs': {'dataset': 'raw_dataset'},\n",
    "  'params': {'min_threshold': 1}},\n",
    " 'validmind.data_validation.ClassImbalance:raw_data': {'inputs': {'dataset': 'raw_dataset'},\n",
    "  'params': {'min_percent_threshold': 10}},\n",
    " 'validmind.data_validation.Duplicates:raw_data': {'inputs': {'dataset': 'raw_dataset'},\n",
    "  'params': {'min_threshold': 1}},\n",
    " 'validmind.data_validation.HighCardinality:raw_data': {'inputs': {'dataset': 'raw_dataset'},\n",
    "  'params': {'num_threshold': 100,\n",
    "   'percent_threshold': 0.1,\n",
    "   'threshold_type': 'percent'}},\n",
    " 'validmind.data_validation.Skewness:raw_data': {'inputs': {'dataset': 'raw_dataset'},\n",
    "  'params': {'max_threshold': 1}},\n",
    " 'validmind.data_validation.UniqueRows:raw_data': {'inputs': {'dataset': 'raw_dataset'},\n",
    "  'params': {'min_percent_threshold': 1}},\n",
    " 'validmind.data_validation.TooManyZeroValues:raw_data': {'inputs': {'dataset': 'raw_dataset'},\n",
    "  'params': {'max_percent_threshold': 0.03}},\n",
    " 'validmind.data_validation.IQROutliersTable:raw_data': {'inputs': {'dataset': 'raw_dataset'},\n",
    "  'params': {'threshold': 5}},\n",
    " 'validmind.data_validation.DescriptiveStatistics:preprocessed_data': {'inputs': {'dataset': 'preprocess_dataset'}},\n",
    " 'validmind.data_validation.TabularDescriptionTables:preprocessed_data': {'inputs': {'dataset': 'preprocess_dataset'}},\n",
    " 'validmind.data_validation.MissingValues:preprocessed_data': {'inputs': {'dataset': 'preprocess_dataset'},\n",
    "  'params': {'min_threshold': 1}},\n",
    " 'validmind.data_validation.TabularNumericalHistograms:preprocessed_data': {'inputs': {'dataset': 'preprocess_dataset'}},\n",
    " 'validmind.data_validation.TabularCategoricalBarPlots:preprocessed_data': {'inputs': {'dataset': 'preprocess_dataset'}},\n",
    " 'validmind.data_validation.TargetRateBarPlots:preprocessed_data': {'inputs': {'dataset': 'preprocess_dataset'},\n",
    "  'params': {'default_column': 'loan_status'}},\n",
    " 'validmind.data_validation.DescriptiveStatistics:development_data': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset']}},\n",
    " 'validmind.data_validation.TabularDescriptionTables:development_data': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset']}},\n",
    " 'validmind.data_validation.ClassImbalance:development_data': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset']},\n",
    "  'params': {'min_percent_threshold': 10}},\n",
    " 'validmind.data_validation.UniqueRows:development_data': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset']},\n",
    "  'params': {'min_percent_threshold': 1}},\n",
    " 'validmind.data_validation.TabularNumericalHistograms:development_data': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset']}},\n",
    " 'validmind.data_validation.MutualInformation:development_data': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset']},\n",
    "  'params': {'min_threshold': 0.01}},\n",
    " 'validmind.data_validation.PearsonCorrelationMatrix:development_data': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset']}},\n",
    " 'validmind.data_validation.HighPearsonCorrelation:development_data': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset']},\n",
    "  'params': {'max_threshold': 0.3, 'top_n_correlations': 10}},\n",
    " 'validmind.data_validation.WOEBinTable': {'input_grid': {'dataset': ['preprocess_dataset']},\n",
    "  'params': {'breaks_adj': {'loan_amnt': [5000, 10000, 15000, 20000, 25000],\n",
    "    'int_rate': [10, 15, 20],\n",
    "    'annual_inc': [50000, 100000, 150000]}}},\n",
    " 'validmind.data_validation.WOEBinPlots': {'input_grid': {'dataset': ['preprocess_dataset']},\n",
    "  'params': {'breaks_adj': {'loan_amnt': [5000, 10000, 15000, 20000, 25000],\n",
    "    'int_rate': [10, 15, 20],\n",
    "    'annual_inc': [50000, 100000, 150000]}}},\n",
    " 'validmind.data_validation.DatasetSplit': {'inputs': {'datasets': ['train_dataset',\n",
    "    'test_dataset']}},\n",
    " 'validmind.model_validation.ModelMetadata': {'input_grid': {'model': ['xgb_model',\n",
    "    'rf_model']}},\n",
    " 'validmind.model_validation.sklearn.ModelParameters': {'input_grid': {'model': ['xgb_model',\n",
    "    'rf_model']}},\n",
    " 'validmind.model_validation.statsmodels.GINITable': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': ['xgb_model', 'rf_model']}},\n",
    " 'validmind.model_validation.sklearn.ClassifierPerformance': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': ['xgb_model', 'rf_model']}},\n",
    " 'validmind.model_validation.sklearn.TrainingTestDegradation:XGBoost': {'inputs': {'datasets': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': 'xgb_model'},\n",
    "  'params': {'max_threshold': 0.1}},\n",
    " 'validmind.model_validation.sklearn.TrainingTestDegradation:RandomForest': {'inputs': {'datasets': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': 'rf_model'},\n",
    "  'params': {'max_threshold': 0.1}},\n",
    " 'validmind.model_validation.sklearn.ROCCurve': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': ['xgb_model']}},\n",
    " 'validmind.model_validation.sklearn.MinimumROCAUCScore': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': ['xgb_model']},\n",
    "  'params': {'min_threshold': 0.5}},\n",
    " 'validmind.model_validation.statsmodels.PredictionProbabilitiesHistogram': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': ['xgb_model']}},\n",
    " 'validmind.model_validation.statsmodels.CumulativePredictionProbabilities': {'input_grid': {'model': ['xgb_model'],\n",
    "   'dataset': ['train_dataset', 'test_dataset']}},\n",
    " 'validmind.model_validation.sklearn.PopulationStabilityIndex': {'inputs': {'datasets': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': 'xgb_model'},\n",
    "  'params': {'num_bins': 10, 'mode': 'fixed'}},\n",
    " 'validmind.model_validation.sklearn.ConfusionMatrix': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': ['xgb_model']}},\n",
    " 'validmind.model_validation.sklearn.MinimumAccuracy': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': ['xgb_model']},\n",
    "  'params': {'min_threshold': 0.7}},\n",
    " 'validmind.model_validation.sklearn.MinimumF1Score': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': ['xgb_model']},\n",
    "  'params': {'min_threshold': 0.5}},\n",
    " 'validmind.model_validation.sklearn.PrecisionRecallCurve': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': ['xgb_model']}},\n",
    " 'validmind.model_validation.sklearn.CalibrationCurve': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': ['xgb_model']}},\n",
    " 'validmind.model_validation.sklearn.ClassifierThresholdOptimization': {'inputs': {'dataset': 'train_dataset',\n",
    "   'model': 'xgb_model'},\n",
    "  'params': {'target_recall': 0.8}},\n",
    " 'validmind.model_validation.statsmodels.ScorecardHistogram': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset']},\n",
    "  'params': {'score_column': 'xgb_scores'}},\n",
    " 'validmind.data_validation.ScoreBandDefaultRates': {'input_grid': {'dataset': ['train_dataset'],\n",
    "   'model': ['xgb_model']},\n",
    "  'params': {'score_column': 'xgb_scores', 'score_bands': [504, 537, 570]}},\n",
    " 'validmind.model_validation.sklearn.ScoreProbabilityAlignment': {'input_grid': {'dataset': ['train_dataset'],\n",
    "   'model': ['xgb_model']},\n",
    "  'params': {'score_column': 'xgb_scores'}},\n",
    " 'validmind.model_validation.sklearn.WeakspotsDiagnosis': {'inputs': {'datasets': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': 'xgb_model'}},\n",
    " 'validmind.model_validation.sklearn.OverfitDiagnosis': {'inputs': {'model': 'xgb_model',\n",
    "   'datasets': ['train_dataset', 'test_dataset']},\n",
    "  'params': {'cut_off_threshold': 0.04}},\n",
    " 'validmind.model_validation.sklearn.RobustnessDiagnosis': {'inputs': {'datasets': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': 'xgb_model'},\n",
    "  'params': {'scaling_factor_std_dev_list': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "   'performance_decay_threshold': 0.05}},\n",
    " 'validmind.model_validation.sklearn.PermutationFeatureImportance': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': ['xgb_model']}},\n",
    " 'validmind.model_validation.FeaturesAUC': {'input_grid': {'model': ['xgb_model'],\n",
    "   'dataset': ['train_dataset', 'test_dataset']}},\n",
    " 'validmind.model_validation.sklearn.SHAPGlobalImportance': {'input_grid': {'model': ['xgb_model'],\n",
    "   'dataset': ['train_dataset', 'test_dataset']},\n",
    "  'params': {'kernel_explainer_samples': 10,\n",
    "   'tree_or_linear_explainer_samples': 200}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in test_config:\n",
    "    print(t)\n",
    "    try:\n",
    "        # Check if test has input_grid\n",
    "        if 'input_grid' in test_config[t]:\n",
    "            # For tests with input_grid, pass the input_grid configuration\n",
    "            if 'params' in test_config[t]:\n",
    "                vm.tests.run_test(t, input_grid=test_config[t]['input_grid'], params=test_config[t]['params']).log()\n",
    "            else:\n",
    "                vm.tests.run_test(t, input_grid=test_config[t]['input_grid']).log()\n",
    "        else:\n",
    "            # Original logic for regular inputs\n",
    "            if 'params' in test_config[t]:\n",
    "                vm.tests.run_test(t, inputs=test_config[t]['inputs'], params=test_config[t]['params']).log()\n",
    "            else:\n",
    "                vm.tests.run_test(t, inputs=test_config[t]['inputs']).log()\n",
    "    except Exception as e:\n",
    "        print(f\"Error running test {t}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

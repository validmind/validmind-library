{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# DeepEval Integration with ValidMind\n",
    "\n",
    "Let's learn how to integrate [DeepEval](https://github.com/confident-ai/deepeval) with the ValidMind Library to evaluate Large Language Models (LLMs) and AI agents. This notebook demonstrates how to use DeepEval's summarization metrics within ValidMind's testing infrastructure.\n",
    "\n",
    "To integrate DeepEval with ValidMind, we'll:\n",
    " 1. Set up both frameworks and install required dependencies\n",
    " 2. Create a dataset with source texts and generated summaries\n",
    " 3. Use ValidMind's Summarization scorer to evaluate summary quality\n",
    " 4. Analyze the evaluation results and reasons\n",
    " 5. Apply the evaluation pipeline to multiple examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Contents    \n",
    "- [Introduction](#toc1_)    \n",
    "- [About DeepEval Integration](#toc2_)    \n",
    "  - [Before you begin](#toc2_1_)    \n",
    "  - [Key concepts](#toc2_2_)    \n",
    "- [Setting up](#toc3_)    \n",
    "  - [Install required packages](#toc3_1_)    \n",
    "  - [Initialize ValidMind](#toc3_2_)    \n",
    "- [Basic Usage - Simple Q&A Evaluation](#toc4_)    \n",
    "- [RAG System Evaluation](#toc5_)    \n",
    "  - [Create test cases](#toc5_1_)    \n",
    "  - [Build dataset](#toc5_2_)    \n",
    "  - [Evaluation metrics](#toc5_3_)    \n",
    "    - [Contextual Relevancy](#toc5_3_1_)    \n",
    "    - [Contextual Precision](#toc5_3_2_)    \n",
    "    - [Contextual Recall](#toc5_3_3_)    \n",
    "- [LLM Agent Evaluation](#toc6_)    \n",
    "  - [Create test cases](#toc6_1_)    \n",
    "  - [Build dataset](#toc6_2_)    \n",
    "  - [Evaluation metrics](#toc6_3_)    \n",
    "    - [Faithfulness](#toc6_3_1_)    \n",
    "    - [Hallucination](#toc6_3_2_)    \n",
    "    - [Summarization](#toc6_3_3_)    \n",
    "    - [AI Agent Evaluation Metrics](#toc6_3_4_)      \n",
    "- [In summary](#toc10_)    \n",
    "- [Next steps](#toc11_)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<a id=\"toc1_\"></a>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Large Language Model (LLM) evaluation is critical for understanding model performance across different tasks and scenarios. This notebook demonstrates how to integrate DeepEval's comprehensive evaluation framework with ValidMind's testing infrastructure to create a robust LLM evaluation pipeline.\n",
    "\n",
    "DeepEval provides over 30 evaluation metrics specifically designed for LLMs, covering scenarios from simple Q&A to complex agent interactions. By integrating with ValidMind, you can leverage these metrics within a structured testing framework that supports documentation, collaboration, and compliance requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<a id=\"toc2_\"></a>\n",
    "\n",
    "## About DeepEval Integration\n",
    "\n",
    "DeepEval is a comprehensive evaluation framework for LLMs that provides metrics for various scenarios including hallucination detection, answer relevancy, faithfulness, and custom evaluation criteria. ValidMind is a platform for managing model risk and documentation through automated testing.\n",
    "\n",
    "Together, these tools enable comprehensive LLM evaluation within a structured, compliant framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<a id=\"toc2_1_\"></a>\n",
    "\n",
    "### Before you begin\n",
    "\n",
    "This notebook assumes you have basic familiarity with Python and Large Language Models. You'll need:\n",
    "\n",
    "- Python 3.8 or higher\n",
    "- Access to OpenAI API (for DeepEval metrics evaluation)\n",
    "- ValidMind account and model registration\n",
    "\n",
    "If you encounter errors due to missing modules, install them with `pip install` and re-run the notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<a id=\"toc2_2_\"></a>\n",
    "\n",
    "### Key concepts\n",
    "\n",
    "**LLMTestCase**: A DeepEval object that represents a single test case with input, expected output, actual output, and optional context.\n",
    "\n",
    "**LLMAgentDataset**: A ValidMind dataset class that bridges DeepEval test cases with ValidMind's testing infrastructure.\n",
    "\n",
    "**RAG Evaluation**: Testing retrieval-augmented generation systems that combine document retrieval with generation.\n",
    "\n",
    "**Agent Evaluation**: Testing LLM agents that can use tools and perform multi-step reasoning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<a id=\"toc3_\"></a>\n",
    "\n",
    "## Setting up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<a id=\"toc3_1_\"></a>\n",
    "\n",
    "### Install required packages\n",
    "\n",
    "First, let's install the required packages and set up our environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q validmind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<a id=\"toc3_2_\"></a>\n",
    "\n",
    "### Initialize ValidMind\n",
    "\n",
    "ValidMind generates a unique _code snippet_ for each registered model to connect with your developer environment. You initialize the ValidMind Library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: #B5B5B510; color: black; border: 1px solid #083E44; border-left-width: 5px; box-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);border-radius: 5px;\"><span style=\"color: #083E44;\"><b>For access to all features available in this notebook, you'll need access to a ValidMind account.</b></span>\n",
    "<br></br>\n",
    "<a href=\"https://docs.validmind.ai/guide/configuration/register-with-validmind.html\" style=\"color: #DE257E;\"><b>Register with ValidMind</b></a></div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your model identifier credentials from an `.env` file\n",
    "%load_ext dotenv\n",
    "%dotenv .env\n",
    "\n",
    "# Or replace with your code snippet\n",
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "    api_host=\"...\",\n",
    "    api_key=\"...\",\n",
    "    api_secret=\"...\",\n",
    "    model=\"...\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from deepeval.test_case import LLMTestCase, ToolCall\n",
    "from validmind.datasets.llm import LLMAgentDataset\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<a id=\"toc4_\"></a>\n",
    "\n",
    "## Basic Usage - Simple Q&A Evaluation\n",
    "\n",
    "Let's start with the simplest use case: evaluating a basic question-and-answer interaction with an LLM. This demonstrates how to create LLMTestCase objects and integrate them with ValidMind's dataset infrastructure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a simple LLM test case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_test_cases = [\n",
    "    LLMTestCase(\n",
    "        input=\"What is machine learning?\",\n",
    "        actual_output=\"\"\"Machine learning is a subset of artificial intelligence (AI) that enables \n",
    "    computers to learn and make decisions from data without being explicitly programmed for every task. \n",
    "    It uses algorithms to find patterns in data and make predictions or decisions based on those patterns.\"\"\",\n",
    "        expected_output=\"\"\"Machine learning is a method of data analysis that automates analytical \n",
    "    model building. It uses algorithms that iteratively learn from data, allowing computers to find \n",
    "    hidden insights without being explicitly programmed where to look.\"\"\",\n",
    "        context=[\"Machine learning is a branch of AI that focuses on algorithms that can learn from data.\"],\n",
    "        retrieval_context=[\"Machine learning is a branch of AI that focuses on algorithms that can learn from data.\"],\n",
    "        tools_called=[\n",
    "            ToolCall(\n",
    "                name=\"search_docs\",\n",
    "                input={\"query\": \"machine learning definition\"},\n",
    "                response=\"Found definition of machine learning in documentation.\"\n",
    "            )\n",
    "        ],\n",
    "        expected_tools=[\n",
    "            ToolCall(\n",
    "                name=\"search_docs\",\n",
    "                input={\"query\": \"machine learning definition\"},\n",
    "                response=\"Found definition of machine learning in documentation.\"\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    LLMTestCase(\n",
    "        input=\"What is deep learning?\",\n",
    "        actual_output=\"\"\"Bananas are yellow fruits that grow on trees in tropical climates. \n",
    "    They are rich in potassium and make a great healthy snack. You can also use them \n",
    "    in smoothies and baking.\"\"\",\n",
    "        expected_output=\"\"\"Deep learning is an advanced machine learning technique that uses neural networks\n",
    "    with many layers to automatically learn representations of data with multiple levels of abstraction.\n",
    "    It has enabled major breakthroughs in AI applications.\"\"\",\n",
    "        context=[\"Deep learning is a specialized machine learning approach that uses deep neural networks to learn from data.\"],\n",
    "        retrieval_context=[\"Deep learning is a specialized machine learning approach that uses deep neural networks to learn from data.\"],\n",
    "        tools_called=[\n",
    "            ToolCall(\n",
    "                name=\"search_docs\", \n",
    "                args={\"query\": \"deep learning definition\"},\n",
    "                response=\"Found definition of deep learning in documentation.\"\n",
    "            )\n",
    "        ],\n",
    "        expected_tools=[\n",
    "            ToolCall(\n",
    "                name=\"summarize_docs\", \n",
    "                args={\"query\": \"deep learning definition\"},\n",
    "                response=\"Generated summary of deep learning from documentation.\"\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LLMAgentDataset from the test case\n",
    "Let's create ValidMind dataset from Deepeval's test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCreating ValidMind dataset...\")\n",
    "\n",
    "simple_dataset = LLMAgentDataset.from_test_cases(\n",
    "    test_cases=simple_test_cases,\n",
    "    input_id=\"simple_qa_dataset\"\n",
    ")\n",
    "\n",
    "print(simple_dataset)\n",
    "# Display the dataset\n",
    "pd.set_option('display.max_colwidth', 40)\n",
    "pd.set_option('display.width', 120)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "print(\"\\nDataset preview:\")\n",
    "display(simple_dataset.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_dataset._df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics using ValidMind scorer interface\n",
    "\n",
    "Now we'll compute metrics on our dataset using ValidMind's scorer interface. This will help us evaluate how well our model is performing by calculating various metrics like answer relevancy. The scorer interface provides a standardized way to assess model outputs against expected results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_dataset.assign_scores(\n",
    "    metrics = \"validmind.scorers.llm.deepeval.AnswerRelevancy\",\n",
    "    input_column = \"input\",\n",
    "    actual_output_column = \"actual_output\",\n",
    ")\n",
    "simple_dataset._df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_dataset.assign_scores(\n",
    "    metrics = \"validmind.scorers.llm.deepeval.Bias\",\n",
    "    input_column = \"input\",\n",
    "    actual_output_column = \"actual_output\",\n",
    ")\n",
    "simple_dataset._df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<a id=\"toc5_\"></a>\n",
    "\n",
    "## RAG System Evaluation\n",
    "\n",
    "Now let's evaluate a more complex use case: a Retrieval-Augmented Generation (RAG) system that retrieves relevant documents and generates responses based on them. RAG systems combine document retrieval with text generation, requiring specialized evaluation approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc5_1_\"></a>\n",
    "\n",
    "### Create test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Creating RAG evaluation test cases...\")\n",
    "rag_test_cases = [\n",
    "    LLMTestCase(\n",
    "        input=\"How do I return a product that doesn't fit?\",\n",
    "        actual_output=\"\"\"You can return any product within 30 days of purchase for a full refund. \n",
    "        Simply visit our returns page on the website and follow the step-by-step instructions. \n",
    "        You'll need your order number and email address. No questions asked!\"\"\",\n",
    "        expected_output=\"We offer a 30-day return policy for full refunds. Visit our returns page to start the process.\",\n",
    "        context=[\"Company policy allows 30-day returns for full refund with no restocking fees.\"],\n",
    "        retrieval_context=[\n",
    "            \"Return Policy: All items can be returned within 30 days of purchase for a full refund.\",\n",
    "            \"Return Process: Visit our website's returns page and enter your order details.\",\n",
    "            \"Customer Service: Available 24/7 to help with returns and refunds.\",\n",
    "            \"No restocking fees apply to returns within the 30-day window.\"\n",
    "        ]\n",
    "    ),\n",
    "    LLMTestCase(\n",
    "        input=\"What are your shipping options and costs?\",\n",
    "        actual_output=\"\"\"We offer three shipping options: Standard (5-7 days, $5.99), \n",
    "        Express (2-3 days, $9.99), and Overnight (next day, $19.99). \n",
    "        Free shipping is available on orders over $50 with Standard delivery.\"\"\",\n",
    "        expected_output=\"Multiple shipping options available with costs ranging from $5.99 to $19.99. Free shipping on orders over $50.\",\n",
    "        context=[\"Shipping information includes various speed and cost options.\"],\n",
    "        retrieval_context=[\n",
    "            \"Standard Shipping: 5-7 business days, $5.99\",\n",
    "            \"Express Shipping: 2-3 business days, $9.99\", \n",
    "            \"Overnight Shipping: Next business day, $19.99\",\n",
    "            \"Free Standard Shipping on orders over $50\"\n",
    "        ]\n",
    "    ),\n",
    "    LLMTestCase(\n",
    "        input=\"Do you have a warranty on electronics?\",\n",
    "        actual_output=\"\"\"Yes, all electronics come with a manufacturer's warranty. \n",
    "        Most items have a 1-year warranty, while premium products may have 2-3 years. \n",
    "        We also offer extended warranty options for purchase.\"\"\",\n",
    "        expected_output=\"Electronics include manufacturer warranty, typically 1-year, with extended options available.\",\n",
    "        context=[\"Electronics warranty information varies by product type and manufacturer.\"],\n",
    "        retrieval_context=[\n",
    "            \"Electronics Warranty: Manufacturer warranty included with all electronic items\",\n",
    "            \"Standard Coverage: 1 year for most electronics\",\n",
    "            \"Premium Products: May include 2-3 year coverage\",\n",
    "            \"Extended Warranty: Available for purchase at checkout\"\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"Created {len(rag_test_cases)} RAG test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc5_2_\"></a>\n",
    "\n",
    "### Build dataset\n",
    "\n",
    "In this section, we'll convert our Deepeval LLMTestCase objects into a ValidMind dataset format.\n",
    "This allows us to leverage ValidMind's powerful evaluation capabilities while maintaining \n",
    "compatibility with Deepeval's test case structure.\n",
    "\n",
    "The dataset will contain:\n",
    "- Input queries\n",
    "- Actual model outputs \n",
    "- Expected outputs\n",
    "- Context information\n",
    "- Retrieved context passages\n",
    "\n",
    "This structured format enables detailed analysis of the RAG system's performance\n",
    "across multiple evaluation dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_dataset = LLMAgentDataset.from_test_cases(\n",
    "    test_cases=rag_test_cases,\n",
    "    input_id=\"rag_evaluation_dataset\"\n",
    ")\n",
    "\n",
    "print(f\"RAG Dataset: {rag_dataset}\")\n",
    "print(f\"Shape: {rag_dataset.df.shape}\")\n",
    "\n",
    "# Show dataset structure\n",
    "print(\"\\nRAG Dataset Preview:\")\n",
    "display(rag_dataset.df[['input', 'actual_output', 'context', 'retrieval_context']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc5_3_\"></a>\n",
    "\n",
    "### Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc5_3_1_\"></a>\n",
    "\n",
    "#### Contextual Relevancy\n",
    "The Contextual Relevancy metric evaluates how well the retrieved context aligns with the input query.\n",
    "It measures whether the context contains the necessary information to answer the query accurately.\n",
    "A high relevancy score indicates that the retrieved context is highly relevant and contains the key information needed.\n",
    "This helps validate that the RAG system is retrieving appropriate context for the given queries.\n",
    "\n",
    "<a id=\"toc5_3_2_\"></a>\n",
    "\n",
    "#### Contextual Precision\n",
    "The Contextual Precision metric evaluates how well a RAG system ranks retrieved context nodes by relevance to the input query. \n",
    "It checks if the most relevant nodes are ranked at the top of the retrieval results.\n",
    "A high precision score indicates that the retrieved context is highly relevant to the query and properly ranked.\n",
    "This is particularly useful for evaluating RAG systems and ensuring they surface the most relevant information first.\n",
    "\n",
    "<a id=\"toc5_3_3_\"></a>\n",
    "\n",
    "#### Contextual Recall\n",
    "The Contextual Recall metric evaluates how well the retrieved context covers all the information needed to generate the expected output.\n",
    "It extracts statements from the expected output and checks how many of them can be attributed to the retrieved context.\n",
    "A high recall score indicates that the retrieved context contains all the key information needed to generate the expected response.\n",
    "This helps ensure the RAG system retrieves comprehensive context that covers all aspects of the expected answer.\n",
    "\n",
    "Now we'll evaluate the RAG system's performance using multiple metrics at once. The `assign_scores()` method accepts a list of metrics to evaluate different aspects of the system's behavior. The metrics will add score and reason columns to the dataset, providing quantitative and qualitative feedback on the system's performance. This multi-metric evaluation gives us comprehensive insights into the strengths and potential areas for improvement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_dataset.assign_scores(\n",
    "    metrics = [\"validmind.scorers.llm.deepeval.ContextualRelevancy\",\n",
    "               \"validmind.scorers.llm.deepeval.ContextualPrecision\",\n",
    "               \"validmind.scorers.llm.deepeval.ContextualRecall\"],\n",
    "    input_column = \"input\",\n",
    "    expected_output_column = \"expected_output\",\n",
    "    retrieval_context_column = \"retrieval_context\",\n",
    ")\n",
    "display(rag_dataset._df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<a id=\"toc6_\"></a>\n",
    "\n",
    "## LLM Agent Evaluation\n",
    "\n",
    "Let's evaluate LLM agents that can use tools to accomplish tasks. This is one of the most advanced evaluation scenarios, requiring assessment of both response quality and tool usage appropriateness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc6_1_\"></a>\n",
    "### Create test cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LLM Agent test cases with tool usage\n",
    "print(\"Creating Agent evaluation test cases...\")\n",
    "\n",
    "# Create test cases\n",
    "agent_test_cases = [\n",
    "    LLMTestCase(\n",
    "        input=\"What's the weather like in New York City today?\",\n",
    "        actual_output=\"\"\"Based on current weather data, New York City is experiencing partly cloudy skies \n",
    "        with a temperature of 72°F (22°C). The humidity is at 60% and there's a light breeze from the west at 8 mph. \n",
    "        No precipitation is expected today.\"\"\",\n",
    "        expected_output=\"Current weather in New York shows mild temperatures with partly cloudy conditions.\",\n",
    "        tools_called=[\n",
    "            ToolCall(\n",
    "                name=\"WeatherAPI\",\n",
    "                description=\"Fetches current weather information for a specified location\",\n",
    "                input_parameters={\"city\": \"New York City\", \"units\": \"fahrenheit\", \"include_forecast\": False},\n",
    "                output={\n",
    "                    \"temperature\": 72,\n",
    "                    \"condition\": \"partly_cloudy\", \n",
    "                    \"humidity\": 60,\n",
    "                    \"wind_speed\": 8,\n",
    "                    \"wind_direction\": \"west\"\n",
    "                },\n",
    "                reasoning=\"User asked for current weather in NYC, so I need to call the weather API\"\n",
    "            )\n",
    "        ],\n",
    "        expected_tools=[\n",
    "            ToolCall(\n",
    "                name=\"WeatherAPI\",\n",
    "                description=\"Should fetch weather information for New York City\",\n",
    "                input_parameters={\"city\": \"New York City\"}\n",
    "            )\n",
    "        ],\n",
    "        retrieval_context=[\n",
    "            \"Temperature: 72°F, Condition: Partly Cloudy, Humidity: 60%, Wind: 8mph from west\",\n",
    "            \"No precipitation in forecast for today\",\n",
    "            \"Historical average temperature for this date: 70°F\"\n",
    "        ]\n",
    "    ),\n",
    "    LLMTestCase(\n",
    "        input=\"Calculate the compound interest on $1000 invested at 5% annually for 3 years\",\n",
    "        actual_output=\"\"\"Using the compound interest formula A = P(1 + r)^t, where P = $1000, r = 0.05, and t = 3:\n",
    "        A = 1000(1 + 0.05)^3 = 1000(1.05)^3 = 1000 × 1.157625 = $1,157.63\n",
    "        The investment will grow to $1,157.63, earning $157.63 in compound interest.\"\"\",\n",
    "        expected_output=\"The compound interest calculation shows the investment grows to approximately $1,157.63.\",\n",
    "        tools_called=[\n",
    "            ToolCall(\n",
    "                name=\"Calculator\",\n",
    "                description=\"Performs mathematical calculations\",\n",
    "                input_parameters={\"expression\": \"1000 * (1.05 ** 3)\", \"precision\": 2},\n",
    "                output=1157.63,\n",
    "                reasoning=\"Need to calculate compound interest using the standard formula\"\n",
    "            )\n",
    "        ],\n",
    "        expected_tools=[\n",
    "            ToolCall(\n",
    "                name=\"Calculator\", \n",
    "                description=\"Should perform compound interest calculation\",\n",
    "                input_parameters={\"calculation_type\": \"compound_interest\"}\n",
    "            )\n",
    "        ],\n",
    "        retrieval_context=[\n",
    "            \"Calculation result: $1,157.63\",\n",
    "            \"Formula used: A = P(1 + r)^t\",\n",
    "            \"Parameters: Principal=$1000, Rate=5%, Time=3 years\"\n",
    "        ]\n",
    "    ),\n",
    "    LLMTestCase(\n",
    "        input=\"Send an email to john@example.com about our meeting tomorrow at 2 PM\",\n",
    "        actual_output=\"\"\"I've sent an email to john@example.com with the subject \"Meeting Reminder - Tomorrow at 2 PM\". \n",
    "        The email confirms your meeting scheduled for tomorrow at 2:00 PM and asks John to confirm his attendance. \n",
    "        The email was successfully delivered to the recipient.\"\"\",\n",
    "        expected_output=\"Email sent successfully to john@example.com about the 2 PM meeting tomorrow.\",\n",
    "        tools_called=[\n",
    "            ToolCall(\n",
    "                name=\"EmailSender\",\n",
    "                description=\"Sends emails to specified recipients\",\n",
    "                input_parameters={\n",
    "                    \"to\": \"john@example.com\",\n",
    "                    \"subject\": \"Meeting Reminder - Tomorrow at 2 PM\", \n",
    "                    \"body\": \"Hi John,\\n\\nThis is a reminder about our meeting scheduled for tomorrow at 2:00 PM. Please confirm your attendance.\\n\\nBest regards\"\n",
    "                },\n",
    "                output={\"status\": \"sent\", \"message_id\": \"msg_12345\", \"timestamp\": \"2024-01-15T10:30:00Z\"},\n",
    "                reasoning=\"User requested to send email, so I need to use the email tool with appropriate content\"\n",
    "            )\n",
    "        ],\n",
    "        expected_tools=[\n",
    "            ToolCall(\n",
    "                name=\"EmailSender\",\n",
    "                description=\"Should send an email about the meeting\",\n",
    "                input_parameters={\"recipient\": \"john@example.com\"}\n",
    "            )\n",
    "        ],\n",
    "        retrieval_context=[\n",
    "            \"Email sent successfully (msg_12345)\",\n",
    "            \"Recipient: john@example.com\",\n",
    "            \"Subject: Meeting Reminder - Tomorrow at 2 PM\",\n",
    "            \"Timestamp: 2024-01-15T10:30:00Z\"\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "print(f\"Created {len(agent_test_cases)} Agent test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc6_2_\"></a>\n",
    "\n",
    "### Build dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Agent dataset\n",
    "agent_dataset = LLMAgentDataset.from_test_cases(\n",
    "    test_cases=agent_test_cases,\n",
    "    input_id=\"agent_evaluation_dataset\"\n",
    ")\n",
    "\n",
    "print(f\"Agent Dataset: {agent_dataset}\")\n",
    "print(f\"Shape: {agent_dataset.df.shape}\")\n",
    "\n",
    "# Analyze tool usage\n",
    "tool_usage = {}\n",
    "for case in agent_test_cases:\n",
    "    if case.tools_called:\n",
    "        for tool in case.tools_called:\n",
    "            tool_usage[tool.name] = tool_usage.get(tool.name, 0) + 1\n",
    "\n",
    "print(\"\\nTool Usage Analysis:\")\n",
    "for tool, count in tool_usage.items():\n",
    "    print(f\"  - {tool}: {count} times\")\n",
    "\n",
    "print(\"\\nAgent Dataset Preview:\")\n",
    "display(agent_dataset.df[['input', 'actual_output', 'tools_called']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_dataset.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc6_3_\"></a>\n",
    "\n",
    "### Evaluation metrics\n",
    "<a id=\"toc6_3_1_\"></a>\n",
    "\n",
    "#### Faithfulness\n",
    "The Faithfulness metric evaluates whether the model's output contains any contradictions or hallucinations compared to the provided context. It ensures that the model's response is grounded in and consistent with the given information, rather than making up facts or contradicting the context. A high faithfulness score indicates that the model's output aligns well with the source material.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_dataset.assign_scores(\n",
    "    metrics = \"validmind.scorers.llm.deepeval.Faithfulness\",\n",
    "    user_input_column = \"input\",\n",
    "    response_column = \"actual_output\",\n",
    "    retrieved_contexts_column = \"retrieval_context\",\n",
    "    )\n",
    "agent_dataset._df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc6_3_2_\"></a>\n",
    "\n",
    "#### Hallucination\n",
    "The Hallucination metric evaluates whether the model's output contains information that is not supported by or contradicts the provided context. It helps identify cases where the model makes up facts or includes details that aren't grounded in the source material. A low hallucination score indicates that the model's response stays faithful to the given context without introducing unsupported information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_dataset.assign_scores(\n",
    "    metrics = \"validmind.scorers.llm.deepeval.Hallucination\",\n",
    "    input_column = \"input\",\n",
    "    actual_output_column = \"actual_output\",\n",
    "    context_column = \"retrieval_context\",\n",
    ")\n",
    "agent_dataset._df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc6_3_3_\"></a>\n",
    "\n",
    "#### Summarization\n",
    "The Summarization metric evaluates how well a model's output summarizes the given context by generating assessment questions to check if the summary is factually aligned with and sufficiently covers the source text. It helps ensure that summaries are accurate, complete, and maintain the key information from the original content without introducing unsupported details or omitting critical points. A high summarization score indicates that the model effectively condenses the source material while preserving its essential meaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_dataset.assign_scores(\n",
    "    metrics = \"validmind.scorers.llm.deepeval.Summarization\",\n",
    "    input_column = \"input\",\n",
    "    actual_output_column = \"actual_output\",\n",
    ")\n",
    "agent_dataset._df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"toc6_3_4_\"></a>\n",
    "\n",
    "### AI Agent Evaluation Metrics\n",
    "\n",
    "AI agent evaluation metrics are specialized measurements designed to assess how well autonomous LLM-based agents reason, plan, select and execute tools, and ultimately complete user tasks by analyzing the **full execution trace**—including reasoning steps, tool calls, intermediate decisions, and outcomes—rather than just single input–output pairs.\n",
    "\n",
    "These metrics are essential because agent failures often occur in ways traditional LLM metrics miss (e.g., choosing the right tool with wrong arguments, creating a good plan but not following it, or completing a task inefficiently).\n",
    "\n",
    "**DeepEval’s AI agent evaluation framework** breaks evaluation into three layers with corresponding metric categories:\n",
    "\n",
    "1. **Reasoning Layer** – Evaluates planning and strategy generation:\n",
    "\n",
    "   * *PlanQualityMetric* – how logical, complete, and efficient the agent’s plan is\n",
    "   * *PlanAdherenceMetric* – whether the agent follows its own plan during execution \n",
    "\n",
    "2. **Action Layer** – Assesses tool usage and argument generation:\n",
    "\n",
    "   * *ToolCorrectnessMetric* – whether the agent selects and calls the right tools\n",
    "   * *ArgumentCorrectnessMetric* – whether the agent generates correct tool arguments\n",
    "\n",
    "3. **Execution Layer** – Measures end-to-end performance:\n",
    "\n",
    "   * *TaskCompletionMetric* – whether the agent successfully completes the intended task\n",
    "\n",
    "Together, these metrics enable granular diagnosis of agent behavior, help pinpoint where failures occur (reasoning, action, or execution), and support both development benchmarking and production monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **Reasoning Layer**\n",
    "#### PlanQualityMetric\n",
    "Let's measures how well the agent generates a plan before acting. A high score means the plan is logical, complete, and efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_dataset.assign_scores(\n",
    "    metrics = \"validmind.scorers.llm.deepeval.PlanQuality\",\n",
    "    input_column = \"input\",\n",
    "    actual_output_column = \"actual_output\",\n",
    "    tools_called_column = \"tools_called\",\n",
    ")\n",
    "agent_dataset._df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PlanAdherenceMetric\n",
    "Let's checks whether the agent follows the plan it created. Deviations lower this score and indicate gaps between reasoning and execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_dataset.assign_scores(\n",
    "    metrics = \"validmind.scorers.llm.deepeval.PlanAdherence\",\n",
    "    input_column = \"input\",\n",
    "    # actual_output_column = \"actual_output\",\n",
    "    agent_output_column = \"actual_output\",\n",
    "    tools_called_column = \"tools_called\",\n",
    ")\n",
    "agent_dataset._df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **Action Layer**\n",
    "#### ToolCorrectnessMetric\n",
    "Let's evaluates if the agent selects the appropriate tool for the task. Choosing the wrong tool reduces performance even if reasoning was correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_dataset.assign_scores(\n",
    "    metrics = \"validmind.scorers.llm.deepeval.ToolCorrectness\",\n",
    "    input_column = \"input\",\n",
    "    actual_output_column = \"actual_output\",\n",
    "    agent_output_column = \"actual_output\",\n",
    "    tools_called_column = \"tools_called\",\n",
    "    expected_tools_column = \"expected_tools\",\n",
    ")\n",
    "agent_dataset._df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ArgumentCorrectnessMetric\n",
    "Let's assesses whether the agent provides correct inputs or arguments to the selected tool. Incorrect arguments can lead to failed or unexpected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_dataset.assign_scores(\n",
    "    metrics = \"validmind.scorers.llm.deepeval.ArgumentCorrectness\",\n",
    "    input_column = \"input\",\n",
    "    actual_output_column = \"actual_output\",\n",
    "    tools_called_column = \"tools_called\",\n",
    ")\n",
    "agent_dataset._df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **Execution Layer**\n",
    "#### TaskCompletionMetric\n",
    "Let's measures whether the agent successfully completes the overall task. This is the ultimate indicator of success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_dataset.assign_scores(\n",
    "    metrics = \"validmind.scorers.llm.deepeval.TaskCompletion\",\n",
    "    input_column = \"input\",\n",
    "    actual_output_column = \"actual_output\",\n",
    "    agent_output_column = \"agent_output\",\n",
    "    tools_called_column = \"tools_called\",\n",
    "\n",
    ")\n",
    "agent_dataset._df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<a id=\"toc10_\"></a>\n",
    "\n",
    "## In summary\n",
    "\n",
    "This notebook demonstrated the comprehensive integration between DeepEval and ValidMind for LLM evaluation:\n",
    "\n",
    "**Key Achievements:**\n",
    "- Successfully created and evaluated different types of LLM test cases (Q&A, RAG, Agents)\n",
    "- Integrated DeepEval metrics with ValidMind's testing infrastructure\n",
    "- Showed how to handle complex agent scenarios with tool usage\n",
    "\n",
    "**Integration Benefits:**\n",
    "- **Comprehensive Coverage**: Evaluate LLMs across 30+ specialized metrics\n",
    "- **Structured Documentation**: Leverage ValidMind's compliance and documentation features\n",
    "- **Flexibility**: Support for custom metrics and domain-specific evaluation criteria\n",
    "- **Production Ready**: Handle real-world LLM evaluation scenarios at scale\n",
    "\n",
    "The `LLMAgentDataset` class provides a seamless bridge between DeepEval's evaluation capabilities and ValidMind's testing infrastructure, enabling robust LLM evaluation within a structured, compliant framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "<a id=\"toc11_\"></a>\n",
    "\n",
    "## Next steps\n",
    "\n",
    "**Explore Advanced Features:**\n",
    "- **Continuous Evaluation**: Set up automated LLM evaluation pipelines\n",
    "- **A/B Testing**: Compare different LLM models and configurations\n",
    "- **Metrics Customization**: Create domain-specific evaluation criteria\n",
    "- **Integration Patterns**: Embed evaluation into your LLM development workflow\n",
    "\n",
    "**Additional Resources:**\n",
    "- [ValidMind Library Documentation](https://docs.validmind.ai/developer/validmind-library.html) - Complete API reference and tutorials\n",
    "\n",
    "**Try These Examples:**\n",
    "- Implement custom business-specific evaluation metrics\n",
    "- Create automated evaluation pipelines for model deployment\n",
    "- Integrate with your existing ML infrastructure and workflows\n",
    "- Explore multi-modal evaluation scenarios (text, code, images)\n",
    "\n",
    "Start building comprehensive LLM evaluation workflows that combine the power of DeepEval's specialized metrics with ValidMind's structured testing and documentation framework.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyright-94c232c772c1435aa4529b67cfcc0bb2",
   "metadata": {},
   "source": [
    "<!-- VALIDMIND COPYRIGHT -->\n",
    "\n",
    "<small>\n",
    "\n",
    "***\n",
    "\n",
    "Copyright © 2023-2026 ValidMind Inc. All rights reserved.<br>\n",
    "Refer to [LICENSE](https://github.com/validmind/validmind-library/blob/main/LICENSE) for details.<br>\n",
    "SPDX-License-Identifier: AGPL-3.0 AND ValidMind Commercial</small>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "validmind-1QuffXMV-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# DeepEval Integration with ValidMind\n",
        "\n",
        "Let's learn how to integrate [DeepEval](https://github.com/confident-ai/deepeval) with the ValidMind Library to evaluate Large Language Models (LLMs) and AI agents. This notebook demonstrates how to use DeepEval's summarization metrics within ValidMind's testing infrastructure.\n",
        "\n",
        "To integrate DeepEval with ValidMind, we'll:\n",
        " 1. Set up both frameworks and install required dependencies\n",
        " 2. Create a dataset with source texts and generated summaries\n",
        " 3. Use ValidMind's Summarization scorer to evaluate summary quality\n",
        " 4. Analyze the evaluation results and reasons\n",
        " 5. Apply the evaluation pipeline to multiple examples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Contents    \n",
        "- [Introduction](#toc1_)    \n",
        "- [About DeepEval Integration](#toc2_)    \n",
        "  - [Before you begin](#toc2_1_)    \n",
        "  - [Key concepts](#toc2_2_)    \n",
        "- [Setting up](#toc3_)    \n",
        "  - [Install required packages](#toc3_1_)    \n",
        "  - [Initialize ValidMind](#toc3_2_)    \n",
        "- [Basic Usage - Simple Q&A Evaluation](#toc4_)    \n",
        "- [RAG System Evaluation](#toc5_)    \n",
        "  - [Create test cases](#toc5_1_)    \n",
        "  - [Build dataset](#toc5_2_)    \n",
        "  - [Evaluation metrics](#toc5_3_)    \n",
        "    - [Contextual Relevancy](#toc5_3_1_)    \n",
        "    - [Contextual Precision](#toc5_3_2_)    \n",
        "    - [Contextual Recall](#toc5_3_3_)    \n",
        "- [LLM Agent Evaluation](#toc6_)    \n",
        "  - [Create test cases](#toc6_1_)    \n",
        "  - [Build dataset](#toc6_2_)    \n",
        "  - [Evaluation metrics](#toc6_3_)    \n",
        "    - [Faithfulness](#toc6_3_1_)    \n",
        "    - [Hallucination](#toc6_3_2_)    \n",
        "    - [Summarization](#toc6_3_3_)    \n",
        "    - [Task Completion](#toc6_3_4_)    \n",
        "- [Working with Golden Templates](#toc7_)    \n",
        "  - [Convert to test cases](#toc7_1_)    \n",
        "  - [Integrate with ValidMind](#toc7_2_)    \n",
        "- [Custom Metrics with G-Eval](#toc9_)    \n",
        "- [In summary](#toc10_)    \n",
        "- [Next steps](#toc11_)    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc1_\"></a>\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Large Language Model (LLM) evaluation is critical for understanding model performance across different tasks and scenarios. This notebook demonstrates how to integrate DeepEval's comprehensive evaluation framework with ValidMind's testing infrastructure to create a robust LLM evaluation pipeline.\n",
        "\n",
        "DeepEval provides over 30 evaluation metrics specifically designed for LLMs, covering scenarios from simple Q&A to complex agent interactions. By integrating with ValidMind, you can leverage these metrics within a structured testing framework that supports documentation, collaboration, and compliance requirements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc2_\"></a>\n",
        "\n",
        "## About DeepEval Integration\n",
        "\n",
        "DeepEval is a comprehensive evaluation framework for LLMs that provides metrics for various scenarios including hallucination detection, answer relevancy, faithfulness, and custom evaluation criteria. ValidMind is a platform for managing model risk and documentation through automated testing.\n",
        "\n",
        "Together, these tools enable comprehensive LLM evaluation within a structured, compliant framework.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc2_1_\"></a>\n",
        "\n",
        "### Before you begin\n",
        "\n",
        "This notebook assumes you have basic familiarity with Python and Large Language Models. You'll need:\n",
        "\n",
        "- Python 3.8 or higher\n",
        "- Access to OpenAI API (for DeepEval metrics evaluation)\n",
        "- ValidMind account and model registration\n",
        "\n",
        "If you encounter errors due to missing modules, install them with `pip install` and re-run the notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc2_2_\"></a>\n",
        "\n",
        "### Key concepts\n",
        "\n",
        "**LLMTestCase**: A DeepEval object that represents a single test case with input, expected output, actual output, and optional context.\n",
        "\n",
        "**Golden Templates**: Pre-defined test templates with inputs and expected outputs that can be converted to test cases by generating actual outputs.\n",
        "\n",
        "**G-Eval**: Generative evaluation using LLMs to assess response quality based on custom criteria.\n",
        "\n",
        "**LLMAgentDataset**: A ValidMind dataset class that bridges DeepEval test cases with ValidMind's testing infrastructure.\n",
        "\n",
        "**RAG Evaluation**: Testing retrieval-augmented generation systems that combine document retrieval with generation.\n",
        "\n",
        "**Agent Evaluation**: Testing LLM agents that can use tools and perform multi-step reasoning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc3_\"></a>\n",
        "\n",
        "## Setting up\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc3_1_\"></a>\n",
        "\n",
        "### Install required packages\n",
        "\n",
        "First, let's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q validmind"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc3_2_\"></a>\n",
        "\n",
        "### Initialize ValidMind\n",
        "\n",
        "ValidMind generates a unique _code snippet_ for each registered model to connect with your developer environment. You initialize the ValidMind Library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\n",
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"background-color: #B5B5B510; color: black; border: 1px solid #083E44; border-left-width: 5px; box-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);border-radius: 5px;\"><span style=\"color: #083E44;\"><b>For access to all features available in this notebook, you'll need access to a ValidMind account.</b></span>\n",
        "<br></br>\n",
        "<a href=\"https://docs.validmind.ai/guide/configuration/register-with-validmind.html\" style=\"color: #DE257E;\"><b>Register with ValidMind</b></a></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load your model identifier credentials from an `.env` file\n",
        "%load_ext dotenv\n",
        "%dotenv .env\n",
        "\n",
        "# Or replace with your code snippet\n",
        "import validmind as vm\n",
        "\n",
        "vm.init(\n",
        "    api_host=\"...\",\n",
        "    api_key=\"...\",\n",
        "    api_secret=\"...\",\n",
        "    model=\"...\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from deepeval.test_case import LLMTestCase, ToolCall\n",
        "from deepeval.dataset import Golden\n",
        "from validmind.datasets.llm import LLMAgentDataset\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc4_\"></a>\n",
        "\n",
        "## Basic Usage - Simple Q&A Evaluation\n",
        "\n",
        "Let's start with the simplest use case: evaluating a basic question-and-answer interaction with an LLM. This demonstrates how to create LLMTestCase objects and integrate them with ValidMind's dataset infrastructure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a simple LLM test case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "simple_test_cases = [\n",
        "LLMTestCase(\n",
        "    input=\"What is machine learning?\",\n",
        "    actual_output=\"\"\"Machine learning is a subset of artificial intelligence (AI) that enables \n",
        "    computers to learn and make decisions from data without being explicitly programmed for every task. \n",
        "    It uses algorithms to find patterns in data and make predictions or decisions based on those patterns.\"\"\",\n",
        "    expected_output=\"\"\"Machine learning is a method of data analysis that automates analytical \n",
        "    model building. It uses algorithms that iteratively learn from data, allowing computers to find \n",
        "    hidden insights without being explicitly programmed where to look.\"\"\",\n",
        "    context=[\"Machine learning is a branch of AI that focuses on algorithms that can learn from data.\"],\n",
        "    retrieval_context=[\"Machine learning is a branch of AI that focuses on algorithms that can learn from data.\"],\n",
        "    tools_called=[\n",
        "        ToolCall(\n",
        "            name=\"search_docs\",\n",
        "            args={\"query\": \"machine learning definition\"},\n",
        "            response=\"Found definition of machine learning in documentation.\"\n",
        "        )\n",
        "    ]\n",
        "),\n",
        "LLMTestCase(\n",
        "    input=\"What is deep learning?\",\n",
        "    actual_output=\"\"\"Bananas are yellow fruits that grow on trees in tropical climates. \n",
        "    They are rich in potassium and make a great healthy snack. You can also use them \n",
        "    in smoothies and baking.\"\"\",\n",
        "    expected_output=\"\"\"Deep learning is an advanced machine learning technique that uses neural networks\n",
        "    with many layers to automatically learn representations of data with multiple levels of abstraction.\n",
        "    It has enabled major breakthroughs in AI applications.\"\"\",\n",
        "    context=[\"Deep learning is a specialized machine learning approach that uses deep neural networks to learn from data.\"],\n",
        "    retrieval_context=[\"Deep learning is a specialized machine learning approach that uses deep neural networks to learn from data.\"],\n",
        "    tools_called=[\n",
        "        ToolCall(\n",
        "            name=\"search_docs\", \n",
        "            args={\"query\": \"deep learning definition\"},\n",
        "            response=\"Found definition of deep learning in documentation.\"\n",
        "        )\n",
        "    ]\n",
        ")]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create LLMAgentDataset from the test case\n",
        "Let's create ValidMind dataset from Deepeval's test cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nCreating ValidMind dataset...\")\n",
        "\n",
        "simple_dataset = LLMAgentDataset.from_test_cases(\n",
        "    test_cases=simple_test_cases,\n",
        "    input_id=\"simple_qa_dataset\"\n",
        ")\n",
        "\n",
        "\n",
        "# Display the dataset\n",
        "pd.set_option('display.max_colwidth', 40)\n",
        "pd.set_option('display.width', 120)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "print(\"\\nDataset preview:\")\n",
        "display(simple_dataset.df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compute metrics using ValidMind scorer interface\n",
        "Now we'll compute metrics on our dataset using ValidMind's scorer interface. This will help us evaluate how well our model is performing by calculating various metrics like answer relevancy. The scorer interface provides a standardized way to assess model outputs against expected results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "simple_dataset.assign_scores(metrics = \"validmind.scorer.llm.deepeval.AnswerRelevancy\")\n",
        "simple_dataset._df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "simple_dataset.assign_scores(metrics = \"validmind.scorer.llm.deepeval.Bias\")\n",
        "simple_dataset._df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc5_\"></a>\n",
        "\n",
        "## RAG System Evaluation\n",
        "\n",
        "Now let's evaluate a more complex use case: a Retrieval-Augmented Generation (RAG) system that retrieves relevant documents and generates responses based on them. RAG systems combine document retrieval with text generation, requiring specialized evaluation approaches.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"toc5_1_\"></a>\n",
        "\n",
        "### Create test cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print(\"Creating RAG evaluation test cases...\")\n",
        "rag_test_cases = [\n",
        "    LLMTestCase(\n",
        "        input=\"How do I return a product that doesn't fit?\",\n",
        "        actual_output=\"\"\"You can return any product within 30 days of purchase for a full refund. \n",
        "        Simply visit our returns page on the website and follow the step-by-step instructions. \n",
        "        You'll need your order number and email address. No questions asked!\"\"\",\n",
        "        expected_output=\"We offer a 30-day return policy for full refunds. Visit our returns page to start the process.\",\n",
        "        context=[\"Company policy allows 30-day returns for full refund with no restocking fees.\"],\n",
        "        retrieval_context=[\n",
        "            \"Return Policy: All items can be returned within 30 days of purchase for a full refund.\",\n",
        "            \"Return Process: Visit our website's returns page and enter your order details.\",\n",
        "            \"Customer Service: Available 24/7 to help with returns and refunds.\",\n",
        "            \"No restocking fees apply to returns within the 30-day window.\"\n",
        "        ]\n",
        "    ),\n",
        "    LLMTestCase(\n",
        "        input=\"What are your shipping options and costs?\",\n",
        "        actual_output=\"\"\"We offer three shipping options: Standard (5-7 days, $5.99), \n",
        "        Express (2-3 days, $9.99), and Overnight (next day, $19.99). \n",
        "        Free shipping is available on orders over $50 with Standard delivery.\"\"\",\n",
        "        expected_output=\"Multiple shipping options available with costs ranging from $5.99 to $19.99. Free shipping on orders over $50.\",\n",
        "        context=[\"Shipping information includes various speed and cost options.\"],\n",
        "        retrieval_context=[\n",
        "            \"Standard Shipping: 5-7 business days, $5.99\",\n",
        "            \"Express Shipping: 2-3 business days, $9.99\", \n",
        "            \"Overnight Shipping: Next business day, $19.99\",\n",
        "            \"Free Standard Shipping on orders over $50\"\n",
        "        ]\n",
        "    ),\n",
        "    LLMTestCase(\n",
        "        input=\"Do you have a warranty on electronics?\",\n",
        "        actual_output=\"\"\"Yes, all electronics come with a manufacturer's warranty. \n",
        "        Most items have a 1-year warranty, while premium products may have 2-3 years. \n",
        "        We also offer extended warranty options for purchase.\"\"\",\n",
        "        expected_output=\"Electronics include manufacturer warranty, typically 1-year, with extended options available.\",\n",
        "        context=[\"Electronics warranty information varies by product type and manufacturer.\"],\n",
        "        retrieval_context=[\n",
        "            \"Electronics Warranty: Manufacturer warranty included with all electronic items\",\n",
        "            \"Standard Coverage: 1 year for most electronics\",\n",
        "            \"Premium Products: May include 2-3 year coverage\",\n",
        "            \"Extended Warranty: Available for purchase at checkout\"\n",
        "        ]\n",
        "    )\n",
        "]\n",
        "\n",
        "print(f\"Created {len(rag_test_cases)} RAG test cases\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"toc5_2_\"></a>\n",
        "\n",
        "### Build dataset\n",
        "\n",
        "In this section, we'll convert our Deepeval LLMTestCase objects into a ValidMind dataset format.\n",
        "This allows us to leverage ValidMind's powerful evaluation capabilities while maintaining \n",
        "compatibility with Deepeval's test case structure.\n",
        "\n",
        "The dataset will contain:\n",
        "- Input queries\n",
        "- Actual model outputs \n",
        "- Expected outputs\n",
        "- Context information\n",
        "- Retrieved context passages\n",
        "\n",
        "This structured format enables detailed analysis of the RAG system's performance\n",
        "across multiple evaluation dimensions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rag_dataset = LLMAgentDataset.from_test_cases(\n",
        "    test_cases=rag_test_cases,\n",
        "    input_id=\"rag_evaluation_dataset\"\n",
        ")\n",
        "\n",
        "print(f\"RAG Dataset: {rag_dataset}\")\n",
        "print(f\"Shape: {rag_dataset.df.shape}\")\n",
        "\n",
        "# Show dataset structure\n",
        "print(\"\\nRAG Dataset Preview:\")\n",
        "display(rag_dataset.df[['input', 'actual_output', 'context', 'retrieval_context']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"toc5_3_\"></a>\n",
        "\n",
        "### Evaluation metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"toc5_3_1_\"></a>\n",
        "\n",
        "#### Contextual Relevancy\n",
        "The Contextual Relevancy metric evaluates how well the retrieved context aligns with the input query.\n",
        "It measures whether the context contains the necessary information to answer the query accurately.\n",
        "A high relevancy score indicates that the retrieved context is highly relevant and contains the key information needed.\n",
        "This helps validate that the RAG system is retrieving appropriate context for the given queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rag_dataset.assign_scores(metrics = \"validmind.scorer.llm.deepeval.ContextualRelevancy\")\n",
        "display(rag_dataset._df.head(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"toc5_3_2_\"></a>\n",
        "\n",
        "#### Contextual Precision\n",
        "The Contextual Precision metric evaluates how well a RAG system ranks retrieved context nodes by relevance to the input query. \n",
        "It checks if the most relevant nodes are ranked at the top of the retrieval results.\n",
        "A high precision score indicates that the retrieved context is highly relevant to the query and properly ranked.\n",
        "This is particularly useful for evaluating RAG systems and ensuring they surface the most relevant information first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rag_dataset.assign_scores(metrics = \"validmind.scorer.llm.deepeval.ContextualPrecision\")\n",
        "rag_dataset._df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"toc5_3_3_\"></a>\n",
        "\n",
        "#### Contextual Recall\n",
        "The Contextual Recall metric evaluates how well the retrieved context covers all the information needed to generate the expected output.\n",
        "It extracts statements from the expected output and checks how many of them can be attributed to the retrieved context.\n",
        "A high recall score indicates that the retrieved context contains all the key information needed to generate the expected response.\n",
        "This helps ensure the RAG system retrieves comprehensive context that covers all aspects of the expected answer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rag_dataset.assign_scores(metrics = \"validmind.scorer.llm.deepeval.ContextualRecall\")\n",
        "rag_dataset._df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc6_\"></a>\n",
        "\n",
        "## LLM Agent Evaluation\n",
        "\n",
        "Let's evaluate LLM agents that can use tools to accomplish tasks. This is one of the most advanced evaluation scenarios, requiring assessment of both response quality and tool usage appropriateness.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"toc6_1_\"></a>\n",
        "\n",
        "### Create test cases\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"toc6_2_\"></a>\n",
        "\n",
        "### Build dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create LLM Agent test cases with tool usage\n",
        "print(\"Creating Agent evaluation test cases...\")\n",
        "\n",
        "# Create test cases\n",
        "agent_test_cases = [\n",
        "    LLMTestCase(\n",
        "        input=\"What's the weather like in New York City today?\",\n",
        "        actual_output=\"\"\"Based on current weather data, New York City is experiencing partly cloudy skies \n",
        "        with a temperature of 72°F (22°C). The humidity is at 60% and there's a light breeze from the west at 8 mph. \n",
        "        No precipitation is expected today.\"\"\",\n",
        "        expected_output=\"Current weather in New York shows mild temperatures with partly cloudy conditions.\",\n",
        "        tools_called=[\n",
        "            ToolCall(\n",
        "                name=\"WeatherAPI\",\n",
        "                description=\"Fetches current weather information for a specified location\",\n",
        "                input_parameters={\"city\": \"New York City\", \"units\": \"fahrenheit\", \"include_forecast\": False},\n",
        "                output={\n",
        "                    \"temperature\": 72,\n",
        "                    \"condition\": \"partly_cloudy\", \n",
        "                    \"humidity\": 60,\n",
        "                    \"wind_speed\": 8,\n",
        "                    \"wind_direction\": \"west\"\n",
        "                },\n",
        "                reasoning=\"User asked for current weather in NYC, so I need to call the weather API\"\n",
        "            )\n",
        "        ],\n",
        "        expected_tools=[\n",
        "            ToolCall(\n",
        "                name=\"WeatherAPI\",\n",
        "                description=\"Should fetch weather information for New York City\",\n",
        "                input_parameters={\"city\": \"New York City\"}\n",
        "            )\n",
        "        ],\n",
        "        retrieval_context=[\n",
        "            \"Temperature: 72°F, Condition: Partly Cloudy, Humidity: 60%, Wind: 8mph from west\",\n",
        "            \"No precipitation in forecast for today\",\n",
        "            \"Historical average temperature for this date: 70°F\"\n",
        "        ]\n",
        "    ),\n",
        "    LLMTestCase(\n",
        "        input=\"Calculate the compound interest on $1000 invested at 5% annually for 3 years\",\n",
        "        actual_output=\"\"\"Using the compound interest formula A = P(1 + r)^t, where P = $1000, r = 0.05, and t = 3:\n",
        "        A = 1000(1 + 0.05)^3 = 1000(1.05)^3 = 1000 × 1.157625 = $1,157.63\n",
        "        The investment will grow to $1,157.63, earning $157.63 in compound interest.\"\"\",\n",
        "        expected_output=\"The compound interest calculation shows the investment grows to approximately $1,157.63.\",\n",
        "        tools_called=[\n",
        "            ToolCall(\n",
        "                name=\"Calculator\",\n",
        "                description=\"Performs mathematical calculations\",\n",
        "                input_parameters={\"expression\": \"1000 * (1.05 ** 3)\", \"precision\": 2},\n",
        "                output=1157.63,\n",
        "                reasoning=\"Need to calculate compound interest using the standard formula\"\n",
        "            )\n",
        "        ],\n",
        "        expected_tools=[\n",
        "            ToolCall(\n",
        "                name=\"Calculator\", \n",
        "                description=\"Should perform compound interest calculation\",\n",
        "                input_parameters={\"calculation_type\": \"compound_interest\"}\n",
        "            )\n",
        "        ],\n",
        "        retrieval_context=[\n",
        "            \"Calculation result: $1,157.63\",\n",
        "            \"Formula used: A = P(1 + r)^t\",\n",
        "            \"Parameters: Principal=$1000, Rate=5%, Time=3 years\"\n",
        "        ]\n",
        "    ),\n",
        "    LLMTestCase(\n",
        "        input=\"Send an email to john@example.com about our meeting tomorrow at 2 PM\",\n",
        "        actual_output=\"\"\"I've sent an email to john@example.com with the subject \"Meeting Reminder - Tomorrow at 2 PM\". \n",
        "        The email confirms your meeting scheduled for tomorrow at 2:00 PM and asks John to confirm his attendance. \n",
        "        The email was successfully delivered to the recipient.\"\"\",\n",
        "        expected_output=\"Email sent successfully to john@example.com about the 2 PM meeting tomorrow.\",\n",
        "        tools_called=[\n",
        "            ToolCall(\n",
        "                name=\"EmailSender\",\n",
        "                description=\"Sends emails to specified recipients\",\n",
        "                input_parameters={\n",
        "                    \"to\": \"john@example.com\",\n",
        "                    \"subject\": \"Meeting Reminder - Tomorrow at 2 PM\", \n",
        "                    \"body\": \"Hi John,\\n\\nThis is a reminder about our meeting scheduled for tomorrow at 2:00 PM. Please confirm your attendance.\\n\\nBest regards\"\n",
        "                },\n",
        "                output={\"status\": \"sent\", \"message_id\": \"msg_12345\", \"timestamp\": \"2024-01-15T10:30:00Z\"},\n",
        "                reasoning=\"User requested to send email, so I need to use the email tool with appropriate content\"\n",
        "            )\n",
        "        ],\n",
        "        expected_tools=[\n",
        "            ToolCall(\n",
        "                name=\"EmailSender\",\n",
        "                description=\"Should send an email about the meeting\",\n",
        "                input_parameters={\"recipient\": \"john@example.com\"}\n",
        "            )\n",
        "        ],\n",
        "        retrieval_context=[\n",
        "            \"Email sent successfully (msg_12345)\",\n",
        "            \"Recipient: john@example.com\",\n",
        "            \"Subject: Meeting Reminder - Tomorrow at 2 PM\",\n",
        "            \"Timestamp: 2024-01-15T10:30:00Z\"\n",
        "        ]\n",
        "    )\n",
        "]\n",
        "\n",
        "print(f\"Created {len(agent_test_cases)} Agent test cases\")\n",
        "\n",
        "# Build dataset\n",
        "# Create Agent dataset\n",
        "agent_dataset = LLMAgentDataset.from_test_cases(\n",
        "    test_cases=agent_test_cases,\n",
        "    input_id=\"agent_evaluation_dataset\"\n",
        ")\n",
        "\n",
        "print(f\"Agent Dataset: {agent_dataset}\")\n",
        "print(f\"Shape: {agent_dataset.df.shape}\")\n",
        "\n",
        "# Analyze tool usage\n",
        "tool_usage = {}\n",
        "for case in agent_test_cases:\n",
        "    if case.tools_called:\n",
        "        for tool in case.tools_called:\n",
        "            tool_usage[tool.name] = tool_usage.get(tool.name, 0) + 1\n",
        "\n",
        "print(\"\\nTool Usage Analysis:\")\n",
        "for tool, count in tool_usage.items():\n",
        "    print(f\"  - {tool}: {count} times\")\n",
        "\n",
        "print(\"\\nAgent Dataset Preview:\")\n",
        "display(agent_dataset.df[['input', 'actual_output', 'tools_called']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"toc6_3_\"></a>\n",
        "\n",
        "### Evaluation metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"toc6_3_1_\"></a>\n",
        "\n",
        "#### Faithfulness\n",
        "The Faithfulness metric evaluates whether the model's output contains any contradictions or hallucinations compared to the provided context. It ensures that the model's response is grounded in and consistent with the given information, rather than making up facts or contradicting the context. A high faithfulness score indicates that the model's output aligns well with the source material.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent_dataset.assign_scores(metrics = \"validmind.scorer.llm.deepeval.Faithfulness\")\n",
        "agent_dataset._df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"toc6_3_2_\"></a>\n",
        "\n",
        "#### Hallucination\n",
        "The Hallucination metric evaluates whether the model's output contains information that is not supported by or contradicts the provided context. It helps identify cases where the model makes up facts or includes details that aren't grounded in the source material. A low hallucination score indicates that the model's response stays faithful to the given context without introducing unsupported information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent_dataset.assign_scores(metrics = \"validmind.scorer.llm.deepeval.Hallucination\", context_column=\"retrieval_context\")\n",
        "agent_dataset._df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"toc6_3_3_\"></a>\n",
        "\n",
        "#### Summarization\n",
        "The Summarization metric evaluates how well a model's output summarizes the given context by generating assessment questions to check if the summary is factually aligned with and sufficiently covers the source text. It helps ensure that summaries are accurate, complete, and maintain the key information from the original content without introducing unsupported details or omitting critical points. A high summarization score indicates that the model effectively condenses the source material while preserving its essential meaning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent_dataset.assign_scores(metrics = \"validmind.scorer.llm.deepeval.Summarization\")\n",
        "agent_dataset._df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"toc6_3_4_\"></a>\n",
        "\n",
        "#### Task Completion\n",
        "The Task Completion metric evaluates whether the model's output successfully accomplishes the intended task or goal specified in the input prompt. It assesses if the model has properly understood the task requirements and provided a complete and appropriate response. A high task completion score indicates that the model has effectively addressed the core objective of the prompt and delivered a satisfactory solution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent_dataset.assign_scores(metrics = \"validmind.scorer.llm.deepeval.TaskCompletion\", tools_called_column=\"tools_called\")\n",
        "agent_dataset._df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc7_\"></a>\n",
        "\n",
        "## Working with Golden Templates\n",
        "\n",
        "Golden templates are a powerful feature of DeepEval that allow you to define test inputs and expected outputs, then generate actual outputs at evaluation time. This approach enables systematic testing across multiple scenarios.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id=\"toc7_1_\"></a>\n",
        "\n",
        "### Convert to test cases\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Golden templates\n",
        "print(\"Creating Golden templates...\")\n",
        "\n",
        "goldens = [\n",
        "    Golden(\n",
        "        input=\"Explain the concept of neural networks in simple terms\",\n",
        "        expected_output=\"Neural networks are computing systems inspired by biological neural networks that constitute animal brains.\",\n",
        "        context=[\"Neural networks are a key component of machine learning and artificial intelligence.\"]\n",
        "    ),\n",
        "    Golden(\n",
        "        input=\"What are the main benefits of cloud computing for businesses?\", \n",
        "        expected_output=\"Cloud computing offers scalability, cost-effectiveness, accessibility, and reduced infrastructure maintenance.\",\n",
        "        context=[\"Cloud computing provides on-demand access to computing resources over the internet.\"]\n",
        "    ),\n",
        "    Golden(\n",
        "        input=\"How does password encryption protect user data?\",\n",
        "        expected_output=\"Password encryption converts passwords into unreadable formats using cryptographic algorithms, protecting against unauthorized access.\",\n",
        "        context=[\"Encryption is a fundamental security technique used to protect sensitive information.\"]\n",
        "    ),\n",
        "    Golden(\n",
        "        input=\"What is the difference between machine learning and deep learning?\",\n",
        "        expected_output=\"Machine learning is a broad field of AI, while deep learning is a subset that uses neural networks with multiple layers.\",\n",
        "        context=[\"Both are important areas of artificial intelligence with different approaches and applications.\"]\n",
        "    )\n",
        "]\n",
        "\n",
        "print(f\"Created {len(goldens)} Golden templates\")\n",
        "\n",
        "# Create dataset from goldens\n",
        "golden_dataset = LLMAgentDataset.from_goldens(\n",
        "    goldens=goldens,\n",
        "    input_id=\"golden_templates_dataset\"\n",
        ")\n",
        "\n",
        "print(f\"Golden Dataset: {golden_dataset}\")\n",
        "print(f\"Shape: {golden_dataset.df.shape}\")\n",
        "\n",
        "print(\"\\nGolden Templates Preview:\")\n",
        "display(golden_dataset.df[['input', 'expected_output', 'context', 'type']].head())\n",
        "\n",
        "# Mock LLM application function for demonstration\n",
        "def mock_llm_application(input_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Simulate an LLM application generating responses.\n",
        "    In production, this would be your actual LLM application.\n",
        "    \"\"\"\n",
        "    \n",
        "    responses = {\n",
        "        \"neural networks\": \"\"\"Neural networks are computational models inspired by the human brain. \n",
        "        They consist of interconnected nodes (neurons) that process information by learning patterns from data. \n",
        "        These networks can recognize complex patterns and make predictions, making them useful for tasks like \n",
        "        image recognition, natural language processing, and decision-making.\"\"\",\n",
        "        \n",
        "        \"cloud computing\": \"\"\"Cloud computing provides businesses with flexible, scalable access to computing resources \n",
        "        over the internet. Key benefits include reduced upfront costs, automatic scaling based on demand, \n",
        "        improved collaboration through shared access, enhanced security through professional data centers, \n",
        "        and reduced need for internal IT maintenance.\"\"\",\n",
        "        \n",
        "        \"password encryption\": \"\"\"Password encryption protects user data by converting passwords into complex, \n",
        "        unreadable strings using mathematical algorithms. When you enter your password, it's immediately encrypted \n",
        "        before storage or transmission. Even if data is intercepted, the encrypted password appears as random characters, \n",
        "        making it virtually impossible for attackers to determine the original password.\"\"\",\n",
        "        \n",
        "        \"machine learning\": \"\"\"Machine learning is a broad approach to artificial intelligence where computers learn \n",
        "        to make predictions or decisions by finding patterns in data. Deep learning is a specialized subset that uses \n",
        "        artificial neural networks with multiple layers (hence 'deep') to process information in ways that mimic \n",
        "        human brain function, enabling more sophisticated pattern recognition and decision-making.\"\"\"\n",
        "    }\n",
        "    \n",
        "    # Simple keyword matching for demonstration\n",
        "    input_lower = input_text.lower()\n",
        "    for keyword, response in responses.items():\n",
        "        if keyword in input_lower:\n",
        "            return response.strip()\n",
        "    \n",
        "    return f\"Thank you for your question about: {input_text}. I'd be happy to provide a comprehensive answer based on current knowledge and best practices.\"\n",
        "\n",
        "print(f\"\\nMock LLM application ready - will generate responses for {len(goldens)} templates\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert goldens to test cases by generating actual outputs\n",
        "print(\"Converting Golden templates to test cases...\")\n",
        "\n",
        "print(\"Before conversion:\")\n",
        "print(f\"  - Test cases: {len(golden_dataset.test_cases)}\")\n",
        "print(f\"  - Goldens: {len(golden_dataset.goldens)}\")\n",
        "\n",
        "# Convert goldens to test cases using our mock LLM\n",
        "golden_dataset.convert_goldens_to_test_cases(mock_llm_application)\n",
        "\n",
        "print(\"\\nAfter conversion:\")\n",
        "print(f\"  - Test cases: {len(golden_dataset.test_cases)}\")\n",
        "print(f\"  - Goldens: {len(golden_dataset.goldens)}\")\n",
        "\n",
        "print(\"\\nConversion completed!\")\n",
        "\n",
        "# Show the updated dataset\n",
        "print(\"\\nUpdated Dataset with Generated Outputs:\")\n",
        "dataset_df = golden_dataset.df\n",
        "# Filter for rows with actual output\n",
        "mask = pd.notna(dataset_df['actual_output']) & (dataset_df['actual_output'] != '')\n",
        "converted_df = dataset_df[mask]\n",
        "\n",
        "if not converted_df.empty:\n",
        "    display(converted_df[['input', 'actual_output', 'expected_output']])\n",
        "    \n",
        "    # Analyze output lengths using pandas string methods\n",
        "    actual_lengths = pd.Series([len(str(x)) for x in converted_df['actual_output']])\n",
        "    expected_lengths = pd.Series([len(str(x)) for x in converted_df['expected_output']])\n",
        "else:\n",
        "    print(\"No converted test cases found\")\n",
        "\n",
        "print(f\"\\nOutput Analysis:\")\n",
        "print(f\"Average actual output length: {actual_lengths.mean():.0f} characters\")\n",
        "print(f\"Average expected output length: {expected_lengths.mean():.0f} characters\")\n",
        "print(f\"Ratio (actual/expected): {(actual_lengths.mean() / expected_lengths.mean()):.2f}x\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc7_2\"></a>\n",
        "\n",
        "### Integrate with ValidMind\n",
        "\n",
        "Now let's demonstrate how to integrate our LLMAgentDataset with ValidMind's testing framework, enabling comprehensive documentation and compliance features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize ValidMind\n",
        "print(\"Integrating with ValidMind framework...\")\n",
        "\n",
        "try:\n",
        "    # Initialize ValidMind\n",
        "    vm.init()\n",
        "    print(\"ValidMind initialized\")\n",
        "    \n",
        "    # Register our datasets with ValidMind\n",
        "    datasets_to_register = [\n",
        "        (simple_dataset, \"simple_qa_dataset\"),\n",
        "        (rag_dataset, \"rag_evaluation_dataset\"),\n",
        "        (agent_dataset, \"agent_evaluation_dataset\"),\n",
        "        (golden_dataset, \"golden_templates_dataset\")\n",
        "    ]\n",
        "    \n",
        "    for dataset, dataset_id in datasets_to_register:\n",
        "        try:\n",
        "            vm.init_dataset(\n",
        "                dataset=dataset.df,\n",
        "                input_id=dataset_id,\n",
        "                text_column=\"input\",\n",
        "                target_column=\"expected_output\"\n",
        "            )\n",
        "            print(f\"Registered: {dataset_id}\")\n",
        "        except Exception as e:\n",
        "            print(f\"WARNING: Failed to register {dataset_id}: {e}\")\n",
        "    \n",
        "    # Note: ValidMind datasets are now registered and can be used in test suites\n",
        "    print(\"\\nValidMind Integration Complete:\")\n",
        "    print(\"  - Datasets registered successfully\")\n",
        "    print(\"  - Ready for use in ValidMind test suites\")\n",
        "    print(\"  - Can be referenced by their input_id in test configurations\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"ERROR: ValidMind integration failed: {e}\")\n",
        "    print(\"Note: Some ValidMind features may require additional setup\")\n",
        "\n",
        "# Demonstrate dataset compatibility\n",
        "print(f\"\\nDataset Compatibility Check:\")\n",
        "print(f\"All datasets inherit from VMDataset: SUCCESS\")\n",
        "\n",
        "for dataset, name in [(simple_dataset, \"Simple Q&A\"), (rag_dataset, \"RAG\"), (agent_dataset, \"Agent\"), (golden_dataset, \"Golden\")]:\n",
        "    print(f\"\\n{name} Dataset:\")\n",
        "    print(f\"  - Type: {type(dataset).__name__}\")\n",
        "    print(f\"  - Inherits VMDataset: {hasattr(dataset, 'df')}\")\n",
        "    print(f\"  - Has text_column: {hasattr(dataset, 'text_column')}\")\n",
        "    print(f\"  - Has target_column: {hasattr(dataset, 'target_column')}\")\n",
        "    print(f\"  - DataFrame shape: {dataset.df.shape}\")\n",
        "    print(f\"  - Columns: {len(dataset.columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc9_\"></a>\n",
        "\n",
        "## Custom Metrics with G-Eval\n",
        "\n",
        "One of DeepEval's most powerful features is the ability to create custom evaluation metrics using G-Eval (Generative Evaluation). This enables domain-specific evaluation criteria tailored to your use case.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a test dataset for evaluating the custom metrics\n",
        "test_cases = [\n",
        "    LLMTestCase(\n",
        "        input=\"What is machine learning?\",\n",
        "        actual_output=\"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It uses statistical techniques to allow computers to find patterns in data.\",\n",
        "        context=[\"Machine learning is a branch of AI that focuses on building applications that learn from data and improve their accuracy over time without being programmed to do so.\"],\n",
        "        expected_output=\"Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.\"\n",
        "    ),  \n",
        "    LLMTestCase(\n",
        "        input=\"How do I implement a neural network?\",\n",
        "        actual_output=\"To implement a neural network, you need to: 1) Define the network architecture (layers, neurons), 2) Initialize weights and biases, 3) Implement forward propagation, 4) Calculate loss, 5) Perform backpropagation, and 6) Update weights using gradient descent.\",\n",
        "        context=[\"Neural networks are computing systems inspired by biological neural networks. They consist of layers of interconnected nodes that process and transmit signals.\"],\n",
        "        expected_output=\"Neural network implementation involves defining network architecture, initializing parameters, implementing forward and backward propagation, and using optimization algorithms for training.\"\n",
        "    )\n",
        "]\n",
        "\n",
        "# Create Agent dataset\n",
        "geval_dataset = LLMAgentDataset.from_test_cases(\n",
        "    test_cases=test_cases,\n",
        "    input_id=\"geval_dataset\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Technical accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name=\"Technical Accuracy\"\n",
        "criteria=\"\"\"Evaluate whether the response is technically accurate and uses appropriate \n",
        "terminology for the domain. Consider if the explanations are scientifically sound \n",
        "and if technical concepts are explained correctly.\"\"\"\n",
        "threshold=0.8\n",
        "geval_dataset.assign_scores(\n",
        "    metrics = \"validmind.scorer.llm.deepeval.GEval\",\n",
        "    metric_name=name, \n",
        "    criteria = criteria,\n",
        "    threshold=threshold\n",
        ")\n",
        "geval_dataset._df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clarity and Comprehensiveness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name=\"Clarity and Comprehensiveness\"\n",
        "criteria=\"\"\"Assess whether the response is clear, well-structured, and comprehensive. \n",
        "The response should be easy to understand, logically organized, and address all \n",
        "aspects of the user's question without being overly verbose.\"\"\"\n",
        "threshold=0.75\n",
        "\n",
        "geval_dataset.assign_scores(\n",
        "    metrics = \"validmind.scorer.llm.deepeval.GEval\",\n",
        "    metric_name=name, \n",
        "    criteria = criteria,\n",
        "    threshold=threshold\n",
        ")\n",
        "geval_dataset._df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Business Context Appropriateness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name=\"Business Context Appropriateness\"\n",
        "criteria=\"\"\"Evaluate whether the response is appropriate for a business context. \n",
        "Consider if the tone is professional, if the content is relevant to business needs, \n",
        "and if it provides actionable information that would be valuable to a business user.\"\"\"\n",
        "threshold=0.7\n",
        "geval_dataset.assign_scores(\n",
        "    metrics = \"validmind.scorer.llm.deepeval.GEval\",\n",
        "    metric_name=name, \n",
        "    criteria = criteria,\n",
        "    threshold=threshold\n",
        ")\n",
        "geval_dataset._df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tool Usage Appropriateness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name=\"Tool Usage Appropriateness\"\n",
        "criteria=\"\"\"Evaluate whether the agent used appropriate tools for the given task. \n",
        "Consider if the tools were necessary, if they were used correctly, and if the \n",
        "agent's reasoning for tool selection was sound.\"\"\"\n",
        "threshold=0.8\n",
        "geval_dataset.assign_scores(\n",
        "    metrics = \"validmind.scorer.llm.deepeval.GEval\",\n",
        "    metric_name=name, \n",
        "    criteria = criteria,\n",
        "    threshold=threshold\n",
        ")\n",
        "geval_dataset._df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "criteria = \"\"\"Coherence (1-5) - the collective quality of all sentences. We align this dimension with\n",
        "the DUC quality question of structure and coherence whereby the summary should be\n",
        "well-structured and well-organized. The summary should not just be a heap of related information, but should build from sentence to sentence to a coherent body of information about a topic.\"\"\"\n",
        "\n",
        "evaluation_steps=[\n",
        "        \"Read the news article carefully and identify the main topic and key points.\",\n",
        "        \"Read the summary and compare it to the news article. Check if the summary covers the main topic and key points of the news article, and if it presents them in a clear and logical order.\",\n",
        "        \"Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the highest based on the Evaluation Criteria.\"\n",
        "    ]\n",
        "\n",
        "rubrics = [\n",
        "      {\n",
        "          \"score\":0, \n",
        "          \"criteria\":\"Measure the fluency of the actual output.\",\n",
        "          \"expected_outcome\": \"The output should be fluent and natural sounding\"\n",
        "      },\n",
        "      {\n",
        "          \"score\":2, \n",
        "          \"criteria\":\"Measure the logical flow of the actual output.\",\n",
        "          \"expected_outcome\": \"The output should flow logically from one point to the next\"\n",
        "      },\n",
        "      {\n",
        "          \"score\":3, \n",
        "          \"criteria\":\"Measure the linguistic flow of the actual output.\",\n",
        "          \"expected_outcome\": \"The output should have good linguistic structure and readability\"\n",
        "      }\n",
        "]\n",
        "\n",
        "geval_dataset.assign_scores(\n",
        "    metrics = \"validmind.scorer.llm.deepeval.GEval\",\n",
        "    metric_name=\"Coherence\", \n",
        "    criteria = criteria,\n",
        "    input_column=\"context\",\n",
        ")\n",
        "geval_dataset._df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc10_\"></a>\n",
        "\n",
        "## In summary\n",
        "\n",
        "This notebook demonstrated the comprehensive integration between DeepEval and ValidMind for LLM evaluation:\n",
        "\n",
        "**Key Achievements:**\n",
        "- Successfully created and evaluated different types of LLM test cases (Q&A, RAG, Agents)\n",
        "- Integrated DeepEval metrics with ValidMind's testing infrastructure\n",
        "- Demonstrated Golden template workflows for systematic testing\n",
        "- Created custom evaluation metrics using G-Eval\n",
        "- Showed how to handle complex agent scenarios with tool usage\n",
        "\n",
        "**Integration Benefits:**\n",
        "- **Comprehensive Coverage**: Evaluate LLMs across 30+ specialized metrics\n",
        "- **Structured Documentation**: Leverage ValidMind's compliance and documentation features\n",
        "- **Flexibility**: Support for custom metrics and domain-specific evaluation criteria\n",
        "- **Production Ready**: Handle real-world LLM evaluation scenarios at scale\n",
        "\n",
        "The `LLMAgentDataset` class provides a seamless bridge between DeepEval's evaluation capabilities and ValidMind's testing infrastructure, enabling robust LLM evaluation within a structured, compliant framework.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc11_\"></a>\n",
        "\n",
        "## Next steps\n",
        "\n",
        "**Explore Advanced Features:**\n",
        "- **Continuous Evaluation**: Set up automated LLM evaluation pipelines\n",
        "- **A/B Testing**: Compare different LLM models and configurations\n",
        "- **Metrics Customization**: Create domain-specific evaluation criteria\n",
        "- **Integration Patterns**: Embed evaluation into your LLM development workflow\n",
        "\n",
        "**Additional Resources:**\n",
        "- [ValidMind Library Documentation](https://docs.validmind.ai/developer/validmind-library.html) - Complete API reference and tutorials\n",
        "\n",
        "**Try These Examples:**\n",
        "- Implement custom business-specific evaluation metrics\n",
        "- Create automated evaluation pipelines for model deployment\n",
        "- Integrate with your existing ML infrastructure and workflows\n",
        "- Explore multi-modal evaluation scenarios (text, code, images)\n",
        "\n",
        "Start building comprehensive LLM evaluation workflows that combine the power of DeepEval's specialized metrics with ValidMind's structured testing and documentation framework.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ValidMind Library",
      "language": "python",
      "name": "validmind"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

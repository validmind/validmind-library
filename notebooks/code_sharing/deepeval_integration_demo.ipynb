{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# DeepEval Integration with ValidMind\n",
        "\n",
        "Learn how to integrate [DeepEval](https://github.com/confident-ai/deepeval) with the ValidMind Library to evaluate Large Language Models (LLMs) and AI agents. This notebook demonstrates the complete integration through the new `LLMAgentDataset` class, enabling you to leverage DeepEval's 30+ evaluation metrics within ValidMind's testing infrastructure.\n",
        "\n",
        "To integrate DeepEval with ValidMind, we'll:\n",
        "\n",
        "1. Set up both frameworks and install required dependencies\n",
        "2. Create and evaluate LLM test cases for different scenarios\n",
        "3. Work with RAG systems and agent evaluations\n",
        "4. Use Golden templates for standardized testing\n",
        "5. Create custom evaluation metrics with G-Eval\n",
        "6. Integrate everything with ValidMind's testing framework\n",
        "7. Apply production-ready evaluation patterns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Contents    \n",
        "- [Introduction](#toc1_)    \n",
        "- [About DeepEval Integration](#toc2_)    \n",
        "  - [Before you begin](#toc2_1_)    \n",
        "  - [Key concepts](#toc2_2_)    \n",
        "- [Setting up](#toc3_)    \n",
        "  - [Install required packages](#toc3_1_)    \n",
        "  - [Initialize ValidMind](#toc3_2_)    \n",
        "- [Basic Usage - Simple Q&A Evaluation](#toc4_)    \n",
        "- [RAG System Evaluation](#toc5_)    \n",
        "- [LLM Agent Evaluation](#toc6_)    \n",
        "- [Working with Golden Templates](#toc7_)    \n",
        "- [ValidMind Integration](#toc8_)    \n",
        "- [Custom Metrics with G-Eval](#toc9_)    \n",
        "- [In summary](#toc10_)    \n",
        "- [Next steps](#toc11_)    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc1_\"></a>\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Large Language Model (LLM) evaluation is critical for understanding model performance across different tasks and scenarios. This notebook demonstrates how to integrate DeepEval's comprehensive evaluation framework with ValidMind's testing infrastructure to create a robust LLM evaluation pipeline.\n",
        "\n",
        "DeepEval provides over 30 evaluation metrics specifically designed for LLMs, covering scenarios from simple Q&A to complex agent interactions. By integrating with ValidMind, you can leverage these metrics within a structured testing framework that supports documentation, collaboration, and compliance requirements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc2_\"></a>\n",
        "\n",
        "## About DeepEval Integration\n",
        "\n",
        "DeepEval is a comprehensive evaluation framework for LLMs that provides metrics for various scenarios including hallucination detection, answer relevancy, faithfulness, and custom evaluation criteria. ValidMind is a platform for managing model risk and documentation through automated testing.\n",
        "\n",
        "Together, these tools enable comprehensive LLM evaluation within a structured, compliant framework.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc2_1_\"></a>\n",
        "\n",
        "### Before you begin\n",
        "\n",
        "This notebook assumes you have basic familiarity with Python and Large Language Models. You'll need:\n",
        "\n",
        "- Python 3.8 or higher\n",
        "- Access to OpenAI API (for DeepEval metrics evaluation)\n",
        "- ValidMind account and model registration\n",
        "\n",
        "If you encounter errors due to missing modules, install them with `pip install` and re-run the notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc2_2_\"></a>\n",
        "\n",
        "### Key concepts\n",
        "\n",
        "**LLMTestCase**: A DeepEval object that represents a single test case with input, expected output, actual output, and optional context.\n",
        "\n",
        "**Golden Templates**: Pre-defined test templates with inputs and expected outputs that can be converted to test cases by generating actual outputs.\n",
        "\n",
        "**G-Eval**: Generative evaluation using LLMs to assess response quality based on custom criteria.\n",
        "\n",
        "**LLMAgentDataset**: A ValidMind dataset class that bridges DeepEval test cases with ValidMind's testing infrastructure.\n",
        "\n",
        "**RAG Evaluation**: Testing retrieval-augmented generation systems that combine document retrieval with generation.\n",
        "\n",
        "**Agent Evaluation**: Testing LLM agents that can use tools and perform multi-step reasoning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc3_\"></a>\n",
        "\n",
        "## Setting up\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc3_1_\"></a>\n",
        "\n",
        "### Install required packages\n",
        "\n",
        "First, let's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -q validmind"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc3_2_\"></a>\n",
        "\n",
        "### Initialize ValidMind\n",
        "\n",
        "ValidMind generates a unique _code snippet_ for each registered model to connect with your developer environment. You initialize the ValidMind Library with this code snippet, which ensures that your documentation and tests are uploaded to the correct model when you run the notebook.\n",
        "\n",
        "<div class=\"alert alert-block alert-info\" style=\"background-color: #B5B5B510; color: black; border: 1px solid #083E44; border-left-width: 5px; box-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);border-radius: 5px;\"><span style=\"color: #083E44;\"><b>For access to all features available in this notebook, you'll need access to a ValidMind account.</b></span>\n",
        "<br></br>\n",
        "<a href=\"https://docs.validmind.ai/guide/configuration/register-with-validmind.html\" style=\"color: #DE257E;\"><b>Register with ValidMind</b></a></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load your model identifier credentials from an `.env` file\n",
        "%load_ext dotenv\n",
        "%dotenv .env\n",
        "\n",
        "# Or replace with your code snippet\n",
        "import validmind as vm\n",
        "\n",
        "vm.init(\n",
        "    api_host=\"...\",\n",
        "    api_key=\"...\",\n",
        "    api_secret=\"...\",\n",
        "    model=\"...\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from deepeval.test_case import LLMTestCase, ToolCall, LLMTestCaseParams\n",
        "from deepeval.dataset import Golden\n",
        "from deepeval.metrics import GEval\n",
        "from validmind.datasets.llm import LLMAgentDataset\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc4_\"></a>\n",
        "\n",
        "## Basic Usage - Simple Q&A Evaluation\n",
        "\n",
        "Let's start with the simplest use case: evaluating a basic question-and-answer interaction with an LLM. This demonstrates how to create LLMTestCase objects and integrate them with ValidMind's dataset infrastructure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a simple LLM test case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "simple_test_cases = [\n",
        "LLMTestCase(\n",
        "    input=\"What is machine learning?\",\n",
        "    actual_output=\"\"\"Machine learning is a subset of artificial intelligence (AI) that enables \n",
        "    computers to learn and make decisions from data without being explicitly programmed for every task. \n",
        "    It uses algorithms to find patterns in data and make predictions or decisions based on those patterns.\"\"\",\n",
        "    expected_output=\"\"\"Machine learning is a method of data analysis that automates analytical \n",
        "    model building. It uses algorithms that iteratively learn from data, allowing computers to find \n",
        "    hidden insights without being explicitly programmed where to look.\"\"\",\n",
        "    context=[\"Machine learning is a branch of AI that focuses on algorithms that can learn from data.\"],\n",
        "    retrieval_context=[\"Machine learning is a branch of AI that focuses on algorithms that can learn from data.\"]\n",
        "),\n",
        "LLMTestCase(\n",
        "    input=\"What is deep learning?\",\n",
        "    actual_output=\"\"\"Bananas are yellow fruits that grow on trees in tropical climates. \n",
        "    They are rich in potassium and make a great healthy snack. You can also use them \n",
        "    in smoothies and baking.\"\"\",\n",
        "    expected_output=\"\"\"Deep learning is an advanced machine learning technique that uses neural networks\n",
        "    with many layers to automatically learn representations of data with multiple levels of abstraction.\n",
        "    It has enabled major breakthroughs in AI applications.\"\"\",\n",
        "    context=[\"Deep learning is a specialized machine learning approach that uses deep neural networks to learn from data.\"],\n",
        "    retrieval_context=[\"Deep learning is a specialized machine learning approach that uses deep neural networks to learn from data.\"]\n",
        ")]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create LLMAgentDataset from the test case\n",
        "Let's create ValidMind dataset from Deepeval's test cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nCreating ValidMind dataset...\")\n",
        "\n",
        "simple_dataset = LLMAgentDataset.from_test_cases(\n",
        "    test_cases=simple_test_cases,\n",
        "    input_id=\"simple_qa_dataset\"\n",
        ")\n",
        "\n",
        "# Display the dataset\n",
        "print(\"\\nDataset preview:\")\n",
        "display(simple_dataset.df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def agent_fn(input):\n",
        "#     \"\"\"\n",
        "#     Invoke the simplified agent with the given input.\n",
        "#     \"\"\"\n",
        "    \n",
        "#     return 1.23\n",
        "\n",
        "    \n",
        "# vm_model = vm.init_model(\n",
        "#     predict_fn=agent_fn,\n",
        "#     input_id=\"test_model\",\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compute metrics using ValidMind scorer interface\n",
        "Now we'll compute metrics on our dataset using ValidMind's scorer interface. This will help us evaluate how well our model is performing by calculating various metrics like answer relevancy. The scorer interface provides a standardized way to assess model outputs against expected results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "simple_dataset.assign_scores(metrics = \"validmind.scorer.llm.deepeval.AnswerRelevancy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "simple_dataset._df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc5_\"></a>\n",
        "\n",
        "## RAG System Evaluation\n",
        "\n",
        "Now let's evaluate a more complex use case: a Retrieval-Augmented Generation (RAG) system that retrieves relevant documents and generates responses based on them. RAG systems combine document retrieval with text generation, requiring specialized evaluation approaches.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create multiple RAG test cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "print(\"Creating RAG evaluation test cases...\")\n",
        "rag_test_cases = [\n",
        "    LLMTestCase(\n",
        "        input=\"How do I return a product that doesn't fit?\",\n",
        "        actual_output=\"\"\"You can return any product within 30 days of purchase for a full refund. \n",
        "        Simply visit our returns page on the website and follow the step-by-step instructions. \n",
        "        You'll need your order number and email address. No questions asked!\"\"\",\n",
        "        expected_output=\"We offer a 30-day return policy for full refunds. Visit our returns page to start the process.\",\n",
        "        context=[\"Company policy allows 30-day returns for full refund with no restocking fees.\"],\n",
        "        retrieval_context=[\n",
        "            \"Return Policy: All items can be returned within 30 days of purchase for a full refund.\",\n",
        "            \"Return Process: Visit our website's returns page and enter your order details.\",\n",
        "            \"Customer Service: Available 24/7 to help with returns and refunds.\",\n",
        "            \"No restocking fees apply to returns within the 30-day window.\"\n",
        "        ]\n",
        "    ),\n",
        "    LLMTestCase(\n",
        "        input=\"What are your shipping options and costs?\",\n",
        "        actual_output=\"\"\"We offer three shipping options: Standard (5-7 days, $5.99), \n",
        "        Express (2-3 days, $9.99), and Overnight (next day, $19.99). \n",
        "        Free shipping is available on orders over $50 with Standard delivery.\"\"\",\n",
        "        expected_output=\"Multiple shipping options available with costs ranging from $5.99 to $19.99. Free shipping on orders over $50.\",\n",
        "        context=[\"Shipping information includes various speed and cost options.\"],\n",
        "        retrieval_context=[\n",
        "            \"Standard Shipping: 5-7 business days, $5.99\",\n",
        "            \"Express Shipping: 2-3 business days, $9.99\", \n",
        "            \"Overnight Shipping: Next business day, $19.99\",\n",
        "            \"Free Standard Shipping on orders over $50\"\n",
        "        ]\n",
        "    ),\n",
        "    LLMTestCase(\n",
        "        input=\"Do you have a warranty on electronics?\",\n",
        "        actual_output=\"\"\"Yes, all electronics come with a manufacturer's warranty. \n",
        "        Most items have a 1-year warranty, while premium products may have 2-3 years. \n",
        "        We also offer extended warranty options for purchase.\"\"\",\n",
        "        expected_output=\"Electronics include manufacturer warranty, typically 1-year, with extended options available.\",\n",
        "        context=[\"Electronics warranty information varies by product type and manufacturer.\"],\n",
        "        retrieval_context=[\n",
        "            \"Electronics Warranty: Manufacturer warranty included with all electronic items\",\n",
        "            \"Standard Coverage: 1 year for most electronics\",\n",
        "            \"Premium Products: May include 2-3 year coverage\",\n",
        "            \"Extended Warranty: Available for purchase at checkout\"\n",
        "        ]\n",
        "    )\n",
        "]\n",
        "\n",
        "print(f\"Created {len(rag_test_cases)} RAG test cases\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create RAG LLMTestCase dataset to ValidMind dataset\n",
        "\n",
        "In this section, we'll convert our Deepeval LLMTestCase objects into a ValidMind dataset format.\n",
        "This allows us to leverage ValidMind's powerful evaluation capabilities while maintaining \n",
        "compatibility with Deepeval's test case structure.\n",
        "\n",
        "The dataset will contain:\n",
        "- Input queries\n",
        "- Actual model outputs \n",
        "- Expected outputs\n",
        "- Context information\n",
        "- Retrieved context passages\n",
        "\n",
        "This structured format enables detailed analysis of the RAG system's performance\n",
        "across multiple evaluation dimensions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rag_dataset = LLMAgentDataset.from_test_cases(\n",
        "    test_cases=rag_test_cases,\n",
        "    input_id=\"rag_evaluation_dataset\"\n",
        ")\n",
        "\n",
        "print(f\"RAG Dataset: {rag_dataset}\")\n",
        "print(f\"Shape: {rag_dataset.df.shape}\")\n",
        "\n",
        "# Show dataset structure\n",
        "print(\"\\nRAG Dataset Preview:\")\n",
        "display(rag_dataset.df[['input', 'actual_output', 'context', 'retrieval_context']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rag_dataset.assign_scores(metrics = \"validmind.scorer.llm.deepeval.ContextualRelevancy\")\n",
        "# Display the dataset\n",
        "print(\"\\nDataset preview:\")\n",
        "display(rag_dataset.df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc6_\"></a>\n",
        "\n",
        "## LLM Agent Evaluation\n",
        "\n",
        "Let's evaluate LLM agents that can use tools to accomplish tasks. This is one of the most advanced evaluation scenarios, requiring assessment of both response quality and tool usage appropriateness.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create LLM Agent test cases with tool usage\n",
        "print(\"Creating Agent evaluation test cases...\")\n",
        "\n",
        "agent_test_cases = [\n",
        "    LLMTestCase(\n",
        "        input=\"What's the weather like in New York City today?\",\n",
        "        actual_output=\"\"\"Based on current weather data, New York City is experiencing partly cloudy skies \n",
        "        with a temperature of 72°F (22°C). The humidity is at 60% and there's a light breeze from the west at 8 mph. \n",
        "        No precipitation is expected today.\"\"\",\n",
        "        expected_output=\"Current weather in New York shows mild temperatures with partly cloudy conditions.\",\n",
        "        tools_called=[\n",
        "            ToolCall(\n",
        "                name=\"WeatherAPI\",\n",
        "                description=\"Fetches current weather information for a specified location\",\n",
        "                input_parameters={\"city\": \"New York City\", \"units\": \"fahrenheit\", \"include_forecast\": False},\n",
        "                output={\n",
        "                    \"temperature\": 72,\n",
        "                    \"condition\": \"partly_cloudy\", \n",
        "                    \"humidity\": 60,\n",
        "                    \"wind_speed\": 8,\n",
        "                    \"wind_direction\": \"west\"\n",
        "                },\n",
        "                reasoning=\"User asked for current weather in NYC, so I need to call the weather API\"\n",
        "            )\n",
        "        ],\n",
        "        expected_tools=[\n",
        "            ToolCall(\n",
        "                name=\"WeatherAPI\",\n",
        "                description=\"Should fetch weather information for New York City\",\n",
        "                input_parameters={\"city\": \"New York City\"}\n",
        "            )\n",
        "        ]\n",
        "    ),\n",
        "    LLMTestCase(\n",
        "        input=\"Calculate the compound interest on $1000 invested at 5% annually for 3 years\",\n",
        "        actual_output=\"\"\"Using the compound interest formula A = P(1 + r)^t, where P = $1000, r = 0.05, and t = 3:\n",
        "        A = 1000(1 + 0.05)^3 = 1000(1.05)^3 = 1000 × 1.157625 = $1,157.63\n",
        "        The investment will grow to $1,157.63, earning $157.63 in compound interest.\"\"\",\n",
        "        expected_output=\"The compound interest calculation shows the investment grows to approximately $1,157.63.\",\n",
        "        tools_called=[\n",
        "            ToolCall(\n",
        "                name=\"Calculator\",\n",
        "                description=\"Performs mathematical calculations\",\n",
        "                input_parameters={\"expression\": \"1000 * (1.05 ** 3)\", \"precision\": 2},\n",
        "                output=1157.63,\n",
        "                reasoning=\"Need to calculate compound interest using the standard formula\"\n",
        "            )\n",
        "        ],\n",
        "                 expected_tools=[\n",
        "             ToolCall(\n",
        "                 name=\"Calculator\", \n",
        "                 description=\"Should perform compound interest calculation\",\n",
        "                 input_parameters={\"calculation_type\": \"compound_interest\"}\n",
        "             )\n",
        "         ]\n",
        "    ),\n",
        "    LLMTestCase(\n",
        "        input=\"Send an email to john@example.com about our meeting tomorrow at 2 PM\",\n",
        "        actual_output=\"\"\"I've sent an email to john@example.com with the subject \"Meeting Reminder - Tomorrow at 2 PM\". \n",
        "        The email confirms your meeting scheduled for tomorrow at 2:00 PM and asks John to confirm his attendance. \n",
        "        The email was successfully delivered to the recipient.\"\"\",\n",
        "        expected_output=\"Email sent successfully to john@example.com about the 2 PM meeting tomorrow.\",\n",
        "        tools_called=[\n",
        "            ToolCall(\n",
        "                name=\"EmailSender\",\n",
        "                description=\"Sends emails to specified recipients\",\n",
        "                input_parameters={\n",
        "                    \"to\": \"john@example.com\",\n",
        "                    \"subject\": \"Meeting Reminder - Tomorrow at 2 PM\", \n",
        "                    \"body\": \"Hi John,\\n\\nThis is a reminder about our meeting scheduled for tomorrow at 2:00 PM. Please confirm your attendance.\\n\\nBest regards\"\n",
        "                },\n",
        "                output={\"status\": \"sent\", \"message_id\": \"msg_12345\", \"timestamp\": \"2024-01-15T10:30:00Z\"},\n",
        "                reasoning=\"User requested to send email, so I need to use the email tool with appropriate content\"\n",
        "            )\n",
        "        ],\n",
        "                 expected_tools=[\n",
        "             ToolCall(\n",
        "                 name=\"EmailSender\",\n",
        "                 description=\"Should send an email about the meeting\",\n",
        "                 input_parameters={\"recipient\": \"john@example.com\"}\n",
        "             )\n",
        "         ]\n",
        "    )\n",
        "]\n",
        "\n",
        "print(f\"Created {len(agent_test_cases)} Agent test cases\")\n",
        "\n",
        "# Create Agent dataset\n",
        "agent_dataset = LLMAgentDataset.from_test_cases(\n",
        "    test_cases=agent_test_cases,\n",
        "    input_id=\"agent_evaluation_dataset\"\n",
        ")\n",
        "\n",
        "print(f\"Agent Dataset: {agent_dataset}\")\n",
        "print(f\"Shape: {agent_dataset.df.shape}\")\n",
        "\n",
        "# Analyze tool usage\n",
        "tool_usage = {}\n",
        "for case in agent_test_cases:\n",
        "    if case.tools_called:\n",
        "        for tool in case.tools_called:\n",
        "            tool_usage[tool.name] = tool_usage.get(tool.name, 0) + 1\n",
        "\n",
        "print(\"\\nTool Usage Analysis:\")\n",
        "for tool, count in tool_usage.items():\n",
        "    print(f\"  - {tool}: {count} times\")\n",
        "\n",
        "print(\"\\nAgent Dataset Preview:\")\n",
        "display(agent_dataset.df[['input', 'actual_output', 'tools_called']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc7_\"></a>\n",
        "\n",
        "## Working with Golden Templates\n",
        "\n",
        "Golden templates are a powerful feature of DeepEval that allow you to define test inputs and expected outputs, then generate actual outputs at evaluation time. This approach enables systematic testing across multiple scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Golden templates\n",
        "print(\"Creating Golden templates...\")\n",
        "\n",
        "goldens = [\n",
        "    Golden(\n",
        "        input=\"Explain the concept of neural networks in simple terms\",\n",
        "        expected_output=\"Neural networks are computing systems inspired by biological neural networks that constitute animal brains.\",\n",
        "        context=[\"Neural networks are a key component of machine learning and artificial intelligence.\"]\n",
        "    ),\n",
        "    Golden(\n",
        "        input=\"What are the main benefits of cloud computing for businesses?\", \n",
        "        expected_output=\"Cloud computing offers scalability, cost-effectiveness, accessibility, and reduced infrastructure maintenance.\",\n",
        "        context=[\"Cloud computing provides on-demand access to computing resources over the internet.\"]\n",
        "    ),\n",
        "    Golden(\n",
        "        input=\"How does password encryption protect user data?\",\n",
        "        expected_output=\"Password encryption converts passwords into unreadable formats using cryptographic algorithms, protecting against unauthorized access.\",\n",
        "        context=[\"Encryption is a fundamental security technique used to protect sensitive information.\"]\n",
        "    ),\n",
        "    Golden(\n",
        "        input=\"What is the difference between machine learning and deep learning?\",\n",
        "        expected_output=\"Machine learning is a broad field of AI, while deep learning is a subset that uses neural networks with multiple layers.\",\n",
        "        context=[\"Both are important areas of artificial intelligence with different approaches and applications.\"]\n",
        "    )\n",
        "]\n",
        "\n",
        "print(f\"Created {len(goldens)} Golden templates\")\n",
        "\n",
        "# Create dataset from goldens\n",
        "golden_dataset = LLMAgentDataset.from_goldens(\n",
        "    goldens=goldens,\n",
        "    input_id=\"golden_templates_dataset\"\n",
        ")\n",
        "\n",
        "print(f\"Golden Dataset: {golden_dataset}\")\n",
        "print(f\"Shape: {golden_dataset.df.shape}\")\n",
        "\n",
        "print(\"\\nGolden Templates Preview:\")\n",
        "display(golden_dataset.df[['input', 'expected_output', 'context', 'type']].head())\n",
        "\n",
        "# Mock LLM application function for demonstration\n",
        "def mock_llm_application(input_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Simulate an LLM application generating responses.\n",
        "    In production, this would be your actual LLM application.\n",
        "    \"\"\"\n",
        "    \n",
        "    responses = {\n",
        "        \"neural networks\": \"\"\"Neural networks are computational models inspired by the human brain. \n",
        "        They consist of interconnected nodes (neurons) that process information by learning patterns from data. \n",
        "        These networks can recognize complex patterns and make predictions, making them useful for tasks like \n",
        "        image recognition, natural language processing, and decision-making.\"\"\",\n",
        "        \n",
        "        \"cloud computing\": \"\"\"Cloud computing provides businesses with flexible, scalable access to computing resources \n",
        "        over the internet. Key benefits include reduced upfront costs, automatic scaling based on demand, \n",
        "        improved collaboration through shared access, enhanced security through professional data centers, \n",
        "        and reduced need for internal IT maintenance.\"\"\",\n",
        "        \n",
        "        \"password encryption\": \"\"\"Password encryption protects user data by converting passwords into complex, \n",
        "        unreadable strings using mathematical algorithms. When you enter your password, it's immediately encrypted \n",
        "        before storage or transmission. Even if data is intercepted, the encrypted password appears as random characters, \n",
        "        making it virtually impossible for attackers to determine the original password.\"\"\",\n",
        "        \n",
        "        \"machine learning\": \"\"\"Machine learning is a broad approach to artificial intelligence where computers learn \n",
        "        to make predictions or decisions by finding patterns in data. Deep learning is a specialized subset that uses \n",
        "        artificial neural networks with multiple layers (hence 'deep') to process information in ways that mimic \n",
        "        human brain function, enabling more sophisticated pattern recognition and decision-making.\"\"\"\n",
        "    }\n",
        "    \n",
        "    # Simple keyword matching for demonstration\n",
        "    input_lower = input_text.lower()\n",
        "    for keyword, response in responses.items():\n",
        "        if keyword in input_lower:\n",
        "            return response.strip()\n",
        "    \n",
        "    return f\"Thank you for your question about: {input_text}. I'd be happy to provide a comprehensive answer based on current knowledge and best practices.\"\n",
        "\n",
        "print(f\"\\nMock LLM application ready - will generate responses for {len(goldens)} templates\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert goldens to test cases by generating actual outputs\n",
        "print(\"Converting Golden templates to test cases...\")\n",
        "\n",
        "print(\"Before conversion:\")\n",
        "print(f\"  - Test cases: {len(golden_dataset.test_cases)}\")\n",
        "print(f\"  - Goldens: {len(golden_dataset.goldens)}\")\n",
        "\n",
        "# Convert goldens to test cases using our mock LLM\n",
        "golden_dataset.convert_goldens_to_test_cases(mock_llm_application)\n",
        "\n",
        "print(\"\\nAfter conversion:\")\n",
        "print(f\"  - Test cases: {len(golden_dataset.test_cases)}\")\n",
        "print(f\"  - Goldens: {len(golden_dataset.goldens)}\")\n",
        "\n",
        "print(\"\\nConversion completed!\")\n",
        "\n",
        "# Show the updated dataset\n",
        "print(\"\\nUpdated Dataset with Generated Outputs:\")\n",
        "dataset_df = golden_dataset.df\n",
        "# Filter for rows with actual output\n",
        "mask = pd.notna(dataset_df['actual_output']) & (dataset_df['actual_output'] != '')\n",
        "converted_df = dataset_df[mask]\n",
        "\n",
        "if not converted_df.empty:\n",
        "    display(converted_df[['input', 'actual_output', 'expected_output']])\n",
        "    \n",
        "    # Analyze output lengths using pandas string methods\n",
        "    actual_lengths = pd.Series([len(str(x)) for x in converted_df['actual_output']])\n",
        "    expected_lengths = pd.Series([len(str(x)) for x in converted_df['expected_output']])\n",
        "else:\n",
        "    print(\"No converted test cases found\")\n",
        "\n",
        "print(f\"\\nOutput Analysis:\")\n",
        "print(f\"Average actual output length: {actual_lengths.mean():.0f} characters\")\n",
        "print(f\"Average expected output length: {expected_lengths.mean():.0f} characters\")\n",
        "print(f\"Ratio (actual/expected): {(actual_lengths.mean() / expected_lengths.mean()):.2f}x\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc8_\"></a>\n",
        "\n",
        "## ValidMind Integration\n",
        "\n",
        "Now let's demonstrate how to integrate our LLMAgentDataset with ValidMind's testing framework, enabling comprehensive documentation and compliance features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize ValidMind\n",
        "print(\"Integrating with ValidMind framework...\")\n",
        "\n",
        "try:\n",
        "    # Initialize ValidMind\n",
        "    vm.init()\n",
        "    print(\"ValidMind initialized\")\n",
        "    \n",
        "    # Register our datasets with ValidMind\n",
        "    datasets_to_register = [\n",
        "        (simple_dataset, \"simple_qa_dataset\"),\n",
        "        (rag_dataset, \"rag_evaluation_dataset\"),\n",
        "        (agent_dataset, \"agent_evaluation_dataset\"),\n",
        "        (golden_dataset, \"golden_templates_dataset\")\n",
        "    ]\n",
        "    \n",
        "    for dataset, dataset_id in datasets_to_register:\n",
        "        try:\n",
        "            vm.init_dataset(\n",
        "                dataset=dataset.df,\n",
        "                input_id=dataset_id,\n",
        "                text_column=\"input\",\n",
        "                target_column=\"expected_output\"\n",
        "            )\n",
        "            print(f\"Registered: {dataset_id}\")\n",
        "        except Exception as e:\n",
        "            print(f\"WARNING: Failed to register {dataset_id}: {e}\")\n",
        "    \n",
        "    # Note: ValidMind datasets are now registered and can be used in test suites\n",
        "    print(\"\\nValidMind Integration Complete:\")\n",
        "    print(\"  - Datasets registered successfully\")\n",
        "    print(\"  - Ready for use in ValidMind test suites\")\n",
        "    print(\"  - Can be referenced by their input_id in test configurations\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"ERROR: ValidMind integration failed: {e}\")\n",
        "    print(\"Note: Some ValidMind features may require additional setup\")\n",
        "\n",
        "# Demonstrate dataset compatibility\n",
        "print(f\"\\nDataset Compatibility Check:\")\n",
        "print(f\"All datasets inherit from VMDataset: SUCCESS\")\n",
        "\n",
        "for dataset, name in [(simple_dataset, \"Simple Q&A\"), (rag_dataset, \"RAG\"), (agent_dataset, \"Agent\"), (golden_dataset, \"Golden\")]:\n",
        "    print(f\"\\n{name} Dataset:\")\n",
        "    print(f\"  - Type: {type(dataset).__name__}\")\n",
        "    print(f\"  - Inherits VMDataset: {hasattr(dataset, 'df')}\")\n",
        "    print(f\"  - Has text_column: {hasattr(dataset, 'text_column')}\")\n",
        "    print(f\"  - Has target_column: {hasattr(dataset, 'target_column')}\")\n",
        "    print(f\"  - DataFrame shape: {dataset.df.shape}\")\n",
        "    print(f\"  - Columns: {len(dataset.columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc9_\"></a>\n",
        "\n",
        "## Custom Metrics with G-Eval\n",
        "\n",
        "One of DeepEval's most powerful features is the ability to create custom evaluation metrics using G-Eval (Generative Evaluation). This enables domain-specific evaluation criteria tailored to your use case.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create custom evaluation metrics using G-Eval\n",
        "print(\"Creating custom evaluation metrics...\")\n",
        "\n",
        "# Custom metric 1: Technical Accuracy\n",
        "technical_accuracy_metric = GEval(\n",
        "    name=\"Technical Accuracy\",\n",
        "    criteria=\"\"\"Evaluate whether the response is technically accurate and uses appropriate \n",
        "    terminology for the domain. Consider if the explanations are scientifically sound \n",
        "    and if technical concepts are explained correctly.\"\"\",\n",
        "    evaluation_params=[\n",
        "        LLMTestCaseParams.INPUT,\n",
        "        LLMTestCaseParams.ACTUAL_OUTPUT,\n",
        "        LLMTestCaseParams.CONTEXT\n",
        "    ],\n",
        "    threshold=0.8\n",
        ")\n",
        "\n",
        "# Custom metric 2: Clarity and Comprehensiveness  \n",
        "clarity_metric = GEval(\n",
        "    name=\"Clarity and Comprehensiveness\",\n",
        "    criteria=\"\"\"Assess whether the response is clear, well-structured, and comprehensive. \n",
        "    The response should be easy to understand, logically organized, and address all \n",
        "    aspects of the user's question without being overly verbose.\"\"\",\n",
        "    evaluation_params=[\n",
        "        LLMTestCaseParams.INPUT,\n",
        "        LLMTestCaseParams.ACTUAL_OUTPUT\n",
        "    ],\n",
        "    threshold=0.75\n",
        ")\n",
        "\n",
        "# Custom metric 3: Business Context Appropriateness\n",
        "business_context_metric = GEval(\n",
        "    name=\"Business Context Appropriateness\", \n",
        "    criteria=\"\"\"Evaluate whether the response is appropriate for a business context. \n",
        "    Consider if the tone is professional, if the content is relevant to business needs, \n",
        "    and if it provides actionable information that would be valuable to a business user.\"\"\",\n",
        "    evaluation_params=[\n",
        "        LLMTestCaseParams.INPUT,\n",
        "        LLMTestCaseParams.ACTUAL_OUTPUT,\n",
        "        LLMTestCaseParams.EXPECTED_OUTPUT\n",
        "    ],\n",
        "    threshold=0.7\n",
        ")\n",
        "\n",
        "# Custom metric 4: Tool Usage Appropriateness (for agents)\n",
        "tool_usage_metric = GEval(\n",
        "    name=\"Tool Usage Appropriateness\",\n",
        "    criteria=\"\"\"Evaluate whether the agent used appropriate tools for the given task. \n",
        "    Consider if the tools were necessary, if they were used correctly, and if the \n",
        "    agent's reasoning for tool selection was sound.\"\"\",\n",
        "    evaluation_params=[\n",
        "        LLMTestCaseParams.INPUT,\n",
        "        LLMTestCaseParams.ACTUAL_OUTPUT\n",
        "    ],\n",
        "    threshold=0.8\n",
        ")\n",
        "\n",
        "custom_metrics = [\n",
        "    technical_accuracy_metric,\n",
        "    clarity_metric, \n",
        "    business_context_metric,\n",
        "    tool_usage_metric\n",
        "]\n",
        "\n",
        "print(\"Custom metrics created:\")\n",
        "for metric in custom_metrics:\n",
        "    print(f\"  - {metric.name}: threshold {metric.threshold}\")\n",
        "\n",
        "# Demonstrate metric application to different dataset types\n",
        "print(f\"\\nMetric-Dataset Matching:\")\n",
        "metric_dataset_pairs = [\n",
        "    (\"Technical Accuracy\", \"golden_templates_dataset (tech questions)\"),\n",
        "    (\"Clarity and Comprehensiveness\", \"simple_qa_dataset (general Q&A)\"),\n",
        "    (\"Business Context Appropriateness\", \"rag_evaluation_dataset (business support)\"),\n",
        "    (\"Tool Usage Appropriateness\", \"agent_evaluation_dataset (agent actions)\")\n",
        "]\n",
        "\n",
        "for metric_name, dataset_name in metric_dataset_pairs:\n",
        "    print(f\"  - {metric_name} → {dataset_name}\")\n",
        "\n",
        "print(f\"\\nEvaluation Setup (Demo Mode):\")\n",
        "print(\"Note: Actual evaluation requires OpenAI API key\")\n",
        "print(\"These metrics would evaluate:\")\n",
        "print(\"  - Technical accuracy of AI/ML explanations\") \n",
        "print(\"  - Clarity of business support responses\")\n",
        "print(\"  - Appropriateness of agent tool usage\")\n",
        "print(\"  - Overall comprehensiveness across all domains\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from validmind.vm_models import VMDataset\n",
        "# Create a test dataset for evaluating the custom metrics\n",
        "test_cases = [\n",
        "    LLMTestCase(\n",
        "        input=\"What is machine learning?\",\n",
        "        actual_output=\"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It uses statistical techniques to allow computers to find patterns in data.\",\n",
        "        context=[\"Machine learning is a branch of AI that focuses on building applications that learn from data and improve their accuracy over time without being programmed to do so.\"],\n",
        "        expected_output=\"Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.\"\n",
        "    ),  \n",
        "    LLMTestCase(\n",
        "        input=\"How do I implement a neural network?\",\n",
        "        actual_output=\"To implement a neural network, you need to: 1) Define the network architecture (layers, neurons), 2) Initialize weights and biases, 3) Implement forward propagation, 4) Calculate loss, 5) Perform backpropagation, and 6) Update weights using gradient descent.\",\n",
        "        context=[\"Neural networks are computing systems inspired by biological neural networks. They consist of layers of interconnected nodes that process and transmit signals.\"],\n",
        "        expected_output=\"Neural network implementation involves defining network architecture, initializing parameters, implementing forward and backward propagation, and using optimization algorithms for training.\"\n",
        "    )\n",
        "]\n",
        "\n",
        "# Convert to VMDataset format\n",
        "\n",
        "# Create Agent dataset\n",
        "geval_dataset = LLMAgentDataset.from_test_cases(\n",
        "    test_cases=test_cases,\n",
        "    input_id=\"geval_dataset\"\n",
        ")\n",
        "\n",
        "\n",
        "# FIXED VERSION: Apply custom metrics to individual test cases\n",
        "print(\"Applying custom metrics to evaluation dataset (FIXED VERSION):\")\n",
        "for metric in custom_metrics:\n",
        "    print(f\"\\nResults for {metric.name}:\")\n",
        "    for i, test_case in enumerate(test_cases):\n",
        "        try:\n",
        "            result = metric.measure(test_case)\n",
        "            print(f\"Test case {i+1}:\")\n",
        "            print(f\"  Score: {metric.score:.2f}\")\n",
        "            print(f\"  Reason: {metric.reason}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Test case {i+1}: Error - {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "name=\"Technical Accuracy\",\n",
        "criteria=\"\"\"Evaluate whether the response is technically accurate and uses appropriate \n",
        "    terminology for the domain. Consider if the explanations are scientifically sound \n",
        "    and if technical concepts are explained correctly.\n",
        "    \"\"\"\n",
        "threshold=0.8\n",
        "input_column=\"input\",\n",
        "actual_output_column=\"actual_output\",\n",
        "context_column=\"context\",\n",
        "\n",
        "geval_dataset.assign_scores(\n",
        "    metrics = \"validmind.scorer.llm.deepeval.GenericEval\",\n",
        "    input_column=input_column,\n",
        "    actual_output_column=actual_output_column,\n",
        "    context_column=context_column,\n",
        "    metric_name=name,\n",
        "    criteria=criteria,\n",
        "    evaluation_params=[\n",
        "        LLMTestCaseParams.INPUT,\n",
        "        LLMTestCaseParams.ACTUAL_OUTPUT,\n",
        "        LLMTestCaseParams.CONTEXT\n",
        "    ],\n",
        "    threshold=0.8,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc10_\"></a>\n",
        "\n",
        "## In summary\n",
        "\n",
        "This notebook demonstrated the comprehensive integration between DeepEval and ValidMind for LLM evaluation:\n",
        "\n",
        "**Key Achievements:**\n",
        "- Successfully created and evaluated different types of LLM test cases (Q&A, RAG, Agents)\n",
        "- Integrated DeepEval metrics with ValidMind's testing infrastructure\n",
        "- Demonstrated Golden template workflows for systematic testing\n",
        "- Created custom evaluation metrics using G-Eval\n",
        "- Showed how to handle complex agent scenarios with tool usage\n",
        "\n",
        "**Integration Benefits:**\n",
        "- **Comprehensive Coverage**: Evaluate LLMs across 30+ specialized metrics\n",
        "- **Structured Documentation**: Leverage ValidMind's compliance and documentation features\n",
        "- **Flexibility**: Support for custom metrics and domain-specific evaluation criteria\n",
        "- **Production Ready**: Handle real-world LLM evaluation scenarios at scale\n",
        "\n",
        "The `LLMAgentDataset` class provides a seamless bridge between DeepEval's evaluation capabilities and ValidMind's testing infrastructure, enabling robust LLM evaluation within a structured, compliant framework.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "<a id=\"toc11_\"></a>\n",
        "\n",
        "## Next steps\n",
        "\n",
        "**Explore Advanced Features:**\n",
        "- **Continuous Evaluation**: Set up automated LLM evaluation pipelines\n",
        "- **A/B Testing**: Compare different LLM models and configurations\n",
        "- **Metrics Customization**: Create domain-specific evaluation criteria\n",
        "- **Integration Patterns**: Embed evaluation into your LLM development workflow\n",
        "\n",
        "**Additional Resources:**\n",
        "- [ValidMind Library Documentation](https://docs.validmind.ai/developer/validmind-library.html) - Complete API reference and tutorials\n",
        "\n",
        "**Try These Examples:**\n",
        "- Implement custom business-specific evaluation metrics\n",
        "- Create automated evaluation pipelines for model deployment\n",
        "- Integrate with your existing ML infrastructure and workflows\n",
        "- Explore multi-modal evaluation scenarios (text, code, images)\n",
        "\n",
        "Start building comprehensive LLM evaluation workflows that combine the power of DeepEval's specialized metrics with ValidMind's structured testing and documentation framework.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ValidMind Library",
      "language": "python",
      "name": "validmind"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# DeepEval Integration with ValidMind - Comprehensive Demo\n",
        "\n",
        "This notebook demonstrates the complete integration between [DeepEval](https://github.com/confident-ai/deepeval) and [ValidMind](https://github.com/validmind/validmind-library) through the new `LLMAgentDataset` class.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "1. **Setup & Installation** - Getting started with both frameworks\n",
        "2. **Basic Usage** - Creating and evaluating simple LLM test cases\n",
        "3. **RAG Evaluation** - Testing retrieval-augmented generation systems\n",
        "4. **Agent Evaluation** - Evaluating LLM agents with tool usage\n",
        "5. **Golden Templates** - Working with evaluation templates\n",
        "6. **Custom Metrics** - Creating domain-specific evaluation criteria\n",
        "7. **ValidMind Integration** - Leveraging ValidMind's testing infrastructure\n",
        "8. **Production Patterns** - Real-world usage scenarios\n",
        "\n",
        "## Key Benefits\n",
        "\n",
        "- **30+ Evaluation Metrics**: Use all DeepEval metrics within ValidMind\n",
        "- **Multi-Modal Support**: Evaluate Q&A, RAG, and Agent systems\n",
        "- **Production Ready**: Handle real-world LLM evaluation scenarios\n",
        "- **Seamless Integration**: Full compatibility with ValidMind workflows\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Installation & Setup\n",
        "\n",
        "First, let's install the required packages and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (uncomment to run)\n",
        "# !pip install deepeval validmind openai\n",
        "\n",
        "# For this demo, we'll also install some additional packages for better output\n",
        "# !pip install tabulate pandas numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import os\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from deepeval.test_case import LLMTestCase, ToolCall, LLMTestCaseParams\n",
        "from deepeval.dataset import Golden\n",
        "from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric, HallucinationMetric, GEval\n",
        "import validmind as vm\n",
        "from validmind.datasets.llm import LLMAgentDataset\n",
        "\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 1: Basic Usage - Simple Q&A Evaluation\n",
        "\n",
        "Let's start with the simplest use case: evaluating a basic question-and-answer interaction with an LLM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Create a simple LLM test case\n",
        "print(\"Creating a simple Q&A test case...\")\n",
        "\n",
        "simple_test_case = LLMTestCase(\n",
        "    input=\"What is machine learning?\",\n",
        "    actual_output=\"\"\"Machine learning is a subset of artificial intelligence (AI) that enables \n",
        "    computers to learn and make decisions from data without being explicitly programmed for every task. \n",
        "    It uses algorithms to find patterns in data and make predictions or decisions based on those patterns.\"\"\",\n",
        "    expected_output=\"\"\"Machine learning is a method of data analysis that automates analytical \n",
        "    model building. It uses algorithms that iteratively learn from data, allowing computers to find \n",
        "    hidden insights without being explicitly programmed where to look.\"\"\",\n",
        "    context=[\"Machine learning is a branch of AI that focuses on algorithms that can learn from data.\"]\n",
        ")\n",
        "\n",
        "# Step 2: Create LLMAgentDataset from the test case\n",
        "print(\"\\nCreating ValidMind dataset...\")\n",
        "\n",
        "simple_dataset = LLMAgentDataset.from_test_cases(\n",
        "    test_cases=[simple_test_case],\n",
        "    input_id=\"simple_qa_dataset\"\n",
        ")\n",
        "\n",
        "# Display the dataset\n",
        "print(\"\\nDataset preview:\")\n",
        "display(simple_dataset.df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import validmind as vm\n",
        "\n",
        "def agent_fn(input):\n",
        "    \"\"\"\n",
        "    Invoke the simplified agent with the given input.\n",
        "    \"\"\"\n",
        "    \n",
        "    return 1.23\n",
        "\n",
        "    \n",
        "vm_model = vm.init_model(\n",
        "    predict_fn=agent_fn,\n",
        "    input_id=\"test_model\",\n",
        "    __log=False\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "simple_dataset._df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "simple_dataset.assign_scores(vm_model, \"AnswerRelevancy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "simple_dataset._df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from deepeval import evaluate\n",
        "from deepeval.metrics import AnswerRelevancyMetric\n",
        "from deepeval.test_case import LLMTestCase\n",
        "from validmind import tags, tasks\n",
        "from validmind.vm_models.dataset import VMDataset\n",
        "from validmind.errors import SkipTestError\n",
        "from typing import Dict, Any\n",
        "\n",
        "# Create custom ValidMind tests for DeepEval metrics\n",
        "@vm.test(\"llm.AnswerRelevancy\") \n",
        "@tags(\"llm\", \"AnswerRelevancy\", \"deepeval\")\n",
        "@tasks(\"llm\")\n",
        "def AnswerRelevancy(dataset: VMDataset, threshold: float = 0.8) -> Dict[str, Any]:\n",
        "\n",
        "    metric = AnswerRelevancyMetric(\n",
        "        threshold=0.7,\n",
        "        model=\"gpt-4o\",\n",
        "        include_reason=True\n",
        "    )\n",
        "    results = []\n",
        "    for index, test_case in dataset.df.iterrows():\n",
        "        input = test_case[\"input\"]\n",
        "        actual_output = test_case[\"actual_output\"]\n",
        "    \n",
        "        test_case = LLMTestCase(\n",
        "            input=input,\n",
        "            actual_output=actual_output,\n",
        "        )\n",
        "        result = evaluate(test_cases=[test_case], metrics=[metric])\n",
        "        results.append({\n",
        "            \"score\": result.test_results[0].metrics_data[0].score,\n",
        "            \"name\": result.test_results[0].metrics_data[0].name,\n",
        "            \"reason\": result.test_results[0].metrics_data[0].reason\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "    \n",
        "    \n",
        "\n",
        "    # # To run metric as a standalone\n",
        "    # # metric.measure(test_case)\n",
        "    # # print(metric.score, metric.reason)\n",
        "\n",
        "    # result = evaluate(test_cases=[test_case], metrics=[metric])\n",
        "    # # print(result, result.reason)\n",
        "    # print(\"--------------------------------\")\n",
        "    # result.test_results[0].metrics_data[0].score\n",
        "    # result.test_results[0].metrics_data[0].name\n",
        "    # result.test_results[0].metrics_data[0].reason\n",
        "    # print(\"--------------------------------\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run AnswerRelevancy test\n",
        "test_results = vm.tests.run_test(\"llm.AnswerRelevancy\", dataset=simple_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from math import e\n",
        "from validmind import tags, tasks\n",
        "from validmind.datasets.llm import LLMAgentDataset\n",
        "from validmind.vm_models.dataset import VMDataset\n",
        "from validmind.errors import SkipTestError\n",
        "from typing import Dict, Any\n",
        "from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric, HallucinationMetric , ContextualRelevancyMetric\n",
        "\n",
        "# Create custom ValidMind tests for DeepEval metrics\n",
        "@vm.test(\"llm.Faithfulness\") \n",
        "@tags(\"llm\", \"faithfulness\", \"deepeval\")\n",
        "@tasks(\"llm\")\n",
        "def Faithfulness(dataset: VMDataset, threshold: float = 0.8) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Evaluates the faithfulness of LLM responses using DeepEval's FaithfulnessMetric.\n",
        "    \n",
        "    Args:\n",
        "        dataset: VMDataset containing LLM inputs and outputs\n",
        "        threshold: Minimum score threshold (default: 0.8)\n",
        "            \n",
        "    Returns:\n",
        "        Dictionary containing metric results and visualization\n",
        "    \"\"\"\n",
        "    if not isinstance(dataset, LLMAgentDataset):\n",
        "        raise SkipTestError(\"Dataset must be an LLMAgentDataset\")\n",
        "        \n",
        "    results = []\n",
        "    for i, test_case in dataset.df.iterrows():\n",
        "        input = test_case[\"input\"]\n",
        "        actual_output = test_case[\"actual_output\"]\n",
        "        retrieval_context = None if test_case[\"retrieval_context\"] is None else list(test_case[\"retrieval_context\"])\n",
        "        metric = ContextualRelevancyMetric(threshold=0.7, model=\"gpt-4o\")\n",
        "        test_case = LLMTestCase(\n",
        "        input=input,\n",
        "        actual_output=actual_output,\n",
        "        retrieval_context=retrieval_context)\n",
        "        results.append(metric.measure(test_case))\n",
        "    \n",
        "    return results\n",
        "\n",
        "# @vm.test(\"llm.Hallucination\")\n",
        "# @tags(\"llm\", \"hallucination\", \"deepeval\") \n",
        "# @tasks(\"llm\")\n",
        "# def Hallucination(dataset: VMDataset, threshold: float = 0.8) -> Dict[str, Any]:\n",
        "#     \"\"\"\n",
        "#     Evaluates hallucination in LLM responses using DeepEval's HallucinationMetric.\n",
        "    \n",
        "#     Args:\n",
        "#         dataset: VMDataset containing LLM inputs and outputs\n",
        "#         threshold: Minimum score threshold (default: 0.8)\n",
        "            \n",
        "#     Returns:\n",
        "#         Dictionary containing metric results and visualization\n",
        "#     \"\"\"\n",
        "#     if not isinstance(dataset, LLMAgentDataset):\n",
        "#         raise SkipTestError(\"Dataset must be an LLMAgentDataset\")\n",
        "        \n",
        "#     metric = HallucinationMetric(threshold=threshold)\n",
        "#     results = dataset.evaluate_with_deepeval(\n",
        "#         metrics=[metric],\n",
        "#         hyperparameters={\n",
        "#             \"model\": \"gpt-4\", \n",
        "#             \"prompt_template\": \"Evaluate hallucination: {{input}}\"\n",
        "#         }\n",
        "#     )\n",
        "    \n",
        "#     return {\n",
        "#         \"metric_name\": \"Hallucination\",\n",
        "#         \"score\": results[\"hallucination_score\"],\n",
        "#         \"passed\": results[\"hallucination_score\"] >= threshold,\n",
        "#         \"threshold\": threshold\n",
        "#     }\n",
        "\n",
        "# # Create custom ValidMind tests for DeepEval metrics\n",
        "# @vm.test(\"llm.AnswerRelevancy\")\n",
        "# @tags(\"llm\", \"answer_relevancy\", \"deepeval\")\n",
        "# @tasks(\"llm\")\n",
        "# def AnswerRelevancy(dataset: VMDataset, threshold = 0.7) -> Dict[str, Any]:\n",
        "#     \"\"\"\n",
        "#     Evaluates the relevancy of LLM responses using DeepEval's AnswerRelevancyMetric.\n",
        "    \n",
        "#     Args:\n",
        "#         dataset: VMDataset containing LLM inputs and outputs\n",
        "#         params: Dictionary containing metric parameters\n",
        "#             - threshold: Minimum score threshold (default: 0.7)\n",
        "            \n",
        "#     Returns:\n",
        "#         Dictionary containing metric results and visualization\n",
        "#     \"\"\"\n",
        "#     if not isinstance(dataset, LLMAgentDataset):\n",
        "#         raise SkipTestError(\"Dataset must be an LLMAgentDataset\")\n",
        "        \n",
        "#     metric = AnswerRelevancyMetric(threshold=threshold)\n",
        "#     results = dataset.evaluate_with_deepeval(\n",
        "#         metrics=[metric],\n",
        "#         hyperparameters={\n",
        "#             \"model\": \"gpt-4\",\n",
        "#             \"evaluation_type\": \"basic_qa\",\n",
        "#             \"prompt_template\": \"Evaluate answer relevancy: {{input}}\"\n",
        "#         }\n",
        "#     )\n",
        "    \n",
        "#     return {\n",
        "#         \"metric_name\": \"Answer Relevancy\",\n",
        "#         \"score\": results[\"answer_relevancy_score\"],\n",
        "#         \"passed\": results[\"answer_relevancy_score\"] >= threshold,\n",
        "#         \"threshold\": threshold\n",
        "#     }\n",
        "\n",
        "# @vm.test(\"llm.Faithfulness\") \n",
        "# @tags(\"llm\", \"faithfulness\", \"deepeval\")\n",
        "# @tasks(\"llm\")\n",
        "# def Faithfulness(dataset: VMDataset, params: Dict[str, Any] = {\"threshold\": 0.8}) -> Dict[str, Any]:\n",
        "#     \"\"\"\n",
        "#     Evaluates the faithfulness of LLM responses using DeepEval's FaithfulnessMetric.\n",
        "    \n",
        "#     Args:\n",
        "#         dataset: VMDataset containing LLM inputs and outputs\n",
        "#         params: Dictionary containing metric parameters\n",
        "#             - threshold: Minimum score threshold (default: 0.8)\n",
        "            \n",
        "#     Returns:\n",
        "#         Dictionary containing metric results and visualization\n",
        "#     \"\"\"\n",
        "#     if not isinstance(dataset, LLMAgentDataset):\n",
        "#         raise SkipTestError(\"Dataset must be an LLMAgentDataset\")\n",
        "        \n",
        "#     metric = FaithfulnessMetric(threshold=params[\"threshold\"])\n",
        "#     results = dataset.evaluate_with_deepeval(\n",
        "#         metrics=[metric],\n",
        "#         hyperparameters={\n",
        "#             \"model\": \"gpt-4\",\n",
        "#             \"prompt_template\": \"Evaluate faithfulness: {{input}}\"\n",
        "#         }\n",
        "#     )\n",
        "    \n",
        "#     return {\n",
        "#         \"metric_name\": \"Faithfulness\",\n",
        "#         \"score\": results[\"faithfulness_score\"],\n",
        "#         \"passed\": results[\"faithfulness_score\"] >= params[\"threshold\"],\n",
        "#         \"threshold\": params[\"threshold\"]\n",
        "#     }\n",
        "\n",
        "# @vm.test(\"llm.Hallucination\")\n",
        "# @tags(\"llm\", \"hallucination\", \"deepeval\") \n",
        "# @tasks(\"llm\")\n",
        "# def Hallucination(dataset: VMDataset, params: Dict[str, Any] = {\"threshold\": 0.3}) -> Dict[str, Any]:\n",
        "#     \"\"\"\n",
        "#     Evaluates hallucination in LLM responses using DeepEval's HallucinationMetric.\n",
        "    \n",
        "#     Args:\n",
        "#         dataset: VMDataset containing LLM inputs and outputs\n",
        "#         params: Dictionary containing metric parameters\n",
        "#             - threshold: Maximum hallucination score threshold (default: 0.3)\n",
        "            \n",
        "#     Returns:\n",
        "#         Dictionary containing metric results and visualization\n",
        "#     \"\"\"\n",
        "#     if not isinstance(dataset, LLMAgentDataset):\n",
        "#         raise SkipTestError(\"Dataset must be an LLMAgentDataset\")\n",
        "        \n",
        "#     metric = HallucinationMetric(threshold=params[\"threshold\"])\n",
        "#     results = dataset.evaluate_with_deepeval(\n",
        "#         metrics=[metric],\n",
        "#         hyperparameters={\n",
        "#             \"model\": \"gpt-4\",\n",
        "#             \"prompt_template\": \"Evaluate hallucination: {{input}}\"\n",
        "#         }\n",
        "#     )\n",
        "    \n",
        "#     return {\n",
        "#         \"metric_name\": \"Hallucination\",\n",
        "#         \"score\": results[\"hallucination_score\"], \n",
        "#         \"passed\": results[\"hallucination_score\"] <= params[\"threshold\"],\n",
        "#         \"threshold\": params[\"threshold\"]\n",
        "#     }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run the Faithfulness test\n",
        "print(\"Running Faithfulness test...\")\n",
        "faithfulness_result = vm.tests.run_test(\n",
        "    \"llm.Faithfulness\",\n",
        "    inputs={\"dataset\": simple_dataset},\n",
        "    params={\n",
        "        \"threshold\": 0.8,\n",
        "    }\n",
        ")\n",
        "print(f\"Faithfulness test result: {faithfulness_result}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Evaluate with DeepEval metrics\n",
        "print(\"Setting up evaluation metrics...\")\n",
        "\n",
        "# Note: These metrics require an OpenAI API key to work\n",
        "# For demonstration, we'll show the setup even if we can't run them\n",
        "\n",
        "basic_metrics = [\n",
        "    AnswerRelevancyMetric(threshold=0.7),\n",
        "    FaithfulnessMetric(threshold=0.8),\n",
        "    HallucinationMetric(threshold=0.3)  # Lower = less hallucination allowed\n",
        "]\n",
        "\n",
        "print(\"Metrics configured:\")\n",
        "for metric in basic_metrics:\n",
        "    print(f\"  - {metric.__class__.__name__}: threshold {getattr(metric, 'threshold', 'N/A')}\")\n",
        "\n",
        "# Check if we can run evaluation (requires API key)\n",
        "api_key_available = os.getenv(\"OPENAI_API_KEY\") is not None\n",
        "\n",
        "if api_key_available:\n",
        "    print(\"\\nRunning evaluation...\")\n",
        "    try:\n",
        "        results = simple_dataset.evaluate_with_deepeval(\n",
        "            metrics=basic_metrics,\n",
        "            hyperparameters={\n",
        "                \"model\": \"gpt-4\",\n",
        "                \"evaluation_type\": \"basic_qa\",\n",
        "                \"dataset_size\": len(simple_dataset.test_cases)\n",
        "            }\n",
        "        )\n",
        "        print(\"Evaluation completed!\")\n",
        "        print(f\"Results: {results}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Evaluation failed: {e}\")\n",
        "else:\n",
        "    print(\"\\nWARNING: OpenAI API key not found - skipping evaluation\")\n",
        "    print(\"To run evaluation, set: os.environ['OPENAI_API_KEY'] = 'your-key'\")\n",
        "    print(\"For now, we'll demonstrate the evaluation setup\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 2: RAG System Evaluation\n",
        "\n",
        "Now let's evaluate a more complex use case: a Retrieval-Augmented Generation (RAG) system that retrieves relevant documents and generates responses based on them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create multiple RAG test cases\n",
        "print(\"Creating RAG evaluation test cases...\")\n",
        "\n",
        "rag_test_cases = [\n",
        "    LLMTestCase(\n",
        "        input=\"How do I return a product that doesn't fit?\",\n",
        "        actual_output=\"\"\"You can return any product within 30 days of purchase for a full refund. \n",
        "        Simply visit our returns page on the website and follow the step-by-step instructions. \n",
        "        You'll need your order number and email address. No questions asked!\"\"\",\n",
        "        expected_output=\"We offer a 30-day return policy for full refunds. Visit our returns page to start the process.\",\n",
        "        context=[\"Company policy allows 30-day returns for full refund with no restocking fees.\"],\n",
        "        retrieval_context=[\n",
        "            \"Return Policy: All items can be returned within 30 days of purchase for a full refund.\",\n",
        "            \"Return Process: Visit our website's returns page and enter your order details.\",\n",
        "            \"Customer Service: Available 24/7 to help with returns and refunds.\",\n",
        "            \"No restocking fees apply to returns within the 30-day window.\"\n",
        "        ]\n",
        "    ),\n",
        "    LLMTestCase(\n",
        "        input=\"What are your shipping options and costs?\",\n",
        "        actual_output=\"\"\"We offer three shipping options: Standard (5-7 days, $5.99), \n",
        "        Express (2-3 days, $9.99), and Overnight (next day, $19.99). \n",
        "        Free shipping is available on orders over $50 with Standard delivery.\"\"\",\n",
        "        expected_output=\"Multiple shipping options available with costs ranging from $5.99 to $19.99. Free shipping on orders over $50.\",\n",
        "        context=[\"Shipping information includes various speed and cost options.\"],\n",
        "        retrieval_context=[\n",
        "            \"Standard Shipping: 5-7 business days, $5.99\",\n",
        "            \"Express Shipping: 2-3 business days, $9.99\", \n",
        "            \"Overnight Shipping: Next business day, $19.99\",\n",
        "            \"Free Standard Shipping on orders over $50\"\n",
        "        ]\n",
        "    ),\n",
        "    LLMTestCase(\n",
        "        input=\"Do you have a warranty on electronics?\",\n",
        "        actual_output=\"\"\"Yes, all electronics come with a manufacturer's warranty. \n",
        "        Most items have a 1-year warranty, while premium products may have 2-3 years. \n",
        "        We also offer extended warranty options for purchase.\"\"\",\n",
        "        expected_output=\"Electronics include manufacturer warranty, typically 1-year, with extended options available.\",\n",
        "        context=[\"Electronics warranty information varies by product type and manufacturer.\"],\n",
        "        retrieval_context=[\n",
        "            \"Electronics Warranty: Manufacturer warranty included with all electronic items\",\n",
        "            \"Standard Coverage: 1 year for most electronics\",\n",
        "            \"Premium Products: May include 2-3 year coverage\",\n",
        "            \"Extended Warranty: Available for purchase at checkout\"\n",
        "        ]\n",
        "    )\n",
        "]\n",
        "\n",
        "print(f\"Created {len(rag_test_cases)} RAG test cases\")\n",
        "\n",
        "# Create RAG dataset\n",
        "rag_dataset = LLMAgentDataset.from_test_cases(\n",
        "    test_cases=rag_test_cases,\n",
        "    input_id=\"rag_evaluation_dataset\"\n",
        ")\n",
        "\n",
        "print(f\"RAG Dataset: {rag_dataset}\")\n",
        "print(f\"Shape: {rag_dataset.df.shape}\")\n",
        "\n",
        "# Show dataset structure\n",
        "print(\"\\nRAG Dataset Preview:\")\n",
        "display(rag_dataset.df[['input', 'actual_output', 'context', 'retrieval_context']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 3: LLM Agent Evaluation\n",
        "\n",
        "Let's evaluate LLM agents that can use tools to accomplish tasks. This is one of the most advanced evaluation scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create LLM Agent test cases with tool usage\n",
        "print(\"Creating Agent evaluation test cases...\")\n",
        "\n",
        "agent_test_cases = [\n",
        "    LLMTestCase(\n",
        "        input=\"What's the weather like in New York City today?\",\n",
        "        actual_output=\"\"\"Based on current weather data, New York City is experiencing partly cloudy skies \n",
        "        with a temperature of 72°F (22°C). The humidity is at 60% and there's a light breeze from the west at 8 mph. \n",
        "        No precipitation is expected today.\"\"\",\n",
        "        expected_output=\"Current weather in New York shows mild temperatures with partly cloudy conditions.\",\n",
        "        tools_called=[\n",
        "            ToolCall(\n",
        "                name=\"WeatherAPI\",\n",
        "                description=\"Fetches current weather information for a specified location\",\n",
        "                input_parameters={\"city\": \"New York City\", \"units\": \"fahrenheit\", \"include_forecast\": False},\n",
        "                output={\n",
        "                    \"temperature\": 72,\n",
        "                    \"condition\": \"partly_cloudy\", \n",
        "                    \"humidity\": 60,\n",
        "                    \"wind_speed\": 8,\n",
        "                    \"wind_direction\": \"west\"\n",
        "                },\n",
        "                reasoning=\"User asked for current weather in NYC, so I need to call the weather API\"\n",
        "            )\n",
        "        ],\n",
        "        expected_tools=[\n",
        "            ToolCall(\n",
        "                name=\"WeatherAPI\",\n",
        "                description=\"Should fetch weather information for New York City\",\n",
        "                input_parameters={\"city\": \"New York City\"}\n",
        "            )\n",
        "        ]\n",
        "    ),\n",
        "    LLMTestCase(\n",
        "        input=\"Calculate the compound interest on $1000 invested at 5% annually for 3 years\",\n",
        "        actual_output=\"\"\"Using the compound interest formula A = P(1 + r)^t, where P = $1000, r = 0.05, and t = 3:\n",
        "        A = 1000(1 + 0.05)^3 = 1000(1.05)^3 = 1000 × 1.157625 = $1,157.63\n",
        "        The investment will grow to $1,157.63, earning $157.63 in compound interest.\"\"\",\n",
        "        expected_output=\"The compound interest calculation shows the investment grows to approximately $1,157.63.\",\n",
        "        tools_called=[\n",
        "            ToolCall(\n",
        "                name=\"Calculator\",\n",
        "                description=\"Performs mathematical calculations\",\n",
        "                input_parameters={\"expression\": \"1000 * (1.05 ** 3)\", \"precision\": 2},\n",
        "                output=1157.63,\n",
        "                reasoning=\"Need to calculate compound interest using the standard formula\"\n",
        "            )\n",
        "        ],\n",
        "                 expected_tools=[\n",
        "             ToolCall(\n",
        "                 name=\"Calculator\", \n",
        "                 description=\"Should perform compound interest calculation\",\n",
        "                 input_parameters={\"calculation_type\": \"compound_interest\"}\n",
        "             )\n",
        "         ]\n",
        "    ),\n",
        "    LLMTestCase(\n",
        "        input=\"Send an email to john@example.com about our meeting tomorrow at 2 PM\",\n",
        "        actual_output=\"\"\"I've sent an email to john@example.com with the subject \"Meeting Reminder - Tomorrow at 2 PM\". \n",
        "        The email confirms your meeting scheduled for tomorrow at 2:00 PM and asks John to confirm his attendance. \n",
        "        The email was successfully delivered to the recipient.\"\"\",\n",
        "        expected_output=\"Email sent successfully to john@example.com about the 2 PM meeting tomorrow.\",\n",
        "        tools_called=[\n",
        "            ToolCall(\n",
        "                name=\"EmailSender\",\n",
        "                description=\"Sends emails to specified recipients\",\n",
        "                input_parameters={\n",
        "                    \"to\": \"john@example.com\",\n",
        "                    \"subject\": \"Meeting Reminder - Tomorrow at 2 PM\", \n",
        "                    \"body\": \"Hi John,\\n\\nThis is a reminder about our meeting scheduled for tomorrow at 2:00 PM. Please confirm your attendance.\\n\\nBest regards\"\n",
        "                },\n",
        "                output={\"status\": \"sent\", \"message_id\": \"msg_12345\", \"timestamp\": \"2024-01-15T10:30:00Z\"},\n",
        "                reasoning=\"User requested to send email, so I need to use the email tool with appropriate content\"\n",
        "            )\n",
        "        ],\n",
        "                 expected_tools=[\n",
        "             ToolCall(\n",
        "                 name=\"EmailSender\",\n",
        "                 description=\"Should send an email about the meeting\",\n",
        "                 input_parameters={\"recipient\": \"john@example.com\"}\n",
        "             )\n",
        "         ]\n",
        "    )\n",
        "]\n",
        "\n",
        "print(f\"Created {len(agent_test_cases)} Agent test cases\")\n",
        "\n",
        "# Create Agent dataset\n",
        "agent_dataset = LLMAgentDataset.from_test_cases(\n",
        "    test_cases=agent_test_cases,\n",
        "    input_id=\"agent_evaluation_dataset\"\n",
        ")\n",
        "\n",
        "print(f\"Agent Dataset: {agent_dataset}\")\n",
        "print(f\"Shape: {agent_dataset.df.shape}\")\n",
        "\n",
        "# Analyze tool usage\n",
        "tool_usage = {}\n",
        "for case in agent_test_cases:\n",
        "    if case.tools_called:\n",
        "        for tool in case.tools_called:\n",
        "            tool_usage[tool.name] = tool_usage.get(tool.name, 0) + 1\n",
        "\n",
        "print(f\"\\nTool Usage Analysis:\")\n",
        "for tool, count in tool_usage.items():\n",
        "    print(f\"  - {tool}: {count} times\")\n",
        "\n",
        "print(\"\\nAgent Dataset Preview:\")\n",
        "display(agent_dataset.df[['input', 'actual_output', 'tools_called']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 4: Working with Golden Templates\n",
        "\n",
        "Golden templates are a powerful feature of DeepEval that allow you to define test inputs and expected outputs, then generate actual outputs at evaluation time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Golden templates\n",
        "print(\"Creating Golden templates...\")\n",
        "\n",
        "goldens = [\n",
        "    Golden(\n",
        "        input=\"Explain the concept of neural networks in simple terms\",\n",
        "        expected_output=\"Neural networks are computing systems inspired by biological neural networks that constitute animal brains.\",\n",
        "        context=[\"Neural networks are a key component of machine learning and artificial intelligence.\"]\n",
        "    ),\n",
        "    Golden(\n",
        "        input=\"What are the main benefits of cloud computing for businesses?\", \n",
        "        expected_output=\"Cloud computing offers scalability, cost-effectiveness, accessibility, and reduced infrastructure maintenance.\",\n",
        "        context=[\"Cloud computing provides on-demand access to computing resources over the internet.\"]\n",
        "    ),\n",
        "    Golden(\n",
        "        input=\"How does password encryption protect user data?\",\n",
        "        expected_output=\"Password encryption converts passwords into unreadable formats using cryptographic algorithms, protecting against unauthorized access.\",\n",
        "        context=[\"Encryption is a fundamental security technique used to protect sensitive information.\"]\n",
        "    ),\n",
        "    Golden(\n",
        "        input=\"What is the difference between machine learning and deep learning?\",\n",
        "        expected_output=\"Machine learning is a broad field of AI, while deep learning is a subset that uses neural networks with multiple layers.\",\n",
        "        context=[\"Both are important areas of artificial intelligence with different approaches and applications.\"]\n",
        "    )\n",
        "]\n",
        "\n",
        "print(f\"Created {len(goldens)} Golden templates\")\n",
        "\n",
        "# Create dataset from goldens\n",
        "golden_dataset = LLMAgentDataset.from_goldens(\n",
        "    goldens=goldens,\n",
        "    input_id=\"golden_templates_dataset\"\n",
        ")\n",
        "\n",
        "print(f\"Golden Dataset: {golden_dataset}\")\n",
        "print(f\"Shape: {golden_dataset.df.shape}\")\n",
        "\n",
        "print(\"\\nGolden Templates Preview:\")\n",
        "display(golden_dataset.df[['input', 'expected_output', 'context', 'type']].head())\n",
        "\n",
        "# Mock LLM application function for demonstration\n",
        "def mock_llm_application(input_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Simulate an LLM application generating responses.\n",
        "    In production, this would be your actual LLM application.\n",
        "    \"\"\"\n",
        "    \n",
        "    responses = {\n",
        "        \"neural networks\": \"\"\"Neural networks are computational models inspired by the human brain. \n",
        "        They consist of interconnected nodes (neurons) that process information by learning patterns from data. \n",
        "        These networks can recognize complex patterns and make predictions, making them useful for tasks like \n",
        "        image recognition, natural language processing, and decision-making.\"\"\",\n",
        "        \n",
        "        \"cloud computing\": \"\"\"Cloud computing provides businesses with flexible, scalable access to computing resources \n",
        "        over the internet. Key benefits include reduced upfront costs, automatic scaling based on demand, \n",
        "        improved collaboration through shared access, enhanced security through professional data centers, \n",
        "        and reduced need for internal IT maintenance.\"\"\",\n",
        "        \n",
        "        \"password encryption\": \"\"\"Password encryption protects user data by converting passwords into complex, \n",
        "        unreadable strings using mathematical algorithms. When you enter your password, it's immediately encrypted \n",
        "        before storage or transmission. Even if data is intercepted, the encrypted password appears as random characters, \n",
        "        making it virtually impossible for attackers to determine the original password.\"\"\",\n",
        "        \n",
        "        \"machine learning\": \"\"\"Machine learning is a broad approach to artificial intelligence where computers learn \n",
        "        to make predictions or decisions by finding patterns in data. Deep learning is a specialized subset that uses \n",
        "        artificial neural networks with multiple layers (hence 'deep') to process information in ways that mimic \n",
        "        human brain function, enabling more sophisticated pattern recognition and decision-making.\"\"\"\n",
        "    }\n",
        "    \n",
        "    # Simple keyword matching for demonstration\n",
        "    input_lower = input_text.lower()\n",
        "    for keyword, response in responses.items():\n",
        "        if keyword in input_lower:\n",
        "            return response.strip()\n",
        "    \n",
        "    return f\"Thank you for your question about: {input_text}. I'd be happy to provide a comprehensive answer based on current knowledge and best practices.\"\n",
        "\n",
        "print(f\"\\nMock LLM application ready - will generate responses for {len(goldens)} templates\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert goldens to test cases by generating actual outputs\n",
        "print(\"Converting Golden templates to test cases...\")\n",
        "\n",
        "print(\"Before conversion:\")\n",
        "print(f\"  - Test cases: {len(golden_dataset.test_cases)}\")\n",
        "print(f\"  - Goldens: {len(golden_dataset.goldens)}\")\n",
        "\n",
        "# Convert goldens to test cases using our mock LLM\n",
        "golden_dataset.convert_goldens_to_test_cases(mock_llm_application)\n",
        "\n",
        "print(\"\\nAfter conversion:\")\n",
        "print(f\"  - Test cases: {len(golden_dataset.test_cases)}\")\n",
        "print(f\"  - Goldens: {len(golden_dataset.goldens)}\")\n",
        "\n",
        "print(\"\\nConversion completed!\")\n",
        "\n",
        "# Show the updated dataset\n",
        "print(\"\\nUpdated Dataset with Generated Outputs:\")\n",
        "dataset_df = golden_dataset.df\n",
        "# Filter for rows with actual output\n",
        "mask = pd.notna(dataset_df['actual_output']) & (dataset_df['actual_output'] != '')\n",
        "converted_df = dataset_df[mask]\n",
        "\n",
        "if not converted_df.empty:\n",
        "    display(converted_df[['input', 'actual_output', 'expected_output']])\n",
        "    \n",
        "    # Analyze output lengths using pandas string methods\n",
        "    actual_lengths = pd.Series([len(str(x)) for x in converted_df['actual_output']])\n",
        "    expected_lengths = pd.Series([len(str(x)) for x in converted_df['expected_output']])\n",
        "else:\n",
        "    print(\"No converted test cases found\")\n",
        "\n",
        "print(f\"\\nOutput Analysis:\")\n",
        "print(f\"Average actual output length: {actual_lengths.mean():.0f} characters\")\n",
        "print(f\"Average expected output length: {expected_lengths.mean():.0f} characters\")\n",
        "print(f\"Ratio (actual/expected): {(actual_lengths.mean() / expected_lengths.mean()):.2f}x\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 5: ValidMind Integration\n",
        "\n",
        "Now let's demonstrate how to integrate our LLMAgentDataset with ValidMind's testing framework.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize ValidMind\n",
        "print(\"Integrating with ValidMind framework...\")\n",
        "\n",
        "try:\n",
        "    # Initialize ValidMind\n",
        "    vm.init()\n",
        "    print(\"ValidMind initialized\")\n",
        "    \n",
        "    # Register our datasets with ValidMind\n",
        "    datasets_to_register = [\n",
        "        (simple_dataset, \"simple_qa_dataset\"),\n",
        "        (rag_dataset, \"rag_evaluation_dataset\"),\n",
        "        (agent_dataset, \"agent_evaluation_dataset\"),\n",
        "        (golden_dataset, \"golden_templates_dataset\")\n",
        "    ]\n",
        "    \n",
        "    for dataset, dataset_id in datasets_to_register:\n",
        "        try:\n",
        "            vm.init_dataset(\n",
        "                dataset=dataset.df,\n",
        "                input_id=dataset_id,\n",
        "                text_column=\"input\",\n",
        "                target_column=\"expected_output\"\n",
        "            )\n",
        "            print(f\"Registered: {dataset_id}\")\n",
        "        except Exception as e:\n",
        "            print(f\"WARNING: Failed to register {dataset_id}: {e}\")\n",
        "    \n",
        "    # Note: ValidMind datasets are now registered and can be used in test suites\n",
        "    print(\"\\nValidMind Integration Complete:\")\n",
        "    print(\"  - Datasets registered successfully\")\n",
        "    print(\"  - Ready for use in ValidMind test suites\")\n",
        "    print(\"  - Can be referenced by their input_id in test configurations\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"ERROR: ValidMind integration failed: {e}\")\n",
        "    print(\"Note: Some ValidMind features may require additional setup\")\n",
        "\n",
        "# Demonstrate dataset compatibility\n",
        "print(f\"\\nDataset Compatibility Check:\")\n",
        "print(f\"All datasets inherit from VMDataset: SUCCESS\")\n",
        "\n",
        "for dataset, name in [(simple_dataset, \"Simple Q&A\"), (rag_dataset, \"RAG\"), (agent_dataset, \"Agent\"), (golden_dataset, \"Golden\")]:\n",
        "    print(f\"\\n{name} Dataset:\")\n",
        "    print(f\"  - Type: {type(dataset).__name__}\")\n",
        "    print(f\"  - Inherits VMDataset: {hasattr(dataset, 'df')}\")\n",
        "    print(f\"  - Has text_column: {hasattr(dataset, 'text_column')}\")\n",
        "    print(f\"  - Has target_column: {hasattr(dataset, 'target_column')}\")\n",
        "    print(f\"  - DataFrame shape: {dataset.df.shape}\")\n",
        "    print(f\"  - Columns: {len(dataset.columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 6: Custom Metrics with G-Eval\n",
        "\n",
        "One of DeepEval's most powerful features is the ability to create custom evaluation metrics using G-Eval (Generative Evaluation).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create custom evaluation metrics using G-Eval\n",
        "print(\"Creating custom evaluation metrics...\")\n",
        "\n",
        "# Custom metric 1: Technical Accuracy\n",
        "technical_accuracy_metric = GEval(\n",
        "    name=\"Technical Accuracy\",\n",
        "    criteria=\"\"\"Evaluate whether the response is technically accurate and uses appropriate \n",
        "    terminology for the domain. Consider if the explanations are scientifically sound \n",
        "    and if technical concepts are explained correctly.\"\"\",\n",
        "    evaluation_params=[\n",
        "        LLMTestCaseParams.INPUT,\n",
        "        LLMTestCaseParams.ACTUAL_OUTPUT,\n",
        "        LLMTestCaseParams.CONTEXT\n",
        "    ],\n",
        "    threshold=0.8\n",
        ")\n",
        "\n",
        "# Custom metric 2: Clarity and Comprehensiveness  \n",
        "clarity_metric = GEval(\n",
        "    name=\"Clarity and Comprehensiveness\",\n",
        "    criteria=\"\"\"Assess whether the response is clear, well-structured, and comprehensive. \n",
        "    The response should be easy to understand, logically organized, and address all \n",
        "    aspects of the user's question without being overly verbose.\"\"\",\n",
        "    evaluation_params=[\n",
        "        LLMTestCaseParams.INPUT,\n",
        "        LLMTestCaseParams.ACTUAL_OUTPUT\n",
        "    ],\n",
        "    threshold=0.75\n",
        ")\n",
        "\n",
        "# Custom metric 3: Business Context Appropriateness\n",
        "business_context_metric = GEval(\n",
        "    name=\"Business Context Appropriateness\", \n",
        "    criteria=\"\"\"Evaluate whether the response is appropriate for a business context. \n",
        "    Consider if the tone is professional, if the content is relevant to business needs, \n",
        "    and if it provides actionable information that would be valuable to a business user.\"\"\",\n",
        "    evaluation_params=[\n",
        "        LLMTestCaseParams.INPUT,\n",
        "        LLMTestCaseParams.ACTUAL_OUTPUT,\n",
        "        LLMTestCaseParams.EXPECTED_OUTPUT\n",
        "    ],\n",
        "    threshold=0.7\n",
        ")\n",
        "\n",
        "# Custom metric 4: Tool Usage Appropriateness (for agents)\n",
        "tool_usage_metric = GEval(\n",
        "    name=\"Tool Usage Appropriateness\",\n",
        "    criteria=\"\"\"Evaluate whether the agent used appropriate tools for the given task. \n",
        "    Consider if the tools were necessary, if they were used correctly, and if the \n",
        "    agent's reasoning for tool selection was sound.\"\"\",\n",
        "    evaluation_params=[\n",
        "        LLMTestCaseParams.INPUT,\n",
        "        LLMTestCaseParams.ACTUAL_OUTPUT\n",
        "    ],\n",
        "    threshold=0.8\n",
        ")\n",
        "\n",
        "custom_metrics = [\n",
        "    technical_accuracy_metric,\n",
        "    clarity_metric, \n",
        "    business_context_metric,\n",
        "    tool_usage_metric\n",
        "]\n",
        "\n",
        "print(\"Custom metrics created:\")\n",
        "for metric in custom_metrics:\n",
        "    print(f\"  - {metric.name}: threshold {metric.threshold}\")\n",
        "\n",
        "# Demonstrate metric application to different dataset types\n",
        "print(f\"\\nMetric-Dataset Matching:\")\n",
        "metric_dataset_pairs = [\n",
        "    (\"Technical Accuracy\", \"golden_templates_dataset (tech questions)\"),\n",
        "    (\"Clarity and Comprehensiveness\", \"simple_qa_dataset (general Q&A)\"),\n",
        "    (\"Business Context Appropriateness\", \"rag_evaluation_dataset (business support)\"),\n",
        "    (\"Tool Usage Appropriateness\", \"agent_evaluation_dataset (agent actions)\")\n",
        "]\n",
        "\n",
        "for metric_name, dataset_name in metric_dataset_pairs:\n",
        "    print(f\"  - {metric_name} → {dataset_name}\")\n",
        "\n",
        "print(f\"\\nEvaluation Setup (Demo Mode):\")\n",
        "print(\"Note: Actual evaluation requires OpenAI API key\")\n",
        "print(\"These metrics would evaluate:\")\n",
        "print(\"  - Technical accuracy of AI/ML explanations\") \n",
        "print(\"  - Clarity of business support responses\")\n",
        "print(\"  - Appropriateness of agent tool usage\")\n",
        "print(\"  - Overall comprehensiveness across all domains\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 7: Best Practices & Production Patterns\n",
        "\n",
        "Let's wrap up with some best practices and real-world usage patterns for production systems.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate best practices and production patterns\n",
        "print(\"Production Best Practices Summary\")\n",
        "\n",
        "# 1. Dataset Organization\n",
        "print(\"\\n1. Dataset Organization by Use Case:\")\n",
        "all_test_cases = simple_dataset.test_cases + rag_test_cases + agent_test_cases + golden_dataset.test_cases\n",
        "\n",
        "# Categorize test cases\n",
        "categorized_cases = {\n",
        "    \"Simple Q&A\": [],\n",
        "    \"RAG Systems\": [],\n",
        "    \"Agent Systems\": [],\n",
        "    \"Technical Content\": []\n",
        "}\n",
        "\n",
        "for case in all_test_cases:\n",
        "    if hasattr(case, 'retrieval_context') and case.retrieval_context:\n",
        "        categorized_cases[\"RAG Systems\"].append(case)\n",
        "    elif hasattr(case, 'tools_called') and case.tools_called:\n",
        "        categorized_cases[\"Agent Systems\"].append(case)\n",
        "    elif any(keyword in case.input.lower() for keyword in ['neural', 'machine learning', 'encryption', 'cloud']):\n",
        "        categorized_cases[\"Technical Content\"].append(case)\n",
        "    else:\n",
        "        categorized_cases[\"Simple Q&A\"].append(case)\n",
        "\n",
        "for category, cases in categorized_cases.items():\n",
        "    print(f\"  - {category}: {len(cases)} test cases\")\n",
        "\n",
        "# 2. Metric Selection Strategy\n",
        "print(\"\\n2. Metric Selection Strategy:\")\n",
        "metric_recommendations = {\n",
        "    \"Simple Q&A\": [\"AnswerRelevancyMetric\", \"GEval(Correctness)\", \"HallucinationMetric\"],\n",
        "    \"RAG Systems\": [\"FaithfulnessMetric\", \"ContextualRelevancyMetric\", \"AnswerRelevancyMetric\"],\n",
        "    \"Agent Systems\": [\"ToolCorrectnessMetric\", \"TaskCompletionMetric\", \"GEval(Tool Usage)\"],\n",
        "    \"Technical Content\": [\"GEval(Technical Accuracy)\", \"GEval(Clarity)\", \"BiasMetric\"]\n",
        "}\n",
        "\n",
        "for use_case, metrics in metric_recommendations.items():\n",
        "    print(f\"  - {use_case}:\")\n",
        "    for metric in metrics:\n",
        "        print(f\"    • {metric}\")\n",
        "\n",
        "# 3. Evaluation Frequency\n",
        "print(\"\\n3. Evaluation Frequency Recommendations:\")\n",
        "evaluation_schedule = {\n",
        "    \"Development\": \"Every code commit\",\n",
        "    \"Staging\": \"Before each deployment\", \n",
        "    \"Production\": \"Daily monitoring\",\n",
        "    \"Model Updates\": \"Before and after model changes\",\n",
        "    \"Dataset Updates\": \"When new training data is added\"\n",
        "}\n",
        "\n",
        "for stage, frequency in evaluation_schedule.items():\n",
        "    print(f\"  - {stage}: {frequency}\")\n",
        "\n",
        "# 4. Production Integration Example\n",
        "print(\"\\n4. Production Integration Pattern:\")\n",
        "production_example = '''\n",
        "# Example production integration\n",
        "def evaluate_llm_system(production_logs, model_version):\n",
        "    # Convert logs to test cases\n",
        "    test_cases = []\n",
        "    for log in production_logs:\n",
        "        test_case = LLMTestCase(\n",
        "            input=log['user_query'],\n",
        "            actual_output=log['llm_response'],\n",
        "            context=log.get('context', []),\n",
        "            retrieval_context=log.get('retrieved_docs', [])\n",
        "        )\n",
        "        test_cases.append(test_case)\n",
        "    \n",
        "    # Create dataset\n",
        "    dataset = LLMAgentDataset.from_test_cases(\n",
        "        test_cases=test_cases,\n",
        "        input_id=f\"production_eval_{model_version}\"\n",
        "    )\n",
        "    \n",
        "    # Run evaluation\n",
        "    metrics = [\n",
        "        AnswerRelevancyMetric(threshold=0.8),\n",
        "        FaithfulnessMetric(threshold=0.85),\n",
        "        HallucinationMetric(threshold=0.2)\n",
        "    ]\n",
        "    \n",
        "    results = dataset.evaluate_with_deepeval(\n",
        "        metrics=metrics,\n",
        "        hyperparameters={\"model_version\": model_version}\n",
        "    )\n",
        "    \n",
        "    return results\n",
        "'''\n",
        "\n",
        "print(production_example)\n",
        "\n",
        "# 5. Performance Optimization\n",
        "print(\"\\n5. Performance Optimization Tips:\")\n",
        "optimization_tips = [\n",
        "    \"Use batch evaluation for multiple test cases\",\n",
        "    \"Cache evaluation results to avoid re-computation\",\n",
        "    \"Run evaluations async when possible\",\n",
        "    \"Set appropriate thresholds based on use case requirements\",\n",
        "    \"Monitor evaluation costs and optimize API usage\",\n",
        "    \"Use sampling for large datasets in development\"\n",
        "]\n",
        "\n",
        "for i, tip in enumerate(optimization_tips, 1):\n",
        "    print(f\"  {i}. {tip}\")\n",
        "\n",
        "# 6. Quality Assurance\n",
        "print(\"\\n6. Quality Assurance Guidelines:\")\n",
        "qa_guidelines = [\n",
        "    \"Maintain diverse test cases covering edge cases\",\n",
        "    \"Regular review and update of evaluation criteria\",\n",
        "    \"Track metric trends over time\",\n",
        "    \"Set up alerts for significant performance drops\",\n",
        "    \"Include human evaluation for critical use cases\",\n",
        "    \"Document evaluation methodology and threshold rationale\"\n",
        "]\n",
        "\n",
        "for i, guideline in enumerate(qa_guidelines, 1):\n",
        "    print(f\"  {i}. {guideline}\")\n",
        "\n",
        "print(f\"\\nCurrent Demo Summary:\")\n",
        "print(f\"  - Total test cases created: {len(all_test_cases)}\")\n",
        "print(f\"  - Datasets created: 4\")\n",
        "print(f\"  - Custom metrics defined: {len(custom_metrics)}\")\n",
        "print(f\"  - ValidMind integration: SUCCESS\")\n",
        "print(f\"  - Production patterns: SUCCESS\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ValidMind Library",
      "language": "python",
      "name": "validmind"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ValidMind for model validation — 114 Finalize testing and reporting\n",
    "\n",
    "Learn how to use ValidMind for your end-to-end model validation process with our series of four introductory notebooks. In this last notebook, you'll configure and run some custom tests, then add test results and findings to your validation report.\n",
    "\n",
    "As we concluded in [113 Perform validation tests](113-perform_validation_tests.ipynb), our challenger random forest classification model was not a viable candidate for our use case and was eliminated as a contender. We'll finish up by comparing our champion application scorecard model against our remaining challenger logistic regression model, then use the ValidMind Platform to put together our validation report supplemented by our logged test results as evidence and findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "In order to finalize the validation testing and reporting for your sample model, you'll need to first have:\n",
    "\n",
    "- [ ] Registered a model within the ValidMind Platform and granted yourself access to the model as a validator\n",
    "- [ ] Installed the ValidMind Library in your local environment, allowing you to access all its features\n",
    "- [ ] Learned how to import and initialize datasets for use with ValidMind\n",
    "- [ ] Learned how to enable custom context for test descriptions generated by ValidMind\n",
    "- [ ] Understood the basics of how to identify and run validation tests\n",
    "- [ ] Run data quality and model performance tests for your champion and challenger models, and logged the results of those tests to the ValidMind Platform\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: #B5B5B510; color: black; border: 1px solid #083E44; border-left-width: 5px; box-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);border-radius: 5px;\"><span style=\"color: #083E44;\"><b>Need help with the above steps?</b></span>\n",
    "<br></br>\n",
    "Refer to the first three notebooks in this series:\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"111-import_champion_model.ipynb\" style=\"color: #DE257E;\"><b>111 Import the champion model</b></a></li>\n",
    "    <li><a href=\"112-develop_challenger_models.ipynb\" style=\"color: #DE257E;\"><b>112 Develop potential challenger models</b></a></li>\n",
    "    <li><a href=\"113-perform_validation_tests.ipynb\" style=\"color: #DE257E;\"><b>113 Perform validation tests</b></a></li>\n",
    "</ol>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up\n",
    "\n",
    "This section should be very familiar to you now — as we performed the same actions in the previous two notebooks in this series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the ValidMind Library\n",
    "\n",
    "As usual, let's first connect up the ValidMind Library to our model we previously registered in the ValidMind Platform:\n",
    "\n",
    "1. In a browser, [log in to ValidMind](https://docs.validmind.ai/guide/configuration/log-in-to-validmind.html).\n",
    "\n",
    "2. In the left sidebar, navigate to **Inventory** and select the model you registered for this \"ValidMind for model validation\" series of notebooks.\n",
    "\n",
    "3. Go to **Getting Started** and click **Copy snippet to clipboard**.\n",
    "\n",
    "Next, [load your model identifier credentials from an `.env` file](https://docs.validmind.ai/developer/model-documentation/store-credentials-in-env-file.html) or replace the placeholder with your own code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the ValidMind Library is installed\n",
    "\n",
    "%pip install -q validmind\n",
    "\n",
    "# Load your model identifier credentials from an `.env` file\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv .env\n",
    "\n",
    "# Or replace with your code snippet\n",
    "\n",
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "    # api_host=\"...\",\n",
    "    # api_key=\"...\",\n",
    "    # api_secret=\"...\",\n",
    "    # model=\"...\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the champion model\n",
    "\n",
    "Next, we'll import the champion model submitted by the model development team as we used in the last notebooks (**[xgb_model_champion.pkl](xgb_model_champion.pkl)**) and load in the same sample [Lending Club](https://www.kaggle.com/datasets/devanshi23/loan-data-2007-2014/data) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "#Load the saved model\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.load_model(\"xgb_model_champion.pkl\")\n",
    "xgb_model\n",
    "\n",
    "# Ensure that we have to appropriate order in feature names from Champion model and dataset\n",
    "cols_when_model_builds = xgb_model.get_booster().feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Lending Club dataset from Kaggle\n",
    "from validmind.datasets.credit_risk import lending_club\n",
    "\n",
    "df = lending_club.load_data(source=\"offline\")\n",
    "df.head()\n",
    "\n",
    "# Preprocess the dataset for data quality testing purposes\n",
    "preprocess_df = lending_club.preprocess(df)\n",
    "\n",
    "# Apply feature engineering to the dataset\n",
    "fe_df = lending_club.feature_engineering(preprocess_df)\n",
    "fe_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our dataset into train and test to start the validation testing process\n",
    "train_df, test_df = lending_club.split(fe_df, test_size=0.2)\n",
    "\n",
    "x_train = train_df.drop(lending_club.target_column, axis=1)\n",
    "y_train = train_df[lending_club.target_column]\n",
    "\n",
    "x_test = test_df.drop(lending_club.target_column, axis=1)\n",
    "y_test = test_df[lending_club.target_column]\n",
    "\n",
    "# Now let's apply the order of features from the champion model construction\n",
    "x_train = x_train[cols_when_model_builds]\n",
    "x_test = x_test[cols_when_model_builds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_use = ['annual_inc_woe',\n",
    " 'verification_status_woe',\n",
    " 'emp_length_woe',\n",
    " 'installment_woe',\n",
    " 'term_woe',\n",
    " 'home_ownership_woe',\n",
    " 'purpose_woe',\n",
    " 'open_acc_woe',\n",
    " 'total_acc_woe',\n",
    " 'int_rate_woe',\n",
    " 'sub_grade_woe',\n",
    " 'grade_woe','loan_status']\n",
    "\n",
    "\n",
    "train_df = train_df[cols_use]\n",
    "test_df = test_df[cols_use]\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the challenger model\n",
    "\n",
    "As we eliminated the random forest classification model as a challenger, we'll only train our logistic regression model here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Logistic Regression grid params\n",
    "log_reg_params = {\n",
    "    \"penalty\": [\"l1\", \"l2\"],\n",
    "    \"C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    \"solver\": [\"liblinear\"],\n",
    "}\n",
    "\n",
    "# Grid search for Logistic Regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\n",
    "grid_log_reg.fit(x_train, y_train)\n",
    "\n",
    "# Logistic Regression best estimator\n",
    "log_reg = grid_log_reg.best_estimator_\n",
    "log_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract predicted probabilities\n",
    "\n",
    "With our challenger model trained, let's extract the predicted probabilities from our two models and convert the probability predictions into a binary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Champion — Application scorecard model\n",
    "train_xgb_prob = xgb_model.predict_proba(x_train)[:, 1]\n",
    "test_xgb_prob = xgb_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Challenger — Logistic regression model\n",
    "train_log_prob = log_reg.predict_proba(x_train)[:, 1]\n",
    "test_log_prob = log_reg.predict_proba(x_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If probability > 0.3 = 1 (positive)\n",
    "cut_off_threshold = 0.3\n",
    "\n",
    "# Champion — Application scorecard model\n",
    "train_xgb_binary_predictions = (train_xgb_prob > cut_off_threshold).astype(int)\n",
    "test_xgb_binary_predictions = (test_xgb_prob > cut_off_threshold).astype(int)\n",
    "\n",
    "# Challenger — Logistic regression model\n",
    "train_log_binary_predictions = (train_log_prob > cut_off_threshold).astype(int)\n",
    "test_log_binary_predictions = (test_log_prob > cut_off_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the ValidMind objects\n",
    "\n",
    "Let's initialize the ValidMind `Dataset` and `Model` objects in preparation for assigning model predictions to each dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the raw dataset\n",
    "vm_raw_dataset = vm.init_dataset(\n",
    "    dataset=df,\n",
    "    input_id=\"raw_dataset\",\n",
    "    target_column=lending_club.target_column,\n",
    ")\n",
    "\n",
    "# Initialize the preprocessed dataset\n",
    "vm_preprocess_dataset = vm.init_dataset(\n",
    "    dataset=preprocess_df,\n",
    "    input_id=\"preprocess_dataset\",\n",
    "    target_column=lending_club.target_column,\n",
    ")\n",
    "\n",
    "# Initialize the feature engineered dataset\n",
    "vm_fe_dataset = vm.init_dataset(\n",
    "    dataset=fe_df,\n",
    "    input_id=\"fe_dataset\",\n",
    "    target_column=lending_club.target_column,\n",
    ")\n",
    "\n",
    "# Initialize the training dataset\n",
    "vm_train_ds = vm.init_dataset(\n",
    "    dataset=train_df,\n",
    "    input_id=\"train_dataset\",\n",
    "    target_column=lending_club.target_column,\n",
    ")\n",
    "\n",
    "# Initialize the test dataset\n",
    "vm_test_ds = vm.init_dataset(\n",
    "    dataset=test_df,\n",
    "    input_id=\"test_dataset\",\n",
    "    target_column=lending_club.target_column,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the champion application scorecard model\n",
    "vm_xgb_model = vm.init_model(\n",
    "    xgb_model,\n",
    "    input_id=\"xgb_model_developer_champion\",\n",
    ")\n",
    "\n",
    "# Initialize the challenger logistic regression model\n",
    "vm_log_model = vm.init_model(\n",
    "    log_reg,\n",
    "    input_id=\"log_model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign predictions\n",
    "\n",
    "With our models registered, we'll move on to assigning both the predictive probabilities coming directly from each model's predictions, and the binary prediction after applying the cutoff threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Champion — Application scorecard model\n",
    "vm_train_ds.assign_predictions(\n",
    "    model=vm_xgb_model,\n",
    "    prediction_values=train_xgb_binary_predictions,\n",
    "    prediction_probabilities=train_xgb_prob,\n",
    ")\n",
    "\n",
    "vm_test_ds.assign_predictions(\n",
    "    model=vm_xgb_model,\n",
    "    prediction_values=test_xgb_binary_predictions,\n",
    "    prediction_probabilities=test_xgb_prob,\n",
    ")\n",
    "\n",
    "# Challenger — Logistic regression model\n",
    "vm_train_ds.assign_predictions(\n",
    "    model=vm_log_model,\n",
    "    prediction_values=train_log_binary_predictions,\n",
    "    prediction_probabilities=train_log_prob,\n",
    ")\n",
    "\n",
    "vm_test_ds.assign_predictions(\n",
    "    model=vm_log_model,\n",
    "    prediction_values=test_log_binary_predictions,\n",
    "    prediction_probabilities=test_log_prob,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the scores\n",
    "train_xgb_scores = lending_club.compute_scores(train_xgb_prob)\n",
    "test_xgb_scores = lending_club.compute_scores(test_xgb_prob)\n",
    "train_log_scores = lending_club.compute_scores(train_log_prob)\n",
    "test_log_scores = lending_club.compute_scores(test_log_prob)\n",
    "\n",
    "# Assign scores to the datasets\n",
    "vm_train_ds.add_extra_column(\"xgb_scores\", train_xgb_scores)\n",
    "vm_test_ds.add_extra_column(\"xgb_scores\", test_xgb_scores)\n",
    "vm_train_ds.add_extra_column(\"log_scores\", train_log_scores)\n",
    "vm_test_ds.add_extra_column(\"log_scores\", test_log_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable use case context\n",
    "\n",
    "We'll also adjust the use case context to focus on comparison between our models for tests going forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"VALIDMIND_LLM_DESCRIPTIONS_CONTEXT_ENABLED\"] = \"1\"\n",
    "\n",
    "context = \"\"\"\n",
    "FORMAT FOR THE LLM DESCRIPTIONS: \n",
    "    **<Test Name>** is designed to <begin with a concise overview of what the test does and its primary purpose, \n",
    "    extracted from the test description>.\n",
    "\n",
    "    The test operates by <write a paragraph about the test mechanism, explaining how it works and what it measures. \n",
    "    Include any relevant formulas or methodologies mentioned in the test description.>\n",
    "\n",
    "    The primary advantages of this test include <write a paragraph about the test's strengths and capabilities, \n",
    "    highlighting what makes it particularly useful for specific scenarios.>\n",
    "\n",
    "    Users should be aware that <write a paragraph about the test's limitations and potential risks. \n",
    "    Include both technical limitations and interpretation challenges. \n",
    "    If the test description includes specific signs of high risk, incorporate these here.>\n",
    "\n",
    "    **Key Insights:**\n",
    "\n",
    "    The test results reveal:\n",
    "\n",
    "    - **<insight title>**: <comprehensive description of one aspect of the results>\n",
    "    - **<insight title>**: <comprehensive description of another aspect>\n",
    "    ...\n",
    "\n",
    "    Based on these results, <conclude with a brief paragraph that ties together the test results with the test's \n",
    "    purpose and provides any final recommendations or considerations.>\n",
    "\n",
    "ADDITIONAL INSTRUCTIONS:\n",
    "\n",
    "    The champion model as the basis for comparison is called \"xgb_model_developer_champion\" and emphasis should be on the following:\n",
    "    - The metrics for the champion model compared against the challenger models\n",
    "    - Which model potentially outperforms the champion model based on the metrics, this should be highlighted and emphasized\n",
    "\n",
    "\n",
    "    For each metric in the test results, include in the test overview:\n",
    "    - The metric's purpose and what it measures\n",
    "    - Its mathematical formula\n",
    "    - The range of possible values\n",
    "    - What constitutes good/bad performance\n",
    "    - How to interpret different values\n",
    "\n",
    "    Each insight should progressively cover:\n",
    "    1. Overall scope and distribution\n",
    "    2. Complete breakdown of all elements with specific values\n",
    "    3. Natural groupings and patterns\n",
    "    4. Comparative analysis between datasets/categories\n",
    "    5. Stability and variations\n",
    "    6. Notable relationships or dependencies\n",
    "\n",
    "    Remember:\n",
    "    - Champion model (xgb_model_developer_champion) is the selection and challenger models are used to challenge the selection\n",
    "    - Keep all insights at the same level (no sub-bullets or nested structures)\n",
    "    - Make each insight complete and self-contained\n",
    "    - Include specific numerical values and ranges\n",
    "    - Cover all elements in the results comprehensively\n",
    "    - Maintain clear, concise language\n",
    "    - Use only \"- **Title**: Description\" format for insights\n",
    "    - Progress naturally from general to specific observations\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "os.environ[\"VALIDMIND_LLM_DESCRIPTIONS_CONTEXT\"] = context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP\n",
    "\n",
    "Now let's dig a little bit deeper into one of the tests that allows the Validator to custoimze parameters and thresholds for performance standards \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = vm.tests.run_test(\n",
    "    \"validmind.model_validation.sklearn.MinimumF1Score:AdjThreshold\",\n",
    "    input_grid={\n",
    "        \"dataset\": [vm_test_ds],\n",
    "        \"model\": [vm_xgb_model, vm_log_model],\n",
    "        \"params\": {\"min_threshold\": 0.35},\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Robustness and Stability Testing Comparison Between the Two Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.list_tests(tags=[\"model_diagnosis\"], task=\"classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if models suffer from any overfit potentials and also where there are potential sub-segments of issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overfit_testing = [\n",
    "    \"validmind.model_validation.sklearn.TrainingTestDegradation:Champion_vs_LogRegression\",\n",
    "    \"validmind.model_validation.sklearn.OverfitDiagnosis:Champion_vs_LogRegression\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in overfit_testing:\n",
    "    vm.tests.run_test(\n",
    "        test,\n",
    "        input_grid={\n",
    "            \"datasets\": [[vm_train_ds,vm_test_ds]], \"model\" : [vm_xgb_model,vm_log_model], \n",
    "        },\n",
    "    ).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now finally let's conduct robustness and stability testing of the two models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stab_robust = ['validmind.model_validation.sklearn.RobustnessDiagnosis:Champion_vs_LogRegression']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in stab_robust:\n",
    "    vm.tests.run_test(\n",
    "        test,\n",
    "        input_grid={\n",
    "            \"datasets\": [[vm_train_ds,vm_test_ds]], \"model\" : [vm_xgb_model,vm_log_model], \n",
    "        },\n",
    "    ).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP2\n",
    "\n",
    "Let's verify the feature importance and inspect differences - different models might have more intuitive feature impacts that might lead to decisions in selection of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FI = list_tests(tags=[\"feature_importance\"], task=\"classification\",pretty=False)\n",
    "FI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test in FI:\n",
    "    vm.tests.run_test(\n",
    "        \"\".join((test,':Champion_vs_LogisticRegression')),\n",
    "        input_grid={\n",
    "            \"dataset\": [vm_test_ds], \"model\" : [vm_xgb_model,vm_log_model], \n",
    "        },\n",
    "    ).log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finish off with a custom test example - scoring (customization of output to a FICO score type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "@vm.test(\"my_custom_tests.ScoreToOdds\")\n",
    "def score_to_odds_analysis(dataset, score_column='score', score_bands=[410, 440, 470]):\n",
    "    \"\"\"\n",
    "    Analyzes the relationship between score bands and odds (good:bad ratio).\n",
    "    Good odds = (1 - default_rate) / default_rate\n",
    "    \n",
    "    Higher scores should correspond to higher odds of being good.\n",
    "\n",
    "    If there are multiple scores provided through score_column, this means that there are two different models and the scores reflect each model\n",
    "\n",
    "    If there are more scores provided in the score_column then focus the assessment on the differences between the two scores and indicate through evidence which one is preferred.\n",
    "    \"\"\"\n",
    "    df = dataset.df\n",
    "    \n",
    "    # Create score bands\n",
    "    df['score_band'] = pd.cut(\n",
    "        df[score_column],\n",
    "        bins=[-np.inf] + score_bands + [np.inf],\n",
    "        labels=[f'<{score_bands[0]}'] + \n",
    "               [f'{score_bands[i]}-{score_bands[i+1]}' for i in range(len(score_bands)-1)] +\n",
    "               [f'>{score_bands[-1]}']\n",
    "    )\n",
    "    \n",
    "    # Calculate metrics per band\n",
    "    results = df.groupby('score_band').agg({\n",
    "        dataset.target_column: ['mean', 'count']\n",
    "    })\n",
    "    \n",
    "    results.columns = ['Default Rate', 'Total']\n",
    "    results['Good Count'] = results['Total'] - (results['Default Rate'] * results['Total'])\n",
    "    results['Bad Count'] = results['Default Rate'] * results['Total']\n",
    "    results['Odds'] = results['Good Count'] / results['Bad Count']\n",
    "    \n",
    "    # Create visualization\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add odds bars\n",
    "    fig.add_trace(go.Bar(\n",
    "        name='Odds (Good:Bad)',\n",
    "        x=results.index,\n",
    "        y=results['Odds'],\n",
    "        marker_color='blue'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title='Score-to-Odds Analysis',\n",
    "        yaxis=dict(title='Odds Ratio (Good:Bad)'),\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = vm.tests.run_test(\n",
    "    \"my_custom_tests.ScoreToOdds:Champion_vs_Challenger\",\n",
    "    inputs={\n",
    "        \"dataset\": vm_test_ds,\n",
    "    },\n",
    "    param_grid={\n",
    "        \"score_column\": [\"xgb_scores\",\"log_scores\"],\n",
    "        \"score_bands\": [[500, 540, 570]],\n",
    "    },\n",
    ").log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP3\n",
    "\n",
    "Finally we got all of the tests from the Developer that was provided as evidence, now as a final task we will verify testing being appropriately recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from validmind.utils import preview_test_config\n",
    "\n",
    "test_config = {'validmind.data_validation.DatasetDescription:raw_data': {'inputs': {'dataset': 'raw_dataset'}},\n",
    " 'validmind.data_validation.DescriptiveStatistics:raw_data': {'inputs': {'dataset': 'raw_dataset'}},\n",
    " 'validmind.data_validation.MissingValues:raw_data': {'inputs': {'dataset': 'raw_dataset'},\n",
    "  'params': {'min_threshold': 1}},\n",
    " 'validmind.data_validation.ClassImbalance:raw_data': {'inputs': {'dataset': 'raw_dataset'},\n",
    "  'params': {'min_percent_threshold': 10}},\n",
    " 'validmind.data_validation.Duplicates:raw_data': {'inputs': {'dataset': 'raw_dataset'},\n",
    "  'params': {'min_threshold': 1}},\n",
    " 'validmind.data_validation.HighCardinality:raw_data': {'inputs': {'dataset': 'raw_dataset'},\n",
    "  'params': {'num_threshold': 100,\n",
    "   'percent_threshold': 0.1,\n",
    "   'threshold_type': 'percent'}},\n",
    " 'validmind.data_validation.Skewness:raw_data': {'inputs': {'dataset': 'raw_dataset'},\n",
    "  'params': {'max_threshold': 1}},\n",
    " 'validmind.data_validation.UniqueRows:raw_data': {'inputs': {'dataset': 'raw_dataset'},\n",
    "  'params': {'min_percent_threshold': 1}},\n",
    " 'validmind.data_validation.TooManyZeroValues:raw_data': {'inputs': {'dataset': 'raw_dataset'},\n",
    "  'params': {'max_percent_threshold': 0.03}},\n",
    " 'validmind.data_validation.IQROutliersTable:raw_data': {'inputs': {'dataset': 'raw_dataset'},\n",
    "  'params': {'threshold': 5}},\n",
    " 'validmind.data_validation.DescriptiveStatistics:preprocessed_data': {'inputs': {'dataset': 'preprocess_dataset'}},\n",
    " 'validmind.data_validation.TabularDescriptionTables:preprocessed_data': {'inputs': {'dataset': 'preprocess_dataset'}},\n",
    " 'validmind.data_validation.MissingValues:preprocessed_data': {'inputs': {'dataset': 'preprocess_dataset'},\n",
    "  'params': {'min_threshold': 1}},\n",
    " 'validmind.data_validation.TabularNumericalHistograms:preprocessed_data': {'inputs': {'dataset': 'preprocess_dataset'}},\n",
    " 'validmind.data_validation.TabularCategoricalBarPlots:preprocessed_data': {'inputs': {'dataset': 'preprocess_dataset'}},\n",
    " 'validmind.data_validation.TargetRateBarPlots:preprocessed_data': {'inputs': {'dataset': 'preprocess_dataset'},\n",
    "  'params': {'default_column': 'loan_status'}},\n",
    " 'validmind.data_validation.DescriptiveStatistics:development_data': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset']}},\n",
    " 'validmind.data_validation.TabularDescriptionTables:development_data': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset']}},\n",
    " 'validmind.data_validation.ClassImbalance:development_data': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset']},\n",
    "  'params': {'min_percent_threshold': 10}},\n",
    " 'validmind.data_validation.UniqueRows:development_data': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset']},\n",
    "  'params': {'min_percent_threshold': 1}},\n",
    " 'validmind.data_validation.TabularNumericalHistograms:development_data': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset']}},\n",
    " 'validmind.data_validation.MutualInformation:development_data': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset']},\n",
    "  'params': {'min_threshold': 0.01}},\n",
    " 'validmind.data_validation.PearsonCorrelationMatrix:development_data': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset']}},\n",
    " 'validmind.data_validation.HighPearsonCorrelation:development_data': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset']},\n",
    "  'params': {'max_threshold': 0.3, 'top_n_correlations': 10}},\n",
    " 'validmind.data_validation.WOEBinTable': {'input_grid': {'dataset': ['preprocess_dataset']},\n",
    "  'params': {'breaks_adj': {'loan_amnt': [5000, 10000, 15000, 20000, 25000],\n",
    "    'int_rate': [10, 15, 20],\n",
    "    'annual_inc': [50000, 100000, 150000]}}},\n",
    " 'validmind.data_validation.WOEBinPlots': {'input_grid': {'dataset': ['preprocess_dataset']},\n",
    "  'params': {'breaks_adj': {'loan_amnt': [5000, 10000, 15000, 20000, 25000],\n",
    "    'int_rate': [10, 15, 20],\n",
    "    'annual_inc': [50000, 100000, 150000]}}},\n",
    " 'validmind.data_validation.DatasetSplit': {'inputs': {'datasets': ['train_dataset',\n",
    "    'test_dataset']}},\n",
    " 'validmind.model_validation.ModelMetadata': {'input_grid': {'model': ['xgb_model',\n",
    "    'rf_model']}},\n",
    " 'validmind.model_validation.sklearn.ModelParameters': {'input_grid': {'model': ['xgb_model',\n",
    "    'rf_model']}},\n",
    " 'validmind.model_validation.statsmodels.GINITable': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': ['xgb_model', 'rf_model']}},\n",
    " 'validmind.model_validation.sklearn.ClassifierPerformance': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': ['xgb_model', 'rf_model']}},\n",
    " 'validmind.model_validation.sklearn.TrainingTestDegradation:XGBoost': {'inputs': {'datasets': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': 'xgb_model'},\n",
    "  'params': {'max_threshold': 0.1}},\n",
    " 'validmind.model_validation.sklearn.TrainingTestDegradation:RandomForest': {'inputs': {'datasets': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': 'rf_model'},\n",
    "  'params': {'max_threshold': 0.1}},\n",
    " 'validmind.model_validation.sklearn.ROCCurve': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': ['xgb_model']}},\n",
    " 'validmind.model_validation.sklearn.MinimumROCAUCScore': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': ['xgb_model']},\n",
    "  'params': {'min_threshold': 0.5}},\n",
    " 'validmind.model_validation.statsmodels.PredictionProbabilitiesHistogram': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': ['xgb_model']}},\n",
    " 'validmind.model_validation.statsmodels.CumulativePredictionProbabilities': {'input_grid': {'model': ['xgb_model'],\n",
    "   'dataset': ['train_dataset', 'test_dataset']}},\n",
    " 'validmind.model_validation.sklearn.PopulationStabilityIndex': {'inputs': {'datasets': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': 'xgb_model'},\n",
    "  'params': {'num_bins': 10, 'mode': 'fixed'}},\n",
    " 'validmind.model_validation.sklearn.ConfusionMatrix': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': ['xgb_model']}},\n",
    " 'validmind.model_validation.sklearn.MinimumAccuracy': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': ['xgb_model']},\n",
    "  'params': {'min_threshold': 0.7}},\n",
    " 'validmind.model_validation.sklearn.MinimumF1Score': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': ['xgb_model']},\n",
    "  'params': {'min_threshold': 0.5}},\n",
    " 'validmind.model_validation.sklearn.PrecisionRecallCurve': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': ['xgb_model']}},\n",
    " 'validmind.model_validation.sklearn.CalibrationCurve': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': ['xgb_model']}},\n",
    " 'validmind.model_validation.sklearn.ClassifierThresholdOptimization': {'inputs': {'dataset': 'train_dataset',\n",
    "   'model': 'xgb_model'},\n",
    "  'params': {'target_recall': 0.8}},\n",
    " 'validmind.model_validation.statsmodels.ScorecardHistogram': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset']},\n",
    "  'params': {'score_column': 'xgb_scores'}},\n",
    " 'validmind.data_validation.ScoreBandDefaultRates': {'input_grid': {'dataset': ['train_dataset'],\n",
    "   'model': ['xgb_model']},\n",
    "  'params': {'score_column': 'xgb_scores', 'score_bands': [504, 537, 570]}},\n",
    " 'validmind.model_validation.sklearn.ScoreProbabilityAlignment': {'input_grid': {'dataset': ['train_dataset'],\n",
    "   'model': ['xgb_model']},\n",
    "  'params': {'score_column': 'xgb_scores'}},\n",
    " 'validmind.model_validation.sklearn.WeakspotsDiagnosis': {'inputs': {'datasets': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': 'xgb_model'}},\n",
    " 'validmind.model_validation.sklearn.OverfitDiagnosis': {'inputs': {'model': 'xgb_model',\n",
    "   'datasets': ['train_dataset', 'test_dataset']},\n",
    "  'params': {'cut_off_threshold': 0.04}},\n",
    " 'validmind.model_validation.sklearn.RobustnessDiagnosis': {'inputs': {'datasets': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': 'xgb_model'},\n",
    "  'params': {'scaling_factor_std_dev_list': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "   'performance_decay_threshold': 0.05}},\n",
    " 'validmind.model_validation.sklearn.PermutationFeatureImportance': {'input_grid': {'dataset': ['train_dataset',\n",
    "    'test_dataset'],\n",
    "   'model': ['xgb_model']}},\n",
    " 'validmind.model_validation.FeaturesAUC': {'input_grid': {'model': ['xgb_model'],\n",
    "   'dataset': ['train_dataset', 'test_dataset']}},\n",
    " 'validmind.model_validation.sklearn.SHAPGlobalImportance': {'input_grid': {'model': ['xgb_model'],\n",
    "   'dataset': ['train_dataset', 'test_dataset']},\n",
    "  'params': {'kernel_explainer_samples': 10,\n",
    "   'tree_or_linear_explainer_samples': 200}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in test_config:\n",
    "    print(t)\n",
    "    try:\n",
    "        # Check if test has input_grid\n",
    "        if 'input_grid' in test_config[t]:\n",
    "            # For tests with input_grid, pass the input_grid configuration\n",
    "            if 'params' in test_config[t]:\n",
    "                vm.tests.run_test(t, input_grid=test_config[t]['input_grid'], params=test_config[t]['params']).log()\n",
    "            else:\n",
    "                vm.tests.run_test(t, input_grid=test_config[t]['input_grid']).log()\n",
    "        else:\n",
    "            # Original logic for regular inputs\n",
    "            if 'params' in test_config[t]:\n",
    "                vm.tests.run_test(t, inputs=test_config[t]['inputs'], params=test_config[t]['params']).log()\n",
    "            else:\n",
    "                vm.tests.run_test(t, inputs=test_config[t]['inputs']).log()\n",
    "    except Exception as e:\n",
    "        print(f\"Error running test {t}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "### Work with your validation report\n",
    "\n",
    "### Learn more\n",
    "\n",
    "#### More how-to guides and code samples\n",
    "\n",
    "#### Discover more learning resources"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

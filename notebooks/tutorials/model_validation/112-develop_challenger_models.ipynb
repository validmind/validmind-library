{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ValidMind for model validation — 112 Develop potential challenger models\n",
    "\n",
    "Learn how to use ValidMind for your end-to-end model validation process based on common scenarios with our series of four introductory notebooks. In this second notebook, develop challenger models through a structured comparison process.\n",
    "\n",
    "A *challenger model* is an alternate model that attempt to outperform the champion model, ensuring that the best performing fit-for-purpose model is always considered for deployment. Challenger models also help avoid over-reliance on a single model, and allow testing of new features, algorithms, or data sources without disrupting the production lifecycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "In order to continue on the model validation journey with notebook, you'll need to first have:\n",
    "\n",
    "- [ ] Registered a model within the ValidMind Platform and granted yourself access to the model as a validator\n",
    "- [ ] Installed the ValidMind Library in your local environment, allowing you to access all its features\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: #B5B5B510; color: black; border: 1px solid #083E44; border-left-width: 5px; box-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);border-radius: 5px;\"><span style=\"color: #083E44;\"><b>Need help with the above steps?</b></span>\n",
    "<br></br>\n",
    "Refer to the first notebook in this series: <a href=\"111-set_up_validmind.ipynb\" style=\"color: #DE257E;\"><b>111 Set up ValidMind</b></a></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the ValidMind Library\n",
    "\n",
    "First, let's connect up the ValidMind Library to our model we previously registered in the ValidMind Platform:\n",
    "\n",
    "1. In a browser, [log in to ValidMind](https://docs.validmind.ai/guide/configuration/log-in-to-validmind.html).\n",
    "\n",
    "2. In the left sidebar, navigate to **Inventory** and select the model you registered for this \"ValidMind for model validation\" series of notebooks.\n",
    "\n",
    "3. Go to **Getting Started** and click **Copy snippet to clipboard**.\n",
    "\n",
    "Next, [load your model identifier credentials from an `.env` file](https://docs.validmind.ai/developer/model-documentation/store-credentials-in-env-file.html) or replace the placeholder with your own code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the ValidMind Library is installed\n",
    "\n",
    "%pip install -q validmind\n",
    "\n",
    "# Load your model identifier credentials from an `.env` file\n",
    "\n",
    "%load_ext dotenv\n",
    "%dotenv .env\n",
    "\n",
    "# Or replace with your code snippet\n",
    "\n",
    "import validmind as vm\n",
    "\n",
    "vm.init(\n",
    "    # api_host=\"...\",\n",
    "    # api_key=\"...\",\n",
    "    # api_secret=\"...\",\n",
    "    # model=\"...\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the champion model\n",
    "\n",
    "Next, we'll import the champion model submitted by the model development team as we used in the last notebook (**[xgb_model_champion.pkl](xgb_model_champion.pkl)**) and load in the same sample [Lending Club](https://www.kaggle.com/datasets/devanshi23/loan-data-2007-2014/data) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "#Load the saved model\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.load_model(\"xgb_model_champion.pkl\")\n",
    "xgb_model\n",
    "\n",
    "# Ensure that we have to appropriate order in feature names from Champion model and dataset\n",
    "cols_when_model_builds = xgb_model.get_booster().feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Lending Club dataset from Kaggle\n",
    "from validmind.datasets.credit_risk import lending_club\n",
    "\n",
    "df = lending_club.load_data(source=\"offline\")\n",
    "df.head()\n",
    "\n",
    "# Preprocess the dataset for data quality testing purposes\n",
    "preprocess_df = lending_club.preprocess(df)\n",
    "\n",
    "# Apply feature engineering to the dataset\n",
    "fe_df = lending_club.feature_engineering(preprocess_df)\n",
    "fe_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the feature engineered dataset\n",
    "\n",
    "With our dummy model imported and our feature-engineered dataset ready to go, let's now **spilt our dataset into train and test** to start the validation testing process.\n",
    "\n",
    "Splitting our dataset into train and test is essential for proper validation testing, as this helps assess how well the model generalizes to unseen data.\n",
    "\n",
    "- We begin by dividing our data, which is based on Weight of Evidence (WoE) features, into training and testing sets (`train_df`, `test_df`).\n",
    "- With `lending_club.split`, we employ a simple random split, randomly allocating data points to each set to ensure a mix of examples in both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "train_df, test_df = lending_club.split(fe_df, test_size=0.2)\n",
    "\n",
    "x_train = train_df.drop(lending_club.target_column, axis=1)\n",
    "y_train = train_df[lending_club.target_column]\n",
    "\n",
    "x_test = test_df.drop(lending_club.target_column, axis=1)\n",
    "y_test = test_df[lending_club.target_column]\n",
    "\n",
    "# Now let's apply the order of features from the champion model construction\n",
    "x_train = x_train[cols_when_model_builds]\n",
    "x_test = x_test[cols_when_model_builds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_use = ['annual_inc_woe',\n",
    " 'verification_status_woe',\n",
    " 'emp_length_woe',\n",
    " 'installment_woe',\n",
    " 'term_woe',\n",
    " 'home_ownership_woe',\n",
    " 'purpose_woe',\n",
    " 'open_acc_woe',\n",
    " 'total_acc_woe',\n",
    " 'int_rate_woe',\n",
    " 'sub_grade_woe',\n",
    " 'grade_woe','loan_status']\n",
    "\n",
    "\n",
    "train_df = train_df[cols_use]\n",
    "test_df = test_df[cols_use]\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating potential challenger models*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train two challenger models as basis for the testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Random Forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=50, \n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "rf_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second Challenger Model a Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Logistic Regression grid params\n",
    "log_reg_params = {\n",
    "    \"penalty\": [\"l1\", \"l2\"],\n",
    "    \"C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    \"solver\": [\"liblinear\"],\n",
    "}\n",
    "\n",
    "# Grid search for Logistic Regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\n",
    "grid_log_reg.fit(x_train, y_train)\n",
    "\n",
    "# Logistic Regression best estimator\n",
    "log_reg = grid_log_reg.best_estimator_\n",
    "log_reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute probabilities as this is the raw probabilitistc output from the models of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xgb_prob = xgb_model.predict_proba(x_train)[:, 1]\n",
    "test_xgb_prob = xgb_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "train_rf_prob = rf_model.predict_proba(x_train)[:, 1]\n",
    "test_rf_prob = rf_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "train_log_prob = log_reg.predict_proba(x_train)[:, 1]\n",
    "test_log_prob = log_reg.predict_proba(x_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute binary predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_off_threshold = 0.3 \n",
    "\n",
    "train_xgb_binary_predictions = (train_xgb_prob > cut_off_threshold).astype(int)\n",
    "test_xgb_binary_predictions = (test_xgb_prob > cut_off_threshold).astype(int)\n",
    "\n",
    "train_rf_binary_predictions = (train_rf_prob > cut_off_threshold).astype(int)\n",
    "test_rf_binary_predictions = (test_rf_prob > cut_off_threshold).astype(int)\n",
    "\n",
    "train_log_binary_predictions = (train_log_prob > cut_off_threshold).astype(int)\n",
    "test_log_binary_predictions = (test_log_prob > cut_off_threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the ValidMind objects*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the ValidMind datasets\n",
    "\n",
    "Before you can run tests, you must first initialize a ValidMind dataset object using the [`init_dataset`](https://docs.validmind.ai/validmind/validmind.html#init_dataset) function from the ValidMind (`vm`) module.\n",
    "\n",
    "This function takes a number of arguments:\n",
    "\n",
    "- `dataset`: The dataset that you want to provide as input to tests.\n",
    "- `input_id`: A unique identifier that allows tracking what inputs are used when running each individual test.\n",
    "- `target_column`: A required argument if tests require access to true values. This is the name of the target column in the dataset.\n",
    "\n",
    "With all datasets ready, you can now initialize the raw, processed, training and test datasets (`raw_df`, `preprocessed_df`, `fe_df`,  `train_df` and `test_df`) created earlier into their own dataset objects using [`vm.init_dataset()`](https://docs.validmind.ai/validmind/validmind.html#init_dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_raw_dataset = vm.init_dataset(\n",
    "    dataset=df,\n",
    "    input_id=\"raw_dataset\",\n",
    "    target_column=lending_club.target_column,\n",
    ")\n",
    "\n",
    "vm_preprocess_dataset = vm.init_dataset(\n",
    "    dataset=preprocess_df,\n",
    "    input_id=\"preprocess_dataset\",\n",
    "    target_column=lending_club.target_column,\n",
    ")\n",
    "\n",
    "vm_fe_dataset = vm.init_dataset(\n",
    "    dataset=fe_df,\n",
    "    input_id=\"fe_dataset\",\n",
    "    target_column=lending_club.target_column,\n",
    ")\n",
    "\n",
    "vm_train_ds = vm.init_dataset(\n",
    "    dataset=train_df,\n",
    "    input_id=\"train_dataset\",\n",
    "    target_column=lending_club.target_column,\n",
    ")\n",
    "\n",
    "vm_test_ds = vm.init_dataset(\n",
    "    dataset=test_df,\n",
    "    input_id=\"test_dataset\",\n",
    "    target_column=lending_club.target_column,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a model object\n",
    "\n",
    "You will also need to initialize a ValidMind model object (`vm_model`) that can be passed to other functions for analysis and tests on the data. You simply intialize this model object with [`vm.init_model()`](https://docs.validmind.ai/validmind/validmind.html#init_model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm_xgb_model = vm.init_model(\n",
    "    xgb_model,\n",
    "    input_id=\"xgb_model_developer_champion\",\n",
    ")\n",
    "\n",
    "vm_rf_model = vm.init_model(\n",
    "    rf_model,\n",
    "    input_id=\"rf_model\",\n",
    ")\n",
    "\n",
    "vm_log_model = vm.init_model(\n",
    "    log_reg,\n",
    "    input_id=\"log_model\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign prediction values and probabilities to the datasets\n",
    "\n",
    "With our model now trained, we'll move on to assigning both the predictive probabilities coming directly from the model's predictions, and the binary prediction after applying the cutoff threshold described in the previous steps. \n",
    "- These tasks are achieved through the use of the `assign_predictions()` method associated with the VM `dataset` object.\n",
    "- This method links the model's class prediction values and probabilities to our VM train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Model\n",
    "vm_train_ds.assign_predictions(\n",
    "    model=vm_xgb_model,\n",
    "    prediction_values=train_xgb_binary_predictions,\n",
    "    prediction_probabilities=train_xgb_prob,\n",
    ")\n",
    "\n",
    "vm_test_ds.assign_predictions(\n",
    "    model=vm_xgb_model,\n",
    "    prediction_values=test_xgb_binary_predictions,\n",
    "    prediction_probabilities=test_xgb_prob,\n",
    ")\n",
    "\n",
    "# Random Forest Model\n",
    "vm_train_ds.assign_predictions(\n",
    "    model=vm_rf_model,\n",
    "    prediction_values=train_rf_binary_predictions,\n",
    "    prediction_probabilities=train_rf_prob,\n",
    ")\n",
    "\n",
    "vm_test_ds.assign_predictions(\n",
    "    model=vm_rf_model,\n",
    "    prediction_values=test_rf_binary_predictions,\n",
    "    prediction_probabilities=test_rf_prob,\n",
    ")\n",
    "\n",
    "\n",
    "# Logistic Regression \n",
    "vm_train_ds.assign_predictions(\n",
    "    model=vm_log_model,\n",
    "    prediction_values=train_log_binary_predictions,\n",
    "    prediction_probabilities=train_log_prob,\n",
    ")\n",
    "\n",
    "vm_test_ds.assign_predictions(\n",
    "    model=vm_log_model,\n",
    "    prediction_values=test_log_binary_predictions,\n",
    "    prediction_probabilities=test_log_prob,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute credit risk scores\n",
    "\n",
    "In this phase, we translate model predictions into actionable scores using probability estimates generated by our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xgb_scores = lending_club.compute_scores(train_xgb_prob)\n",
    "test_xgb_scores = lending_club.compute_scores(test_xgb_prob)\n",
    "train_rf_scores = lending_club.compute_scores(train_rf_prob)\n",
    "test_rf_scores = lending_club.compute_scores(test_rf_prob)\n",
    "train_log_scores = lending_club.compute_scores(train_log_prob)\n",
    "test_log_scores = lending_club.compute_scores(test_log_prob)\n",
    "\n",
    "# Assign scores to the datasets\n",
    "vm_train_ds.add_extra_column(\"xgb_scores\", train_xgb_scores)\n",
    "vm_test_ds.add_extra_column(\"xgb_scores\", test_xgb_scores)\n",
    "vm_train_ds.add_extra_column(\"rf_scores\", train_rf_scores)\n",
    "vm_test_ds.add_extra_column(\"rf_scores\", test_rf_scores)\n",
    "vm_train_ds.add_extra_column(\"log_scores\", train_log_scores)\n",
    "vm_test_ds.add_extra_column(\"log_scores\", test_log_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIP BELOW**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7347ccf",
   "metadata": {},
   "source": [
    "### Enable custom context for test descriptions\n",
    "\n",
    "When you run ValidMind tests, test descriptions are automatically generated with LLM using the test results, the test name, and the static test definitions provided in the test’s docstring. While this metadata offers valuable high-level overviews of tests, insights produced by the LLM-based descriptions may not always align with your specific use cases or incorporate organizational policy requirements.\n",
    "\n",
    "Here, we'll include some initial custom use case context, improving the relevancy, insight, and format of the test descriptions returned.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"background-color: #B5B5B510; color: black; border: 1px solid #083E44; border-left-width: 5px; box-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);border-radius: 5px;\"><span style=\"color: #083E44;\"><b>Want to learn more about setting custom context for LLM-generated test descriptions?</b></span>\n",
    "<br></br>\n",
    "Refer to our extended walkthrough notebook: <a href=\"https://docs.validmind.ai/notebooks/how_to/add_context_to_llm_descriptions.html\" style=\"color: #DE257E;\"><b>Add context to LLM-generated test descriptions\n",
    "</b></a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ad6e51",
   "metadata": {},
   "source": [
    "#### Review default LLM-generated descriptions\n",
    "\n",
    "By default, custom context for LLM-generated descriptions is disabled, meaning that the output will not include any additional context.\n",
    "\n",
    "Let's generate an initial test description for the `HighPearsonCorrelation` test for comparison with later iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb4450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.data_validation.HighPearsonCorrelation\",\n",
    "    inputs={\n",
    "        \"dataset\": fe_df,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5612ba38",
   "metadata": {},
   "source": [
    "#### Enable use case context\n",
    "\n",
    "To enable custom use case context, set the `VALIDMIND_LLM_DESCRIPTIONS_CONTEXT_ENABLED` environment variable to `1`.\n",
    "\n",
    "This is a global setting that will affect all tests for your linked model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6105468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"VALIDMIND_LLM_DESCRIPTIONS_CONTEXT_ENABLED\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1e90ba",
   "metadata": {},
   "source": [
    "Enabling use case context allows you to pass in additional context to the LLM-generated text descriptions within `context`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f7fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"\n",
    "FORMAT FOR THE LLM DESCRIPTIONS: \n",
    "    **<Test Name>** is designed to <begin with a concise overview of what the test does and its primary purpose, \n",
    "    extracted from the test description>.\n",
    "\n",
    "    The test operates by <write a paragraph about the test mechanism, explaining how it works and what it measures. \n",
    "    Include any relevant formulas or methodologies mentioned in the test description.>\n",
    "\n",
    "    The primary advantages of this test include <write a paragraph about the test's strengths and capabilities, \n",
    "    highlighting what makes it particularly useful for specific scenarios.>\n",
    "\n",
    "    Users should be aware that <write a paragraph about the test's limitations and potential risks. \n",
    "    Include both technical limitations and interpretation challenges. \n",
    "    If the test description includes specific signs of high risk, incorporate these here.>\n",
    "\n",
    "    **Key Insights:**\n",
    "\n",
    "    The test results reveal:\n",
    "\n",
    "    - **<insight title>**: <comprehensive description of one aspect of the results>\n",
    "    - **<insight title>**: <comprehensive description of another aspect>\n",
    "    ...\n",
    "\n",
    "    Based on these results, <conclude with a brief paragraph that ties together the test results with the test's \n",
    "    purpose and provides any final recommendations or considerations.>\n",
    "\n",
    "ADDITIONAL INSTRUCTIONS:\n",
    "    Present insights in order from general to specific, with each insight as a single bullet point with bold title.\n",
    "    You are a model validator and the goal is to identify risk and/or suggest room for improvements or recommendations on what Model Developer should do in order to improve outcomes and reduce risk\n",
    "\n",
    "    For each metric in the test results, include in the test overview:\n",
    "    - The metric's purpose and what it measures\n",
    "    - Its mathematical formula\n",
    "    - The range of possible values\n",
    "    - What constitutes good/bad performance\n",
    "    - How to interpret different values\n",
    "\n",
    "    Each insight should progressively cover:\n",
    "    1. Overall scope and distribution\n",
    "    2. Complete breakdown of all elements with specific values\n",
    "    3. Natural groupings and patterns\n",
    "    4. Comparative analysis between datasets/categories\n",
    "    5. Stability and variations\n",
    "    6. Notable relationships or dependencies\n",
    "\n",
    "    Remember:\n",
    "    - Keep all insights at the same level (no sub-bullets or nested structures)\n",
    "    - Make each insight complete and self-contained\n",
    "    - Include specific numerical values and ranges\n",
    "    - Cover all elements in the results comprehensively\n",
    "    - Maintain clear, concise language\n",
    "    - Use only \"- **Title**: Description\" format for insights\n",
    "    - Progress naturally from general to specific observations\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "os.environ[\"VALIDMIND_LLM_DESCRIPTIONS_CONTEXT\"] = context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e38bd2",
   "metadata": {},
   "source": [
    "With the use case context set, generate an updated test description for the `DatasetDescription` test for comparision with default output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adede511",
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.tests.run_test(\n",
    "    \"validmind.data_validation.HighPearsonCorrelation\",\n",
    "    inputs={\n",
    "        \"dataset\": fe_df,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
